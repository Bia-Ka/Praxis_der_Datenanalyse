{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Folien für das Modul 'Praxis der Datenanalyse'\"\n#author: \"ses\"\n#institute: \"FOM\" \ndate: \"WS17\"\nlinkcolor: blue\nurlcolor: blue\nlang: de-De\noutput:\n  beamer_presentation:\n    fig_caption: no\n    slide_level: 2\n    includes:\n        in_header: \"Rahmen.tex\" \n---\n\n\n\n```{r setup, include=FALSE}\nlibrary(knitr)\n\nknitr::opts_chunk$set(\n    echo = TRUE,\n    background='#E0E0E0',\n#   fig.keep=\"none\",\n    out.width=\"80%\",\n#   cache = TRUE,\n    tidy=TRUE,\n    fig.align = \"center\",\n    width.cutoff=70,\n    fig.asp = .61,\n    message = FALSE,\n    warning = FALSE\n)\n\noptions(width = 73)\n\n\n```\n\n\n```{r libs, include = FALSE, echo = FALSE}\n#library(mosaic)\nlibrary(tidyverse)\nlibrary(formatR)\nlibrary(knitr)\nlibrary(gridExtra)\nlibrary(broom)\nlibrary(grid)\nlibrary(modelr)\nlibrary(viridis)\nlibrary(SDMTools)  # Güte von Klassifikationsmodellen\nlibrary(pROC)  # für ROC- und AUC-Berechnung\nlibrary(BaylorEdPsych)  # Pseudo-R-Quadrat\nlibrary(pander)\nlibrary(cluster)\nlibrary(psych)\nlibrary(stringr)  # Textverarbeitung\nlibrary(tidytext)  # Textmining\nlibrary(pdftools)  # PDF einlesen\nlibrary(downloader)  # Daten herunterladen\nlibrary(lsa)  # Stopwörter \nlibrary(SnowballC)  # Wörter trunkieren\nlibrary(wordcloud)  # Wordcloud anzeigen\n```\n\n\n## Grobgliederung\n\n\n- [Vorwort](#Vorwort)\n- [Rahmen](#Rahmen)\n- [Daten einlesen]{#tidy}\n- [Datenjudo](#Datenjudo)\n- [Daten visualisieren](#vis)\n- [Grundlagen des Modellierens](#mod)\n- [Der p-Wert](#p-wert)\n- [Lineare Regression](#Lineare-Regression)\n- [Klassifizierende (logistische) Regression](#Klassifizierende-(logistische)- Regression)\n- [Clusteranalyse](#Clusteranalyse)\n- [Dimensionsreduktion](#Dimensionsreduktion)\n- [Textmining](#Textmining)\n- [Anhang](#Anhang)\n\n# Vorwort \n\n## Hinweise\n\n- Diese Folien vermitteln *nicht* den Stoff.\n\n- Sie visualisieren nur einige zentrale Ideen.\n\n- Der Stoff wird vom Skript vermittelt.\n\n- Nutzen Sie das Skript zum eigentlichen Arbeiten.\n\n\n# Organisatorisches\n\n## Modulziele\n\nDie Studierenden können nach erfolgreichem Abschluss des Moduls:\n\n\n- den Ablauf eines Projekts aus der Datenanalyse in wesentlichen Schritten nachvollziehen,\n-\tDaten aufbereiten und ansprechend visualisieren,\n-\tInferenzstatistik anwenden und kritisch hinterfragen,\n- klassische Vorhersagemethoden (Regression) anwenden,\n-\tmoderne Methoden der angewandten Datenanalyse anwenden (z.B. Textmining),\n-\tbetriebswirtschaftliche Fragestellungen mittels datengetriebener Vorhersagemodellen beantworten.\n\n\n## Themen pro Termin (insgesamt 44UE Lehre)\n\n\n\n\\footnotesize\n```{r termin-themen, echo = FALSE}\n\ndf <- readr::read_csv(\"../includes/Termin_Themen.csv\")\n\nknitr::kable(df)\n\n```\n\n\n\n## Prüfung - Allgemeine Hinweise\n\n\n- Die Prüfung besteht aus zwei Teilen\n    - einer Klausur (50% der Teilnote)\n    - einer Datenanalyse (50% der Teilnote).\n   \n   \n   \n*Prüfungsrelevant* ist der gesamte Stoff aus dem Skript und dem Unterricht mit folgenden Ausnahmen:\n\n  - Inhalte/Abschnitte, die als \"nicht klausurrelevant\" gekennzeichnet sind,\n  - Inhalte/Abschnitte, die als \"Vertiefung\" gekennzeichnet sind,\n  - Fallstudien (nur für Klausuren nicht prüfungslevant),\n  - die Inhalte von Links,\n  - die Inhalte von Fußnoten,\n  - die Kpaitel *Vorwort*, *Organisatorisches* und *Anhang*.  \n  \n\nAlle Hinweise zur Prüfung gelten nur insoweit nicht anders vom Dozenten festgelegt.  \n\n\n\n## Klausur und Datenanalyse\n\n\n### Klausur\n- Hinweise zur Klausur finden Sie [hier]()\n- Im Unterricht findet eine Probeklausur statt.\n- Lernaufgaben finden sich im Skript.\n\n\n### Datenanalyse\n- Hinweise zur Datenanalyse finden Sie [hier]().\n- Die Datenanalyse wird (in fast jeder Stunde) praktisch eingeübt.\n- Beispiele für gute Datenanalysen von Studierenden finden Sie [hier]().\n\n\n# Rahmen\n\n## Lernziele\n\n- Einen Überblick über die fünf wesentliche Schritte der Datenanalyse gewinnen.\n- R und RStudio installieren können.\n- Einige häufige technische Probleme zu lösen wissen.\n- R-Pakete installieren können.\n- Einige grundlegende R-Funktionalitäten verstehen.\n- Auf die Frage \"Was ist Statistik?\" eine Antwort geben können.\n\n\n\n## Prozess der Datenanalyse - Überblick über das Modul\n\n```{r fig-prozess, echo = FALSE, fig.cap = \"Der Prozess der Datenanalyse\"}\nknitr::include_graphics(\"../images/Rahmen/Prozess_Datenanalyse.png\") \n```\n\n\n## R und RStudio installieren\n\n![](../images/Rahmen/Rlogo.png){ width=10% } ![](../images/Rahmen/rstudiologo.png){ width=10% }\n\n\n\n\n```{r echo = FALSE, fig.cap = \"RStudio\"} \nknitr::include_graphics(\"../images/Rahmen/RStudio-Screenshot.png\")\n\n```\n\n\n## Hilfe! R!\n\nBeliebte Fehler beim Installieren von Paketen:\n\n```\n- install.packages(dplyr) \n\n- install.packages(\"dliar\")\n\n- install.packages(\"derpyler\") \n\n- install.packages(\"dplyr\")  # dependencies vergessen \n\n- Keine Internet-Verbindung \n\n- library(dplyr)  # ohne vorher zu installieren\n```\n\n## Pakete installieren leichtgemacht\n\n\n```{r fig-install-packages, echo = FALSE, fig.cap = \"So installiert man Pakete in RStudio\", out.width = \"50%\"}\n\nknitr::include_graphics(\"../images/Rahmen/install_packages.png\")\n\n```\n\n\n\n## Was ist Statistik?\n\n*Eine* Antwort dazu ist, dass Statistik die Wissenschaft von\nSammlung, Analyse, Interpretation und Kommunikation von Daten ist mithilfe \nmathematischer Verfahren ist und zur Entscheidungshilfe beitragen soll. \n\n```{r desk-vs-inf, echo = FALSE, fig.cap  =\"Sinnbild für die Deskriptiv- und die Inferenzstatistik\"}\n\nknitr::include_graphics(\"../images/Rahmen/desk_vs_inf-crop.png\")\n```\n\n\n## Abduktion als klassische Denkfigur in der Statistik\n\n\n`r knitr::asis_output(\"\\\\footnotesize\")\n\n```\nPrämisse 1: Wenn Modell M wahr ist,   \ndann sollten die Daten das Muster D aufweisen.\nPrämisse 2: Die Daten weisen das Muster D auf.\n---\nKonklusion: Daher muss das Modell M wahr sein.\n```\n\n`r knitr::asis_output(\"\\\\normalsize\")`\n\nDie Konklusion ist *nicht* zwangsläufig richtig.\n\n\n# Daten einlesen {#tidy}\n\n## Lernziele\n\n- Wissen, was eine CSV-Datei ist.\n- Wissen, was UTF-8 bedeutet.\n- Erläutern können, was R unter dem \"working directory\" versteht.\n- Erkennen können, ob eine Tabelle in Normalform vorliegt.\n- Daten aus R hinauskriegen (exportieren).\n\n\nDieses Kapitel beantwortet eine Frage: \"Wie kriege ich Daten in vernünftiger Form in R hinein?\".\n\n\n## Prozess der Datenanalyse -- Einlesen\n\n\n```{r step-Einlesen, echo = FALSE, fig.cap = \"Daten sauber einlesen\"}\nknitr::include_graphics(\"../images/tidy/Einlesen.png\")\n```\n\n\n## Daten (CSV, XLS,...) mit RStudio importieren\n\n\n```{r data-import-RStudio, echo = FALSE, out.width = \"50%\", fig.cap = \"Daten einlesen (importieren) mit RStudio\"}\nknitr::include_graphics(\"../images/tidy/import_RStudio.png\")\n```\n\n## CSV-Dateien sind einer der wichtigsten Daten-Formate\n\n\n```\nrow_number,date_time,study_time,self_eval,interest,score\n1,05.01.2017 13:57:01,5,8,5,29\n2,05.01.2017 21:07:56,3,7,3,29\n3,05.01.2017 23:33:47,5,10,6,40\n4,06.01.2017 09:58:05,2,3,2,18\n5,06.01.2017 14:13:08,4,8,6,34\n6,06.01.2017 14:21:18,NA,NA,NA,39\n```\n\n## Das Arbeitsverzeichnis mit RStudio wählen\n\n\n```{r Arbeitsverzeichnis, echo = FALSE, fig.cap = \"Das Arbeitsverzeichnis mit RStudio auswählen\", out.width = \"50%\"}\n\nknitr::include_graphics(\"../images/tidy/Arbeitsverzeichnis.png\")\n```\n\n\n## Normalform einer Tabelle\n\n```{r tidy1, fig.cap = \"Schematische Darstellung eines Dataframes in Normalform\", echo = FALSE}\nknitr::include_graphics(\"../images/tidy/tidy-1.png\")\n```\n\n\n## Breit vs. Lang\n\n```{r lang-breit, echo = FALSE, fig.cap = \"Dieselben Daten - einmal breit, einmal lang\"}\nknitr::include_graphics(\"../images/tidy/breit_lang.png\")\n```\n\n\n## Ein Dataframe in Normalform - Beispiel\n\n```{r fig-Normalform, echo = FALSE, fig.cap = \"Illustration eines Datensatzes in Normalform\", out.width = \"40%\"}\nknitr::include_graphics(\"../images/tidy/Normalform.png\")\n```\n\n\n## Tabelle in Normalform bringen {#normalform}\n\n\n\n```{r gather-spread, echo = FALSE, fig.cap = \"Mit 'gather' und 'spread' wechselt man von der breiten Form zur langen Form\"}\n\nknitr::include_graphics(\"../images/tidy/gather_spread-crop.png\")\n\n```\n\n\n## Beispiel für die Normalisierung einer Tabelle\n\n```{r bsp-abb, echo = FALSE, fig.cap = \"Ein Beispiel für eine Abbildung zu einer Normalform-Tabelle\", out.width = \"50%\"}\n\nknitr::include_graphics(\"../images/tidy/bsp_diagramm-crop.png\")\n```\n\n\n## `gather` und `spread`\n\n```{r eval = FALSE}\n\n\ndf_lang <- gather(df_breit, key = \"Quartal\",\n                  value = \"Umsatz\")\n\ndf_breit <- spread(df_lang, Quartal, Umsatz)\n\ndf_lang <- gather(df_breit, key = \"Quartal\",\n                  value = \"Umsatz\", -ID)\n```\n\n\n## Textkodierung und Daten exportieren\n\n>    Speichern Sie R-Textdateien wie Skripte stets mit UTF-8-Kodierung ab.\n\n\n```{r eval = FALSE}\nwrite.csv(name_der_tabelle, \"Dateiname.csv\")\n```\n\n\n\n# Datenjudo\n\n## Lernziele für das Kapitel 'Datenjudo'\n\n- Die zentralen Ideen der Datenanalye mit dplyr verstehen.\n- Typische Probleme der Datenanalyse schildern können.\n- Zentrale `dplyr`-Befehle anwenden können.\n- `dplyr`-Befehle kombinieren können.\n- Die Pfeife anwenden können.\n- Werte umkodieren und \"binnen\" können.\n\n\n\n## Prozess der Datenanalyse -- Datenjudo\n\n\n```{r fig-datenjudo, echo = FALSE, fig.cap = \"Daten aufbereiten\"}\nknitr::include_graphics(\"../images/Datenjudo/Aufbereiten.png\")\n```\n\n\n## Typische Probleme bei der Datenaufbereitung\n\n\n\nTypische Probleme, die immer wieder auftreten, sind:\n\n- *Fehlende Werte*\n- *Unerwartete Daten*\n- *Daten müssen umgeformt werden*\n- *Neue Variablen (Spalten) berechnen*: \n- ...\n\n\n## Daten aufbereiten mit `dplyr`\n\n\n\\begin{columns}\n  \\begin{column}{0.49\\textwidth}\n    \n```{r bausteine, echo = FALSE, fig.cap = \"Lego-Prinzip: Zerlege eine komplexe Struktur in einfache Bausteine\", out.width = \"100%\"}\nknitr::include_graphics(\"../images/Datenjudo/Bausteine_dplyr-crop.png\")\n```\n\n \\end{column}\n  \\begin{column}{0.49\\textwidth}\n\n\n```{r durchpfeifen-allgemein, echo = FALSE, fig.cap = \"Durchpfeifen: Ein Dataframe wird von Operation zu Operation weitergereicht\", out.width = \"100%\"}\nknitr::include_graphics(\"../images/Datenjudo/durchpfeifen_allgemein_crop.png\")\n```\n\n\n  \\end{column}\n\\end{columns}\n\n## Zeilen filtern mit `filter`\n\n\n```{r fig-filter, echo = FALSE, fig.cap = \"Zeilen filtern\"}\nknitr::include_graphics(\"../images/Datenjudo/filter.png\")\n```\n\n\n\n## Spalten wählen mit `select`\n\n\n```{r fig-select, echo = FALSE, fig.cap = \"Spalten auswählen\"}\nknitr::include_graphics(\"../images/Datenjudo/select.png\")\n```\n\n\n## Zeilen sortieren mit `arrange`\n\n\n\n```{r fig-arrange, echo = FALSE, fig.cap = \"Spalten sortieren\"}\nknitr::include_graphics(\"../images/Datenjudo/arrange-crop.png\")\n```\n\n\n## Datensatz gruppieren mit `group_by`\n\n\n```{r fig-groupby, echo = FALSE, fig.cap = \"Datensätze nach Subgruppen aufteilen\"}\nknitr::include_graphics(\"../images/Datenjudo/group_by.png\")\n```\n\n\n\n## Eine Spalte zusammenfassen mit `summarise`\n\n\n```{r fig-summarise, echo = FALSE, fig.cap = \"Spalten zu einer Zahl zusammenfassen\"}\nknitr::include_graphics(\"../images/Datenjudo/summarise.png\")\n```\n\n\n## Zeilen zählen mit `n` und `count`\n\n\n```{r fig-count, echo = FALSE, fig.cap = \"Sinnbild für 'count'\"}\n\nknitr::include_graphics(\"../images/Datenjudo/count-crop.png\")\n```\n\n\n## Die Pfeife\n\n```{r cecie-une-pipe, echo = FALSE, fig.cap = \"Das ist keine Pfeife\"}\nknitr::include_graphics(\"../images/Datenjudo/800px-Pipa_savinelli.jpg\")\n```\n\n\n## Befehle hintereinander reihen mit der Pfeife\n\n\n```{r fig-durchpfeifen, echo = FALSE, out.width  = \"80%\", fig.cap = \"Das 'Durchpeifen'\"}\nknitr::include_graphics(\"../images/Datenjudo/durchpfeifen.png\")\n```\n\n## Introducing Pipe-Syntax\n\nVergleichen Sie mal diese Syntax\n\n```{r eval = FALSE}\nfilter(summarise(group_by(filter(stats_test, \n       !is.na(score)), interest), mw = mean(score)), \n       mw > 30)\n```\n\nmit dieser\n\n```{r eval = FALSE}\nstats_test %>% \n  filter(!is.na(score)) %>% \n  group_by(interest) %>% \n  summarise(mw = mean(score)) %>% \n  filter(mw > 30)\n```\n\n\n## Pfeifen macht das Leben leichter\n\nTipp: In RStudio gibt es einen Shortcut für die Pfeife: Strg-Shift-M (auf allen Betriebssystemen).\n\n\n\n\nDie Syntax von oben auf Deutsch:\n\n- Nimm die Tabelle \"stats_test\" UND DANN  \n- filtere alle nicht-fehlenden Werte UND DANN  \n- gruppiere die verbleibenden Werte nach \"interest\" UND DANN  \n- bilde den Mittelwert (pro Gruppe) für \"score\" UND DANN  \n- liefere nur die Werte größer als 30 zurück.  \n\n\n## Spalten berechnen mit `mutate`\n\n\n```{r fig-mutate, echo = FALSE, fig.cap = \"Sinnbild für mutate\"}\nknitr::include_graphics(\"../images/Datenjudo/mutate.png\")\n```\n\n\n## Beispiel für `mutate`\n\n\n```{r eval = FALSE}\nstats_test %>% \n  mutate(Streber = score > 38) %>% \n  head()\n```\n\n\n## Deskriptive Statistik mit `dplyr`\n\n\n```{r eval = FALSE}\nstats_test2 <- select(stats_test, -date_time) \ndesctable(stats_test2l)\n```\n\n\n\n\n\n\n# Daten visualisieren {#vis}\n\n\n## Lernziele für das Kapitel 'Daten visualisieren'\n\n- An einem Beispiel erläutern können, warum/ wann ein Bild mehr sagt, als 1000 Worte.\n- Häufige Arten von Diagrammen erstellen können.\n- Diagramme bestimmten Zwecken zuordnen können.\n\n\n## Statistik ist wie ein Bikini...\n\n\n```{r fig-anscombe, echo = FALSE, fig.cap = \"Das Anscombe-Quartett\"}\nknitr::include_graphics(\"../images/visualisieren/anscombe.png\")\n```\n\n\n[Dinosaurier-Video](https://youtu.be/DbJyPELmhJc)\n\n\n\n\n## Die Anatomie eines Diagramms\n\n\n\n```{r fig-anatomie, echo = FALSE, fig.cap = \"Anatomie eines Diagramms\"}\nknitr::include_graphics(\"../images/visualisieren/anatomie_diagramm_crop.jpg\")\n```\n\n\n## Beispiel für ein Diagramm it mit `ggplot2::qplot`\n\n```{r echo = FALSE}\nmovies <- read.csv(\"../data/movies.csv\")\n```\n\n\n```{r fig-movies, message = FALSE, fig.cap = \"Mittleres Budget pro Jahr\"}\nqplot(x = year, y = budget, geom = \"point\", data = movies)\n```\n\n## Anatomiestunde mit `qplot`\n\n\n\n- `qplot`: Erstelle schnell (q wie quick in `qplot`) mal einen Plot (engl. \"plot\": Diagramm).    \n- `x`: Der X-Achse soll die Variable \"year\" zugeordnet werden.    \n- `y`: Der Y-Achse soll die Variable \"budget\" zugeorndet werden.  \n- `geom`: (\"geometriches Objekt\") Gemalt werden sollen Punkte und zwar pro Beobachtung (hier: Film) ein Punkt; nicht etwa Linien oder Boxplots.\n- `data`: Als Datensatz bitte `movies` verwenden.  \n\n\n## Syntax-Blaupause für `qplot`\n\nDiese Syntax des letzten Beispiels ist recht einfach, nämlich:\n\n```{r, eval = FALSE}\nqplot (x = X_Achse, \n       y = Y_Achse, \n       data = mein_dataframe, \n       geom = \"ein_geom\")\n```\n\n\n## Häufige Diagrammtypen\n\n\n- Histogramm, Dichtediagramm\n- Punkte, Schachbrett-Diagramme\n- Balkendiagramm\n- Mosaicplot (Fliesendiagramm)\n- Punktediagramm für Zusammenfassungen\n- Boxplots \n\n\n# Grundlagen des Modellierens {#mod}\n\n\n## Prozess der Datenanalyse - Modellieren\n\n\n```{r echo = FALSE, out.width = \"70%\"}\nknitr::include_graphics(\"../images/modellieren/Modellieren.png\")\n```\n\n\n\n## Was ist ein Modell\n\n```{r vwmodell, echo = FALSE, fig.cap = \"Modell eines VW-Käfers\"}\nknitr::include_graphics(\"../images/modellieren/vw_modell.JPG\")\n```\n\n\n\n## Die Beziehung von Gegenstandsbereich und Modell\n\n```{r modellieren-plot, echo = FALSE, fig.cap = \"Modellieren\"}\nknitr::include_graphics(\"../images/modellieren/Modell.png\")\n```\n\n\n## Modelle spiegeln empirische Relationen in numerischen Relationen\n\n>   Modellieren bedeutet ein Verfahren zu erstellen, welches empirische Sachverhalte adäquat in numerische Sachverhalte umsetzt.\n\n```{r modellieren-formal, echo = FALSE, fig.cap = \"Formaleres Modell des Modellierens\"}\nknitr::include_graphics(\"../images/modellieren/Modellieren_formal_crop.png\")\n```\n\n\n\n## Ein Beispiel zum Modellieren aus der Datenanalyse\n\n```{r plot-stats-smooth, echo = FALSE}\nstats_test <- readr::read_csv(\"../data/test_inf_short.csv\")\n\nstats_test %>% \nggplot(aes(y = score, x = self_eval)) +\ngeom_jitter() -> p1\n\np2 <- p1 + geom_smooth(method = \"lm\", se = FALSE)\n\n# p3 <- grid::rasterGrob(readPNG(\"../images/Modellieren_Bsp1.png\"), interpolate=TRUE)\ngrid.arrange(p1, p2, nrow = 1)\n```\n\nDie blaue Gerade ist ein Modell für den Datensatz (sie versucht es zumindest).\n\n\n## Modelle umfassen drei Aspekte\n\n```{r fig-blackbox, echo = FALSE, fig.cap = \"Modelle mit schwarzer Kiste\"}\nknitr::include_graphics(\"../images/modellieren/Modell_Blackbox.png\")\n```\n\n\n## Taxonomie der Ziele des Modellierens \n\n- Geleitetes Modellieren\n    - Prädiktives Modellieren\n    - Explikaties Modellieren\n- Ungeleitetes Modellieren\n    - Dimensionsreduzierendes Modellieren\n    - Fallreduzierendes Modellieren\n\n\n## Veranschaulichung der beiden Arten des Modellierens\n\n```{r ungeleitetes-modellieren, echo = FALSE, fig.cap = \"Die zwei Arten des ungeleiteten Modellierens\"}\n\nknitr::include_graphics(\"../images/modellieren/ungeleitetes_Modellieren_crop.png\")\n\n```\n\n\n\n## Die vier Schritte des statistischen Modellierens\n\n\n1. Man wählt eines der vier Ziele des Modellierens (z.B. ein prädiktives Modell).\n1. Man wählt ein Modell aus (genauer: eine Modellfamilie), z.B. postuliert man, dass die Körpergröße einen linearen Einfluss auf die Schuhgröße habe.\n3. Man bestimmt (berechnet) die Details des Modells anhand der Daten: Wie groß ist die Steigung der Geraden und wo ist der Achsenabschnitt? Man sagt auch, dass man die *Modellparameter* anhand der Daten schätzt (\"Modellinstantiierung\" oder \"Modellanpassung\", engl. \"model fitting\").\n4. Dann prüft man, wie gut das Modell zu den Daten passt (Modellgüte, engl. \"model fit\"); wie gut lässt sich die Schuhgröße anhand der Körpergröße vorhersagen bzw. wie groß ist der Vorhersagefehler?\n\n\n## Einfache vs. komplexe Modelle: Unter- vs. Überanpassung\n\n\n\n```{r overfitting-prep-4-plots, echo = FALSE, include = FALSE, fig.height = 3, fig.width = 3}\nx <- seq(from = 1, to = 10, by = .3)\ny <- sin(x) + rnorm(n = length(x), mean = 0, sd = .3)\n\ndaten <- data_frame(x, y)\n\nggplot(daten) +\n  aes(x = x, y = y) + \n  coord_fixed(ratio = 5/1) +\n  labs(y = \"\") +\n  geom_point() +\n  ggtitle(\"A\") -> p1\n\nggplot(daten) +\n  aes(x = x, y = y) +\n  geom_point() + \n  coord_fixed(ratio = 5/1) +\n  labs(y = \"\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  ggtitle(\"B\")-> p2\n\n\nggplot(daten) +\n  aes(x = x, y = y) +\n  geom_point() + \n  coord_fixed(ratio = 5/1) +\n  labs(y = \"\") +\n  geom_line(color = \"blue\") +\n  ggtitle(\"C\") -> p3\n\nggplot(daten) +\n  aes(x = x, y = y) +\n  geom_point() + \n  coord_fixed(ratio = 5/1) +\n  labs(y = \"\") +\n  stat_function(n = 99, fun = sin, color = \"darkgreen\") +\n  ggtitle(\"D\") -> p4\n```\n\n\n```{r overfitting-4-plots, echo = FALSE, fig.cap = \"Welches Modell (Teil B-D; rot, grün, blau) passt am besten zu den Daten (Teil A) ?\", out.width = \"90%\"}\n\ngrid.arrange(p1, p2, p3, p4, ncol = 4)\n\n```\n\n\n\n## Vorhersagegüte der Trainings-Stichprobe vs. der Test-Stichprobe\n\nBeschreibt ein Modell (wie das blaue Modell hier) eine Stichprobe sehr gut, heißt das noch *nicht*, dass es auch zukünftige (und vergleichbare) Stichproben gut beschreiben wird.  Die Güte (Vorhersagegenauigkeit) eines Modells sollte sich daher stets auf eine neue Stichprobe beziehen (Test-Stichprobe), die nicht in der Stichprobe beim Anpassen des Modells (Trainings-Stichprobe) enthalten war. \n\n\n## Overfitting\n\n```{r overfitting-schema, echo = FALSE, fig.cap = \"'Mittlere' Komplexität hat die beste Vorhersagegenauigkeit (am wenigsten Fehler) in der Test-Stichprobe\"}\nknitr::include_graphics(\"../images/modellieren/overfitting.png\")\n```\n\n\n## Bias-Varianz-Abwägung\n\n>    Einfache Modelle: Viel Bias, wenig Varianz.\n     Komplexe Modelle: Wenig Bias, viel Varianz.\n     \n     \n\n```{r plot-bias-variance, echo = FALSE, fig.cap = \"Der Spagat zwischen Verzerrung und Varianz\", out.width = \"50%\"}\npoly_degree = 15\ndf <- data_frame(x = seq(from = 1, to = 10, by = .3),\n                 y = sin(x) + rnorm(n = length(x), mean = 0, sd = .3))\n\ndf %>% \n  mutate(binned = cut(.$x, breaks = c(-Inf, 5.5, +Inf))) %>% \n  group_by(binned) %>% \n  mutate(y_group_md = median(y)) -> df\n\n\np1 <- ggplot(df) +\n  aes(x = x, y = y) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, poly_degree), se = FALSE) +\n  coord_fixed(ratio = 5/1)\n\n\np2 <-  ggplot(df) +\n  aes(x = x) +\n  geom_point(aes(y = y)) +\n  geom_line(aes(y = y_group_md, group = binned), color = \"firebrick\") + \n  coord_fixed(ratio = 5/1) \n\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n\n\n\n\n\n# Der p-Wert {#p-wert}\n\n\n## Lernziele\n\n\n- Den p-Wert erläutern können.\n- Den p-Wert kritisieren können.\n- Alternativen zum p-Wert kennen.\n- Inferenzstatistische Verfahren für häufige Fragestellungen kennen.\n\n\n\n## Sir Ronald Fisher, Erfinder des Nullhypothesen Testens \n\n\n```{r sir-fisher, echo = FALSE, fig.cap = \"Der größte Statistiker des 20. Jahrhunderts (p < .05)\", out.width = \"20%\", fig.align = \"center\"}\nknitr::include_graphics(\"../images/inferenz/Ronald_Fisher.jpg\")\n```\n\n## Der p-Wert ist die heilige Kuh der Forscher\n\n```{r who-said, echo = FALSE, out.width = \"35%\", fig.cap  = \"Der p-Wert wird oft als wichtig erachtet\", fig.align = \"center\"}\nknitr::include_graphics(\"../images/inferenz/p_value_who_said.png\")\n```\n\n>   Der p-Wert sagt, wie gut die Daten zur Nullhypothese passen.\n\n\n## Von Männern und Päpsten\n\n$$ P(M|P) \\ne P(P|M) $$\n\n```{r moslems-terroristen, echo = FALSE, fig.cap = \"Mann und Papst zu sein, ist nicht das gleiche.\"}\nknitr::include_graphics(\"../images/inferenz/maenner_papst-crop.png\")\n```\n\n## Der p-Wert ist eine Funktion der Stichprobengröße\n\n```{r einfluss-pwert, echo = FALSE, fig.cap = \"Zwei Haupteinflüsse auf den p-Wert\"}\n\nknitr::include_graphics(\"../images/inferenz/einfluss_pwert-crop.png\")\n```\n\n\n\n\n\n## Zur Philosophie des p-Werts: Frequentismus\n\n\n\n```{r echo = FALSE}\ndf <- data_frame(\n  i = 1:500,\n  flip = sample(x = c(0,1), size = 500, replace = TRUE),\n  prop = cumsum(flip) / i\n)\n```\n\n\n\n\n```{r muenzwurf, echo = FALSE, fig.cap = \"Anteil von 'Kopf' bei wiederholtem Münzwurf\"}\n\nggplot(df) +\n  aes(x = i, y = prop) +\n  geom_line() +\n  geom_point(color = \"grey40\") +\n  ylim(c(0,1)) +\n  labs(x = \"Anzahl der Würfe\",\n       y = \"Anteil von 'Kopf'\")\n\n```\n\n\n\n\n## Alternativen zum p-Wert - Konfidenzintervalle\n\n>   Das 95%-Konfidenzintervall ist der Bereich, in dem der Parameter in 95% der Fälle fallen würde bei sehr häufiger Wiederholung des Versuchs. \n\n\n\n[Visualisierung zum Konfidenzintervall](http://rpsychologist.com/d3/CI/)\n\n\n\n\n## Alternativen zum p-Wert - Effektstärken\n\n```{r effectsizes, echo = FALSE}\n\ndf <- read_csv(\"../includes/effectsizes.csv\")\n\npander::pander(df, caption = \"Überblick über gängige Effektstärkemaße\")\n```\n\n\n## Alternativen zum p-Wert - Bayes-Statistik\n\n$p(D|H)$\n\n```{r bayes, echo = FALSE, fig.cap = \"Die zwei Stufen der Bayes-Statistik in einem einfachen Beispieli\"}\n\nknitr::include_graphics(\"../images/inferenz/bayes-crop.png\")\n\n```\n\n\n\n# Lineare Regression\n\n\n## Lernziele\n\n\n- Wissen, was man unter Regression versteht.\n- Die Annahmen der Regression überprüfen können.\n- Regression mit kategorialen Prädiktoren durchführen können.\n- Die Modellgüte bei der Regression bestimmen können.\n- Interaktionen erkennen und ihre Stärke einschätzen können.\n\n\n\n\n## Beispiel für eine lineare Regression\n\n```\nscore = achsenabschnitt + steigung*study_time\n```\n\n```{r bsp-regression, fig.cap = \"Beispiel für eine Regression\", echo = FALSE}\nstats_test <- readr::read_csv(\"../data/test_inf_short.csv\")\n\nstats_test %>% \n  ggplot +\n  aes(x = study_time, y = score) +\n  geom_jitter() +\n  geom_abline(intercept = 24, \n              slope = 2.3, \n              color = \"red\")\n\n```\n\n\n## Die Formel einer einfachen Regression\n\n\n```\nscore = achsenabschnitt + steigung*study_time\n```\n\n\n## Vorhersagegüte - Veranschaulichung\n\n```{r resids-plot, echo = FALSE, results = \"hold\", fig.cap = \"Geringer (links) vs. hoher (rechts) Vorhersagefehler\"}\n\nset.seed(42)  \nN      <- 100\nbeta   <- 0.4\nintercept <- 1\n\n\nsim <- data_frame(\n  x = rnorm(N),\n  error1 = rnorm(N, mean = 0, sd = .5),\n  error2 = rnorm(N, mean = 0, sd = 2),\n  y1 = intercept + x*beta + error1,\n  y2 = intercept + x*beta + error2,\n  pred = 1 + x*beta\n)\n\n\n\np1 <- ggplot(sim, aes(x, y1)) + \n  geom_abline(intercept = intercept, slope = beta, colour = \"red\") +\n  geom_point(colour = \"#00998a\") +\n  geom_linerange(aes(ymin = y1, ymax = pred), colour = \"grey40\") +\n  ylim(-6,+6)\n\n\np2 <- ggplot(sim, aes(x, y2)) + \n  geom_abline(intercept = intercept, slope = beta, colour = \"red\") +\n  geom_point(colour = \"#00998a\") +\n  geom_linerange(aes(ymin = y2, ymax = pred), colour = \"grey40\") +\n  ylim(-6,+6)\n\n\ngrid.arrange(p1, p2, ncol = 2)\n\n```\n\n\n## Vorhersagegüte - MSE und $R^2$\n\n$$ MSE = \\frac{1}{n} \\sum{(pred - obs)^2} $$\n\n$$ R^2 = 1 - \\left( \\frac{SS_T - SS_M}{SS_T} \\right)$$\n\n\n## Überprüfung der Annahmen der linearen Regression\n\n- Linearität des Zusammenhangs\n- Normalverteilung der Residuen\n- Konstante Varianz\n- Extreme Ausreißer\n- Unabhängigkeit der Beobachtungen\n\n\n## Unterscheiden sich Interessierten von Nicht-I. im Klausurerfolg?\n\n```{r}\nstats_test$interessiert <- stats_test$interest > 3\nstats_test %>% \n  group_by(interessiert) %>% \n  summarise(score = mean(score)) -> score_interesse\n\nscore_interesse\n```\n\n\n## Kategoriale Prädiktoren\n\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  ggplot() +\n  aes(x = interessiert, y = score) +\n  geom_jitter(width = .1) +\n  geom_point(data = score_interesse, color = \"red\", size = 5) +\n  geom_line(data = score_interesse, group = 1, color = \"red\")\n```\n\n\n## Multiple Regression\n\n\n```{r no-interakt, echo = FALSE, fig.cap = \"Eine multivariate Analyse fördert Einsichten zu Tage, die bei einfacheren Analysen verborgen bleiben\"}\n\nlibrary(viridis)\n\ndf1 <- data_frame(\n  interessiert = c(T, F),\n  slope = c(2.3, 2.3),\n  intercept = c(23.7, 24)\n)\n\n\np1 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=study_time, y = score) +\n  geom_jitter(width = .1) +\n  #facet_wrap(~interessiert) +\n  geom_abline(data = df1, mapping = aes(slope = slope, intercept = intercept, color = interessiert)) +\n  guides(color = FALSE) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(title = \"A\") +\n  coord_cartesian(ylim = c(25,35))\n\n\ndf2 <- data_frame(\n  slope = rep(-.33, 5),\n  intercept = rep(c(24), 5),\n  study_time = 1:5)\n\n\np2 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=interessiert, y = score) +\n  geom_jitter(width = .1) +\n  facet_wrap(~study_time) +\n  geom_abline(data = df2, mapping = aes(slope = slope, intercept = intercept, color = study_time)) +\n  scale_color_viridis() +\n  guides(color = FALSE) +\n  labs(title = \"B\")\n\n\ngridExtra::grid.arrange(p1, p2, nrow = 2)\n```\n\n\n## Multivariate Analysen sind cool\n\n>    Die multivariate Analyse zeigt ein anderes Bild, ein genaueres Bild als die einfachere Analyse. Ein Sachverhalt, der für den ganzen Datensatz gilt, kann in Subgruppen anders sein.\n\nErlaubt man der Regression, dass die Regressionsgeraden nicht parallel sein müssen, spricht man von einer *Interaktion*.\n\n\n\n## Ein Beispiel für einen Interaktionseffekt\n\nDie Linien sind *nicht* (ganz) parallel: ein kleiner Interaktionseffekt.\n\n```{r interakt-stats-test, echo = FALSE, fig.cap = \"Eine Regressionsanalyse mit Interaktionseffekten\"}\n\nlm4 <- lm(score ~ interessiert*study_time, data = stats_test)\n\n\ndf1 <- data_frame(\n  interessiert = c(F, T),\n  slope = c(2.44, 2.44-.32),\n  intercept = c(23.6, 23.6+.66)\n)\n\n\np1 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=study_time, y = score) +\n  geom_jitter(width = .1) +\n  #facet_wrap(~interessiert) +\n  geom_abline(data = df1, mapping = aes(slope = slope, intercept = intercept, color = interessiert)) +\n  guides(color = FALSE) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(title = \"A\") +\n  coord_cartesian(ylim = c(25,35))\n\n\n\ndf2 <- stats_test %>% \n  data_grid(\n    study_time = 1:5,\n    interessiert = c(F, T)\n  ) %>% \n  add_predictions(lm4)\n  \n\n\np2 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=interessiert, group = study_time) +\n  geom_jitter(aes(y = score), width = .1) +\n  geom_line(data = df2, aes(y = pred, color = factor(study_time))) +\n  scale_color_viridis(discrete = TRUE) +\n  theme(legend.position = \"bottom\")\n  \n\n\ngrid.arrange(p1, p2, nrow = 2)\n```\n\n\n## Fallstudie zu Overfitting {#overfitting-casestudy}\n\n\n```{r echo = FALSE, message = FALSE}\nlm1 <- lm(score ~ study_time, data = stats_test)\ntrain <- stats_test %>% \n  sample_frac(.8, replace = FALSE)  # Stichprobe von 80%, ohne Zurücklegen\n\ntest <- stats_test %>% \n  anti_join(train)  # Alle Zeilen von \"df\", die nicht in \"train\" vorkommen\n\nlm_train <- lm(score ~ study_time, data = train)\nlm2_predict <- predict(lm_train, newdata = test)\n\n\n```\n\n\n```{r}\ncaret::postResample(pred = lm2_predict, obs = test$score)\n\n```\n\nDie Modellgüte im in der Test-Stichprobe ist meist schlechter als in der Trainings-Stichprobe. Das warnt uns vor Befunden, die naiv nur die Werte aus der Trainings-Stichprobe berichten.\n\n\n\n\n# Klassifizierende (logistische) Regression\n\n## Lernziele\n\n- Die Idee der logistischen Regression verstehen.\n- Die Koeffizienten der logistischen Regression interpretieren können.\n- Die Modellgüte einer logistischten Regression einschätzen können.\n- Klassifikatorische Kennzahlen kennen und beurteilen können.\n\n\n## Problemstellung\n\n```{r echo = FALSE}\nAktien <- readr::read_csv(\"../data/Aktien.csv\") %>% na.omit\n\n```\n\n\n```{r fig-logist-regr1, fig.cap = \"Streudiagramm von Risikobereitschaft und Aktienkauf\"}\np1 <- ggplot(aes(y = Aktienkauf, x = Risikobereitschaft), data = Aktien) + geom_point()\np1\n```\n\n\n## R-Befehl\n\n\n\nDie Funktion `glm` führt die logistische Regression durch. \n```{r}\n\nglm1 <- glm(Aktienkauf ~ Risikobereitschaft, \n            family = binomial(\"logit\"),\n            data = Aktien)\n```\n\n\n## Visualisierung der logistischen Regression\n\n```{r fig-logist-regr2, fig.cap = \"Regressionsgerade für Aktien-Modell\"}\n\np1 + geom_abline(intercept = .18, \n                 slope = .05, color = \"red\")\n\n\n```\n\n\n## Formel der logistischen Regression\n\n$p(y=1)=\\frac{e^x}{1+e^x}$\n\n\n```{r logist-curve, echo = FALSE, fig.cap = \"Die logistische Regression beschreibt eine 's-förmige' Kurve\"}\n# eta-Werte von -10 bis +10 erzeugen\nx <- seq(-10,10,by = 0.1)\n# y-Werte mit logistischer Funktion berechnen\ny <- 1/(1+exp(-x))        # exp() ist die e-Funktion\n\n\nlibrary(latex2exp)\n\n\ndata_frame(\n  x = x,\n  y = y) %>% \nggplot() +\n  aes(x = x, y = y) + \n  geom_line(color = \"#00998a\") +\n  ggtitle(TeX(\"Die e-Funktion: $p(y=1)=\\\\frac{e^x}{1+e^x}$\"))\n```\n\n## Die logistische Regression für die Aktien-Daten\n\n```{r aktien-plot, echo = FALSE, fig.cap = \"Modelldiagramm für den Aktien-Datensatz\"}\n\nAktien %>% \n  mutate(pred = stats::predict(glm1, \n                              data = data.frame(Aktienkauf = Aktien$Aktienkauf), \n                               type = \"response\")) %>% \n  ggplot() +\n  aes(x = Risikobereitschaft) +\n  geom_point(aes(y = Aktienkauf)) +\n  geom_line(aes(y = pred), color = \"red\")\n\n```\n\n\n\n## Interpretation der Koeffizienten\n\n\nIst ein Logit $\\mathfrak{L}$ größer als $0$, so ist die zugehörige Wahrscheinlichkeit größer als 50% (und umgekehrt.)\n\n*Logits*\\index{Logit} $\\mathfrak{L}$:\n\n$\\mathfrak{L} = ln\\left( \\frac{p}{1-p} \\right)$\n\n`y = intercept + 3*Risikobereitschaft`, also\n\n```{r}\n(y <- -1.469 + 3 * 0.257)\n```\n\nAlso y = `r y` *Logits* ($\\mathfrak{L}$).\n\n\n\n## Vorhersage individueller Wahrscheinlichkeiten  \n\n```{r}\npredict(glm1, data.frame(Risikobereitschaft = 1), \n        type = \"response\")\n```\n\n\n## Kategoriale Prädiktoren\n\n\n\n```{r eval = FALSE}\nstr(stats_test$bestanden)\nstats_test$bestanden <- factor(stats_test$bestanden, \n                               levels = c(\"nein\", \"ja\"))\nlog_stats <- glm(bestanden ~ interessiert,\n                 family = binomial(\"logit\"),\n                 data = stats_test)\nsummary(log_stats)\n```\n\n\n\n\n\n## Vier Arten von Ergebnisse von Klassfikationen\n\n```{r class-stats1, echo = FALSE}\ndf <- readr::read_csv(\"../includes/class_results.csv\")\n\npander(df, caption = \"Vier Arten von Ergebnisse von Klassfikationen\")\n```\n\n\n\n## Konfusionsmatrix\n\n```{r}\n(cm <- SDMTools::confusion.matrix(Aktien$Aktienkauf, \n                                  glm1$fitted.values)) \nsensitivity(cm); specificity(cm)\n```\n\n\n## Vier Arten von Ergebnissen einer Klassifikation\n\n\n```{r class-stats, echo = FALSE}\n\ndf <- readr::read_csv(\"../includes/diag_stats.csv\")\n\nknitr::kable(df[-3], caption = \"Geläufige Kennwerte der Klassifikation\")\n```\n\n\n\n\n\n## ROC-Kurven\n\n```{r}\nlets_roc <- roc(Aktien$Aktienkauf, glm1$fitted.values)\nplot(lets_roc)\n\n```\n\n\n\n## Beispiele für ROC-Kurven\n\n\n```{r example-rocs, echo = FALSE, fig.cap = \"Beispiel für eine sehr gute (A), gute (B) und schlechte (C) Klassifikation\"}\n\nlibrary(plotROC)\nlibrary(gridExtra)\nD.ex <- rbinom(200, size = 1, prob = .5)\nM1 <- rnorm(200, mean = D.ex, sd = .3)\nM2 <- rnorm(200, mean = D.ex, sd = 1.5)\nM3 <- rnorm(200, mean = D.ex, sd = 10)\n\n\ntest <- data.frame(D = D.ex, D.str = c(\"Healthy\", \"Ill\")[D.ex + 1], \n                   M1 = M1, M2 = M2, stringsAsFactors = FALSE)\n\n\np1 <- ggplot(test, aes(d = D, m = M1)) + geom_roc(labels = FALSE) + style_roc() + ggtitle(\"A\")\np2 <- ggplot(test, aes(d = D, m = M2)) + geom_roc(labels = FALSE) + style_roc() + ggtitle(\"B\")\np3 <- ggplot(test, aes(d = D, m = M3)) + geom_roc(labels = FALSE) + style_roc() + ggtitle(\"C\")\n\ngrid.arrange(p1, p2, p3, nrow = 1)\n\n```\n\n\n\n# Clusteranalyse\n\n\n## Lernziele\n\n- Das Ziel einer Clusteranalyse erläutern können.\n- Das Konzept der euklidischen Abstände verstehen.\n- Eine k-Means-Clusteranalyse berechnen und interpretieren können.\n\n\n\n## Clustern Sie diesen Datensatz!\n\n```{r cluster-intuition, echo = FALSE}\n\n\nset.seed(2014)\ncenters <- data.frame(cluster=factor(1:3), size=c(100, 150, 50), x1=c(5, 0, -3), x2=c(-1, 1, -2))\npoints <- centers %>% group_by(cluster) %>%\n    do(data.frame(x1=rnorm(.$size[1], .$x1[1]),\n                  x2=rnorm(.$size[1], .$x2[1])))\n\nlibrary(ggplot2)\np1 <- ggplot(points, aes(x1, x2)) + geom_point() +\n  xlab(\"Lernzeit\") + ylab(\"Klasurpunkte\")\n\np2 <- ggplot(points, aes(x1, x2, color=cluster)) + geom_point() +\n  xlab(\"Lernzeit\") + ylab(\"Klasurpunkte\")\n```\n\n\n```{r cluster1, echo = FALSE, fig.cap = \"Ein Streudiagramm - sehen Sie Gruppen (Cluster) ?\"}\np1\n```\n\n## Intuitive Darstellung der Clusteranalayse\n\n\n```{r cluster2, echo = FALSE, fig.cap = \"Ein Streudiagramm - mit drei Clustern\"}\np2\n```\n\n\n## Unterschiedliche Anzahlen von Clustern im Vergleich\n\n\n```{r cluster3, echo = FALSE, fig.cap = \"Unterschiedliche Anzahlen von Clustern im Vergleich\"}\n\n\npoints.matrix <- cbind(x1 = points$x1, x2 = points$x2)\nkclust <- kmeans(points.matrix, 3)\nkclusts <- data.frame(k=1:9) %>% group_by(k) %>% do(kclust=kmeans(points.matrix, .$k))\n\nclusters <- kclusts %>% group_by(k) %>% do(tidy(.$kclust[[1]]))\nassignments <- kclusts %>% group_by(k) %>% do(augment(.$kclust[[1]], points.matrix))\nclusterings <- kclusts %>% group_by(k) %>% do(glance(.$kclust[[1]]))\n\np3 <- ggplot(assignments, aes(x1, x2)) + geom_point(aes(color=.cluster)) + facet_wrap(~ k)\n\n\np4 <- p3 + geom_point(data=clusters, size=10, shape=\"x\")\np4\n```\n\n\n## Wie groß ist der \"Abstand\" zwischen Anna und Berta?\n\n```{r distanz, echo = FALSE, fig.cap = \"Distanz zwischen zwei Punkten in der Ebene\", out.width = \"50%\"}\n\nknitr::include_graphics(\"../images/cluster/distanz_crop.png\")\n```\n\n\n## Pythagoras strikes back\n\n\n$$c^2 = a^2 + b^2$$\n\nIn unserem Beispiel heißt das $c^2 = 3^2+4^2 = 25$. Folglich ist $\\sqrt{c^2}=\\sqrt{25}=5$. Der Abstand oder der Unterschied zwischen Anna und Berta beträgt also 5 - diese Art von \"Abstand\" nennt man den *euklidischen Abstand*.\n\n\n## Pythagoras in 3D\n\n\n```{r pythagoras2, echo = FALSE, fig.cap = \"Pythagoras in 3D\", out.width = \"50%\"}\n\nknitr::include_graphics(\"../images/cluster/pythagoras2_crop.png\")\n```\n\n\n## Pythagoras in Reihe geschaltet\n\n\n```{r pythagoras, echo = FALSE, fig.cap = \"Pythagoras in Reihe geschaltet\"}\n\nknitr::include_graphics(\"../images/cluster/pythagoras_crop.png\")\n```\n\n\n\n\n## k-Means Clusteranalyse\n\n```{r echo = FALSE, results = \"hide\"}\nset.seed(1896)\nsegment <- read.csv2(\"../data/segment.csv\")\n\nsegment.num <- segment %>%\n  mutate(Frau = Geschlecht == \"Frau\") %>%\n  mutate(Eigenheim = Eigenheim == \"Ja\") %>%\n  mutate(Mitgliedschaft = Mitgliedschaft == \"Ja\") %>%\n  dplyr::select(-Geschlecht, -Segment)\nseg.k <- kmeans(segment.num, centers = 4, nstart = 10)\nseg.k\n\n```\n\n\n```{r echo = FALSE}\nclusplot(segment.num, seg.k$cluster, \n         color = TRUE, shade = TRUE, labels = 4)\n```\n\n# Dimensionsreduktion\n\n\n## Lernziele\n\n\n- Den Unterschied zwischen einer Hauptkomponentenanalyse und einer Exploratorische Faktorenanalyse kennen\n- Methoden kennen, um die Anzahl von Dimensionen zu bestimmen\n- Methoden der Visualisierung anwenden können\n- Umsetzungsmethoden in R anwenden können\n- Ergebnisse interpretieren können.\n\n\n\n## PCA vs. EFA\n\n- Die *Hauptkomponentenanalyse* (engl. principal component analysis, PCA) \n    - reduziert Daten\n    - erklärt die Gesamtvarianz\n\n- Die *Exploratorische Faktorenanalyse (EFA)* \n    - führt manifeste Variablen (Items) auf latente Faktoren zurück\n    - erklärt nicht die komplette Varianz, sondern nur die Varianz, die durch die vorhandenen Variablen erklärt wird\n  \n    \n    \n\n## Nutzen der Dimensionsreduktion\n\n- *Dimensionen reduzieren*\n\n- *Unsicherheit verringern*\n\n- *Aufwand verringern*\n\n\n## Intuition zur Dimensionsreduktion\n\n\n```{r fig-scatter3d, echo = FALSE, fig.cap = \"Der Pfeil ist eindimensional; reduziert also die drei Dimensionen auf eine\"}\n\nlibrary(MASS)\nlibrary(plot3D)\n\n\nmu3d <- c(0, 0,0 )\nsigma3d <- c(17, 16, 16, 16, 17, 14, 16, 14, 17) %>% matrix(nrow = 3)\n\n\n\ncorvars3d <- MASS::mvrnorm(n=100, mu=mu3d, Sigma=sigma3d) %>% as_tibble\n\n\nlm3d <- lm(V3 ~ V1+V2, data = corvars3d)\n\n\n\n\n\nadd_3dline <- function(){\n  \n  arrows3D(x0 =  -10, y0 = -10 , z0 = -10, \n           x1 = 7, y1 = 7, z1 = 7*1.31+7*-.41,\n           add = TRUE)\n}\n\n\n\nop <- par(mfrow = c(2,2),\n          oma = c(5,4,0,0) + 0.1,\n          mar = c(0,0,1,1) + 0.1)\n\nscatter3D(corvars3d$V1, corvars3d$V2, corvars3d$V2, corvars3d$V3, theta = 15, phi = 20)\nadd_3dline()\n\nscatter3D(corvars3d$V1, corvars3d$V2, corvars3d$V2, corvars3d$V3, theta = 15, phi = 20)\nadd_3dline()\n\nscatter3D(corvars3d$V1, corvars3d$V2, corvars3d$V2, corvars3d$V3, theta = 190, phi = 20)\nadd_3dline()\n\nscatter3D(corvars3d$V1, corvars3d$V2, corvars3d$V2, corvars3d$V3, theta = 100, phi = 60)\nadd_3dline()\n\npar(op)\n\n\n```\n\n\n\n\n\n\n\n\n\n## Datensatz `Werte` -- z-transformiert\n\n```{r echo = FALSE}\nWerte <- read.csv2(\"../data/Werte.csv\")\n\nWerte %>% scale %>% as_tibble -> Werte.sc\n```\n\n\n\n```{r echo = FALSE}\nWerte.sc %>% \n  dplyr::select(1:5) %>% \n  slice(1:5) %>% \n  knitr::kable()\n\n```\n\n\n## Korrelationsplot\n\n\n```{r out.width = \"50%\"}\ncorrplot::corrplot(cor(Werte.sc), order = \"hclust\")\n```\n\n\n## PCA berechnen\n\n```{r werte-prcomp, results = \"hide\"}\nWerte.pc <- prcomp(Werte.sc)  # Principal Components berechnen\nsummary(Werte.pc)\n\nGesamtvarianz <- sum(Werte.pc$sdev^2)\n\n# Varianzanteil der ersten Hauptkomponente\nWerte.pc$sdev[1]^2 / Gesamtvarianz\n```\n\n\n## Scree-Plot\n\n\n```{r pca-scree, fig.cap = \"Screeplot\"}\nplot(Werte.pc, type=\"l\")\n```\n\n\n## Eigenwert-Kriterium\n\nDer *Eigenwert*\\index{Eigenwert} ist eine Metrik für den Anteil der erklärten Varianz pro Hauptkomponente. \n\n```{r show-eigenvalues, results = \"hide\"}\neigen(cor(Werte))\n```\n\n\nLaut dem Eigenwert-Kriterium sollen nur Faktoren mit einem *Eigenwert größer 1* extrahiert werden.\n\n\n\n## Screeplot\n\n```{r vss-scree, fig.cap = \"VSS-Screeplot\"}\nVSS.scree(Werte)\n```\n\n\n## Biplot\n\n\n```{r out.width = \"50%\"}\nbiplot(Werte.pc)\n```\n\n\n## Ladungen der Items auf die Hauptkomponenten\n\n\n```{r results = \"hide\"}\nWerte.pca <- principal(Werte, nfactors = 5, rotate = \"none\")\n\n```\n\n\n## Pfaddiagramm der Ladungen auf die Hauptkomponenten\n```{r out.width = \"50%\"}\nfa.diagram(Werte.pca)\n```\n\n\n## Rotation\n```{r rotation, echo = FALSE, fig.cap = \"Beispiel für eine rechtwinklige Rotation\"}\n\nknitr::include_graphics(\"../images/dimred/rotation.png\")\n\n```\n\n\n## Heatmap\n\n\n```{r efa-heatmap, echo = FALSE, fig.cap = \"Heatmap einer EFA\"}\nWerte.fa<-factanal(Werte, factors = 5)\n\ngplots::heatmap.2(Werte.fa$loadings,\n          dendrogram = \"both\",\n          labRow = NULL,\n          labCol = NULL,\n          cexRow=1,\n          cexCol=1,\n          margins = c(7,7),\n          trace = \"none\",\n          #lmat = rbind(c(0,0),c(0,1)),\n          lhei = c(1,4),\n          keysize=0.75, \n          key.par = list(cex=0.5)\n          )\n```\n\n\n## Faktorwerte\n\n```{r eval = FALSE}\nWerte.ob <- factanal(Werte, factors = 5, scores = \"Bartlett\")\n```\n\n\n## Interne Konsistenz der Skalen\nInhaltlich ist Alpha eine Art mittlere Korrelation, die sich ergibt wenn man alle Items (paarweise) miteinander korrliert: I1-I2, I1-I3,...\n\n```{r eval = FALSE}\nalpha(Werte[, c(\"W12\",\"W13\", \"W14\", \"W15\")], check.keys = TRUE)\n\n```\n\n## Faustregeln zur Höhe von Cronbachs Alpha\n\nAlpha      |   Bedeutung    \n-----------|--------------\ngrößer 0,9 |   exzellent     \ngrößer 0,8 |   gut     \ngrößer 0,7 |   akzeptabel     \ngrößer 0,6 |   fragwürdig     \ngrößer 0,5 |   schlecht     \n\n\n# Textmining\n\n\n## Lernziele\n\n\n\n- Sie kennen zentrale Ziele und Begriffe des Textminings.\n- Sie wissen, was ein 'tidy text dataframe' ist.\n- Sie können Worthäufigkeiten auszählen.\n- Sie können Worthäufigkeiten anhand einer Wordcloud visualisieren.\n\n\n## Zentrale Begriffe\n\n\n\n- Ein *Corpus* bezeichnet die Menge der zu analyisierenden Dokumente-\n\n  - Ein *Token* (*Term*) ist ein elementarer Baustein eines Texts, die kleinste Analyseeinheit, häufig ein Wort.\n\n- Unter *tidy text* versteht man einen Dataframe, in dem pro Zeile nur ein Term steht.\n\n\n## Tidytext -- Input\n\n```{r}\ntext <- c(\"Wir haben die Frauen zu Bett gebracht,\",\n          \"als die Männer in Frankreich standen.\",\n          \"Wir hatten uns das viel schöner gedacht.\",\n          \"Wir waren nur Konfirmanden.\")\ntext_df <- data_frame(Zeile = 1:4,\n                      text = text)\n```\n\n## Tidytext -- Output\n\n```{r}\ntext_df %>%\n  unnest_tokens(output = wort, input = text) -> tidytext_df\n\ntidytext_df %>% head\n```\n\n>   In einem 'tidy text Dataframe'  steht in jeder Zeile ein Wort (token) und die Häufigkeit des Worts\nim Dokument.\n\n\n```{r echo = FALSE}\nafd_url <- paste0(\"https://www.alternativefuer.de\",\n                  \"/wp-content/uploads/sites/7/2016/05/\",\n                  \"2016-06-27_afd-grundsatzprogramm_web-version.pdf\")\n\nafd_url <- \"../data/2016-06-27_afd-grundsatzprogramm_web-version.pdf\"\n\nafd_pfad <- \"../data/afd_programm.pdf\"\n\n# download(afd_url, afd_pfad)\n\nafd_raw <- pdftools::pdf_text(afd_pfad)\nafd_df <- data_frame(Zeile = 1:96,\n                     afd_raw)\n\n\nafd_df %>%\n  unnest_tokens(token, afd_raw) %>%\n  filter(str_detect(token, \"[a-z]\")) -> afd_df\n\ncount(afd_df)\n\n```\n\n\n\n## Worthäufigkeiten auszählen\n\n```{r}\nafd_df %>%\n  na.omit() %>%  # fehlende Werte löschen\n  count(token, sort = TRUE) %>%\n  head\n```\n\n## Stopwörter entfernen\n\n```{r echo = FALSE}\ndata(stopwords_de)\n\nstopwords_de <- data_frame(word = stopwords_de)\n\nstopwords_de <- stopwords_de %>%\n  rename(token = word)\n\nafd_df %>%\n  anti_join(stopwords_de) -> afd_df\n\nafd_df %>%\n  count(token, sort = TRUE) -> afd_count\n\n\n```\n\n\n```{r echo = FALSE}\nafd_count %>%\n  top_n(5) %>%\n  knitr::kable()\n```\n\n\n\n\n\n# Anhang\n",
    "created" : 1496762457799.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "136459389",
    "id" : "1B4C7198",
    "lastKnownWriteTime" : 1497023151,
    "last_content_update" : 1497023151490,
    "path" : "~/Documents/Publikationen/In_Arbeit/Praxis_der_Datenanalyse/slides/PraDa_Folien.Rmd",
    "project_path" : "PraDa_Folien.Rmd",
    "properties" : {
        "docOutlineVisible" : "1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}