{
    "collab_server" : "",
    "contents" : "```{r include=FALSE, cache=FALSE}\nset.seed(1014)\noptions(digits = 3)\n\nknitr::opts_chunk$set(\n  comment = \"#>\",\n  message = FALSE,\n  warning = FALSE,\n  collapse = TRUE,\n  cache = TRUE,\n  out.width = \"70%\",\n  fig.align = 'center',\n  fig.width = 6,\n  fig.asp = 0.618,  # 1 / phi\n  fig.show = \"hold\"\n)\n\n```\n\n\n\\part{Geleitetes Modellieren}\n\n\n\n\n# Lineare Regression\n\n\n```{r echo = FALSE, out.width = \"30%\", fig.align = \"center\"}\nknitr::include_graphics(\"images/FOM.jpg\")\n```\n\n```{r echo = FALSE, out.width = \"10%\", fig.align = \"center\"}\nknitr::include_graphics(\"images/licence.png\")\n```\n\n\n```{block2, ziele-regression, type='rmdcaution', echo = TRUE} \nLernziele:\n\n\n- Wissen, was man unter Regression versteht.\n- Die Annahmen der Regression überprüfen können.\n- Regression mit kategorialen Prädiktoren durchführen können.\n- Die Modellgüte bei der Regression bestimmen können.\n- Interaktionen erkennen und ihre Stärke einschätzen können.\n\n\n\n```\n\n\nFür dieses Kapitel benötigen Sie folgende Pakete:\n```{r libs-regr}\nlibrary(caret)  # Modellieren\nlibrary(tidyverse)  # Datenjudo, Visualisierung,...\nlibrary(gridExtra)  # Mehrere Plots kombinieren\nlibrary(modelr)  # Residuen und Schätzwerte zum Datensatz hinzufügen\nlibrary(broom)  # Regressionswerte geordnet ausgeben lassen\n```\n\n\n## Die Idee der klassischen Regression\n\nRegression\\index{Regression} ist eine bestimmte Art der *Modellierung* von Daten. Wir legen eine Gerade 'schön mittig' in die Daten; damit haben wir ein einfaches Modell der Daten (vgl. Abb. \\@ref(fig:bsp-regression)). Die Gerade 'erklärt' die Daten: Für jeden X-Wert liefert sie einen Y-Wert als Vorhersage zurück.\n\n```{r bsp-regression, fig.cap = \"Beispiel für eine Regression\"}\nstats_test <- read.csv(\"data/test_inf_short.csv\")\n\nstats_test %>% \n  ggplot +\n  aes(x = study_time, y = score) +\n  geom_jitter() +\n  geom_abline(intercept = 24, \n              slope = 2.3, \n              color = \"red\")\n\n```\n\nWie wir genau die Regressionsgerade berechnet haben, dazu gleich mehr. Fürs Erste begnügen wir uns mit der etwas groberen Beobachtung, dass die Gerade 'schön mittig' in der Punktewolke liegt. \n\nSchauen wir uns zunächst die Syntax genauer an.\n\n```{block2, pseudo-regression, type='rmdpseudocode', echo = TRUE} \n\nLade die CSV-Datei mit den Daten als `stats_test`.\n  \nNehme `stats_test` UND DANN...  \nstarte ein neues Diagramm mit ggplot  \ndefiniere das Diagramm (X-Achse, Y-Achse)  \nzeichne das Geom \"Jitter\" (verwackeltes Punktediagramm)  \nund zeichne danach eine Gerade (\"abline\" in rot). \n\n\n```\n\nEine Regression zeigt anhand einer Regressionsgeraden einen \"Trend\" in den Daten an (s. weitere Beispiele in Abb. \\@ref(fig:bsp-regression2)).\n\n```{r bsp-regression2, echo = FALSE, fig.cap = \"Zwei weitere Beispiele für Regressionen\"}\n\nstats_test %>% \n   ggplot +\n  aes(x = self_eval, y = score) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) -> p1\n\n\nstats_test %>% \n   ggplot +\n  aes(x = interest, y = score) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) -> p2\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n```\n\n\nEine Regression lädt förmlich dazu ein, Vorhersagen zu treffen: Hat man erstmal eine Gerade, so kann man für jeden X-Wert (\"Prädiktor\") eine Vorhersage für den Y-Wert (\"Kriterium\") treffen. Anhand des Diagramms kann man also für jede Person (d.h. jeden Wert innerhalb des Wertebereichs von `study_time` oder einem anderen Prädiktor) einen Wert für `score` vorhersagen. Wie gut die Vorhersage ist, steht erstmal auf einen anderen Blatt.\n\nMan beachte, dass eine Gerade über ihre *Steigung* und ihren *Achsenabschnitt* festgelegt ist; in Abb. \\@ref(fig:bsp-regression) ist die Steigung 2.3 und der Achsenabschnitt 24.\n\nDer Achsenabschnitt zeigt also an, wie viele Klausurpunkte man \"bekommt\", wenn man gar nicht lernt (Gott bewahre); die Steigung gibt eine Art \"Wechselkurs\" an: Wie viele Klausurpunkte bekomme ich pro Stunde, die ich lerne. \n\nUnser Modell ist übrigens einfach gehalten: Man könnte argumentieren, dass der Zusatznutzen der 393. Stunde lernen geringer ist als der Zusatznutzen der ersten paar Stunden. Aber dann müssten wir anstelle der Gerade eine andere Funktion nutzen, um die Daten zu modellieren. Lassen wir es erst einmal einfach hier.\n\nAls \"Pseudo-R-Formel\" ausgedrückt: \n```\nscore = achsenabschnitt + steigung*study_time\n```\n\nDie Vorhersage für die Klausurpunkte (`score`) einer Person sind der Wert des Achsenabschnitts plus das Produkt aus der Anzahl der gelernten Stunden mal den Zusatznutzen pro gelernter Stunde.\n\n\nAber wie erkannt man, ob eine Regression \"gut\" ist - die Vorhersagen also präzise?\n\nIn R kann man eine Regression so berechnen:\n\n```{r lm1-stats-test}\nlm(score ~ study_time, data = stats_test)\n```\n\n`lm` steht dabei für \"lineares Modell\"; allgemeiner gesprochen lautet die Rechtschreibung für diesen Befehl:\n\n```\nlm(kriterium ~ praediktor, data = meine_datentabelle)\n```\n\nUm ausführlichere Informationen über das Regressionsmodell zu bekommen, kann man die Funktion `summary` nutzen:\n\n\n```{r eval = FALSE}\nmein_lm <- lm(kriterium ~ praediktor, data = meine_datentabelle)\nsummary(mein_lm)\n```\n\n\nNatürlich kann das auch ~~in der Pfeife rauchen~~ mit der Pfeife darstellen:\n\n```\nlm(kriterium ~ praediktor, data = meine_datentabelle) %>% \n  summary\n```\n\n\n## Vorhersagegüte\n\nDer einfache Grundsatz lautet: Je geringer die Vorhersagefehler, desto besser; Abb. \\@ref(fig:resids-plot) zeigt ein Regressionsmodell mit wenig Vorhersagefehler (links) und ein Regressionsmodell mit viel Vorhersagefehler (rechts).\n\n```{r resids-plot, echo = FALSE, results = \"hold\", fig.cap = \"Geringer (links) vs. hoher (rechts) Vorhersagefehler\"}\n\nset.seed(42)  \nN      <- 100\nbeta   <- 0.4\nintercept <- 1\n\n\nsim <- data_frame(\n  x = rnorm(N),\n  error1 = rnorm(N, mean = 0, sd = .5),\n  error2 = rnorm(N, mean = 0, sd = 2),\n  y1 = intercept + x*beta + error1,\n  y2 = intercept + x*beta + error2,\n  pred = 1 + x*beta\n)\n\n\n\np1 <- ggplot(sim, aes(x, y1)) + \n  geom_abline(intercept = intercept, slope = beta, colour = \"red\") +\n  geom_point(colour = \"#00998a\") +\n  geom_linerange(aes(ymin = y1, ymax = pred), colour = \"grey40\") +\n  ylim(-6,+6)\n\n\np2 <- ggplot(sim, aes(x, y2)) + \n  geom_abline(intercept = intercept, slope = beta, colour = \"red\") +\n  geom_point(colour = \"#00998a\") +\n  geom_linerange(aes(ymin = y2, ymax = pred), colour = \"grey40\") +\n  ylim(-6,+6)\n\n\ngrid.arrange(p1, p2, ncol = 2)\n\n```\n\n\nIn einem Regressionsmodell lautet die grundlegenden Überlegung zur Modellgüte damit:\n\n>    Wie groß ist der Unterschied zwischen Vorhersage und Wirklichkeit?\n\nDie Größe des Unterschieds (Differenz, \"Delta\") zwischen vorhergesagten (geschätzten) Wert und Wirklichkeit, bezeichnet man als *Fehler*, *Residuum* oder Vohersagefehler, häufig mit $\\epsilon$ (griechisches e wie \"error\") abgekürzt.\n\n\nBetrachten Sie die beiden Plots in Abb. \\@ref(fig:resids-plot). Die rote Linie gibt die *vorhergesagten* (geschätzten) Werte wieder; die Punkte die *beobachteten* (\"echten\") Werte. Je länger die blauen Linien, desto größer die Vorhersagefehler. Je größer der Vorhersagefehler, desto schlechter. Und umgekehrt.\n\n>   Je kürzer die typische \"Abweichungslinie\", desto besser die Vohersage.\n\n\nSagt mein Modell voraus, dass Ihre Schuhgröße 49 ist, aber in Wahrheit liegt sie bei 39, so werden Sie dieses Modell als schlecht beurteilen, wahrscheinlich.\n\nLeider ist es nicht immer einfach zu sagen, wie groß der Fehler sein muss, damit das Modell als \"gut\" bzw. \"schlecht\" gilt. Man kann argumentieren, dass es keine wissenschaftliche Frage sei, wie viel \"viel\" oder \"genug\" ist [@uncertainty]. Das ist zwar plausibel, hilft aber nicht, wenn ich eine Entscheidung treffen muss. Stellen Sie sich vor: Ich zwinge Sie mit der Pistole auf der Brust, meine Schuhgröße zu schätzen.\n\nEine einfache Lösung ist, das beste Modell unter mehreren Kandidaten zu wählen.\n\nEin anderer Ansatz ist, die Vorhersage in Bezug zu einem Kriterium zu setzen. Dieses \"andere Kriterium\" könnte sein \"einfach die Schuhgröße raten\". Oder, etwas intelligenter, Sie schätzen meine Schuhgröße auf einen Wert, der eine gewisse Plausibilität hat, also z.B. die durchschnittliche Schuhgröße des deutschen Mannes. Auf dieser Basis kann man dann quantifizieren, ob und wieviel besser man als dieses Referenzkriterium ist.\n\n### Mittlere Quadratfehler\nEine der häufigsten Gütekennzahlen ist der *mittlere quadrierte Fehler* (engl. \"mean squared error\", MSE), wobei Fehler wieder als Differenz zwischen Vorhersage (`pred`) und beobachtete Wirklichkeit (`obs`, `y`) definiert ist. Dieser berechnet für jede Beobachtung den Fehler, quadriert diesen Fehler und bilden dann den Mittelwert dieser \"Quadratfehler\", also einen *mittleren Quadratfehler*. Die englische Abkürzung *MSE* ist auch im Deutschen gebräuchlich.\n\n$$ MSE = \\frac{1}{n} \\sum{(pred - obs)^2} $$\n\nKonzeptionell ist dieses Maß an die Varianz angelehnt. Zieht man aus diesem Maß die Wurzel, so erhält man den sog. *root mean square error* (RMSE), welchen man sich als die Standardabweichung der Vorhersagefehler vorstellen kann. In Pseudo-R-Syntax:\n\n```\nRMSE <- sqrt(mean((df$pred - df$obs)^2))\n```\n\nDer RMSE hat die selben Einheiten wie die zu schätzende Variable, also z.B. Schuhgrößen-Nummern.\n\n\n\n\n### R-Quadrat ($R^2$)\n$R^2$, auch *Bestimmtheitsmaß*\\index{Bestimmtheitsmaß} oder *Determinationskoeffizient*\\index{Determinationskoeffizient} genannt, setzt die Höhe unseres Vorhersagefehlers\\index{Vorhersagefehler} im Verhältnis zum Vorhersagefehler eines \"Nullmodell\". Das Nullmodell hier würde sagen, wenn es sprechen könnte: \"Keine Ahnung, was ich schätzen soll, mich interessieren auch keine Prädiktoren, ich schätzen einfach immer den Mittelwert der Grundgesamtheit!\".\n\nAnalog zum Nullmodell-Fehler spricht auch von der Gesamtvarianz oder $SS_T$ (sum of squares total); beim Vorhersagefehler des eigentlichen Modells spricht man auch von $SS_M$ (sum of squares model).\n\nDamit gibt $R^2$ an, wie gut unsere Vorhersagen im Verhältnis zu den Vorhersagen des Nullmodells sind. Ein $R^2$ von 25% (0.25) hieße, dass unser Vorhersagefehler 25% *kleiner* ist als der der Nullmodells. Ein $R^2$ von 100% (1) heißt also, dass wir den kompletten Fehler reduziert haben (Null Fehler übrig) - eine perfekte Vorhersage. Etwas formaler, kann man $R^2$ so definieren:\n\n$$ R^2 = 1 - \\left( \\frac{SS_T - SS_M}{SS_T} \\right)$$\n\nPräziser, in R-Syntax:\n\n```\nR2 <- 1 - sum((df$pred - df$obs)^2) / sum((mean(df$obs) - df$obs)^2)\n```\n\n\nPraktischerweise gibt es einige R-Pakete, z.B. `caret`, die diese Berechnung für uns besorgen:\n\n```{r R2-caret, eval = FALSE}\npostResample(obs = obs, pred = pred)\n```\n\nHier steht `obs` für beobachtete Werte und `pred` für die vorhergesagten Werte (beides numerische Vektoren). Dieser Befehl gibt sowohl RMSE als auch $R^2$ wieder.\n\n\n\n\n```{block2, r-nicht-als-guete, type='rmdcaution', echo = TRUE}\n\nVerwendet man die Korrelation (r) oder $R^2$ als Gütekriterium, so sollte man sich über folgenden Punkt klar sein. Bei Skalierung der Variablen ändert sich die Korrelation nicht; das gilt auch für $R^2$. Beide Koeffizienten ziehen allein auf das *Muster* der Zusammenhänge ab - nicht die Größe der Abstände. Aber häufig ist die Größe der Abstände zwischen beobachteten und vorhergesagten Werten das, was uns interessiert. In dem Fall wäre der MSE vorzuziehen.\n\n```\n\n\n\n## Die Regression an einem Beispiel erläutert\n\nSchauen wir uns den Datensatz zur Statistikklausur noch einmal an. Welchen Einfluss hat die Lernzeit auf den Klausurerfolg? Wieviel bringt es also zu lernen? Wenn das Lernen keinen Einfluss auf den Klausurerfolg hat, dann kann man es ja gleich sein lassen... Aber umgekehrt, wenn es viel bringt, ok gut, dann könnte man sich die Sache (vielleicht) noch mal überlegen. Aber was heißt \"viel bringen\" eigentlich?\n\n>   Wenn für jede Stunde Lernen viele zusätzliche Punkte herausspringen, dann bringt Lernen viel. Allgemeiner: Je größer der Zuwachs im Kriterium ist pro zusätzliche Einheit des Prädiktors, desto größer ist der Einfluss des Prädiktors.\n\nNatürlich könnte jetzt jemand argumentieren, dass die ersten paar Stunden lernen viel bringen, aber dann flacht der Nutzen ab, weil es ja schnell einfach und trivial wird. Aber wir argumentieren (erstmal) so nicht. Wir gehen davon aus, dass jede Stunde Lernen gleich viel (oder wenig) Nutzen bringt.\n\n>   Geht man davon aus, dass jede Einheit des Prädiktors gleich viel Zuwachs bringt, unabhängig von dem Wert des Prädiktors, so geht man von einem linearen Einfluss aus.\n\n\nVersuchen wir im ersten Schritt die Stärke des Einfluss an einem Streudiagramm abzuschätzen (s. Abb. \\@ref(fig:bsp-regression)).\n\n\nHey R - berechne uns die \"Trendlinie\"! Dazu nimmt man den Befehl `lm`:\n\n\n```{r}\nmein_lm <- lm(score ~ study_time, data = stats_test)\nsummary(mein_lm)\n\n```\n\n`lm` steht für 'lineares Modell', eben weil eine *Linie* als Modell in die Daten gelegt wird. Aha. Die Steigung der Geraden beträgt 2.3 - das ist der Einfluss des Prädiktors Lernzeit auf das Kriterium Klausurerfolg! Man könnte sagen: Der \"Wechselkurs\" von Lernzeit auf Klausurpunkte. Für jede Stunde Lernzeit bekommt man offenbar 2.3 Klausurpunkte (natürlich viel zu leicht). Wenn man nichts lernt (`study_time == 0`) hat man 24 Punkte.\n\n>    Der Einfluss des Prädiktors steht unter 'estimate'. Der Kriteriumswert wenn der Prädiktor Null ist steht unter '(Intercept)'.\n\n\n\nMalen wir diese Gerade in unser Streudiagramm (Abbildung \\@ref(fig:stats-test-scatter2).\n\n\n```{r stats-test-scatter2, fig.cap = \"Streudiagramm von Lernzeit und Klausurerfolg\"}\nggplot(data = stats_test) +\n  aes(y = score, x = study_time) +\n  geom_jitter() +\n  geom_abline(slope = 2.3, intercept = 24, color = \"red\")\n```\n\nJetzt kennen wir die Stärke (und Richtung) des Einflusses der Lernzeit. Ob das viel oder wenig ist, ist am besten im Verhältnis zu einem Referenzwert zu sagen.\n\nDie Gerade wird übrigens so in die Punktewolke gelegt, dass die (quadrierten) Abstände der Punkte zur Geraden minimal sind. Dies wird auch als *Kriterium der Kleinsten Quadrate*\\index{Kriterium der Kleinsten Quadrate} (*Ordinary Least Squares*, *OLS*)\\index{Ordinary Least Squares} bezeichnet.\n\nJetzt können wir auch einfach Vorhersagen machen. Sagt uns jemand, ich habe \"viel\" gelernt (Lernzeit = 4), so können wir den Klausurerfolg grob im Diagramm ablesen.\n\nGenauer geht es natürlich mit dieser Rechnung:\n\n$y = 4*2.3 + 24$\n\nOder mit diesem R-Befehl:\n\n\n\n```{r}\npredict(mein_lm, data.frame(study_time = 4))\n```\n\n\n\nBerechnen wir noch die Vorversagegüte des Modells.\n\n```{r}\nsummary(mein_lm)\n```\n\n\nDas Bestimmtheitsmaß $R^2$ ist mit `r round(summary(mein_lm)$r.squared,2)` \"ok\": `r round(summary(mein_lm)$r.squared*100)`-\\% der Varianz des Klausurerfolg wird im Modell 'erklärt'. 'Erklärt' meint hier, dass wenn die Lernzeit konstant wäre, würde die Varianz von Klausurerfolg um diesen Prozentwert sinken.\n\n\n## Überprüfung der Annahmen der linearen Regression\n\nAber wie sieht es mit den Annahmen aus?\n\n- Die *Linearität des Zusammenhangs* haben wir zu Beginn mit Hilfe des Scatterplots überprüft. Es schien einigermaßen zu passen.\n\n- Zur Überprüfung der *Normalverteilung der Residuen* zeichnen wir ein Histogramm (s. Abbildung \\@ref(resid-distrib)). Die *Residuen*\\index{Residuen} können über den Befehl `add_residuals` (Paket `modelr`) zum Datensatz hinzugefügt werden. Dann wird eine Spalte mit dem Namen `resid` zum Datensatz hinzugefügt.  \n\nHier scheint es zu passen:\n\n```{r resid-distrib, fig.cap = \"Die Residuen verteilen sich hinreichend normal.\"}\nstats_test %>% \n  add_residuals(mein_lm) %>% \n  ggplot +\n  aes(x = resid) +\n  geom_histogram()\n```\n\n\nSieht passabel aus. Übrigens kann man das Paket `modelr` auch nutzen, um sich komfortabel die vorhergesagten Werte zum Datensatz hinzufügen zu lassen (Spalte `pred`):\n\n```{r}\nstats_test %>% \n  add_predictions(mein_lm) %>% \n  select(pred) %>% \n  head\n```\n\n\n- *Konstante Varianz*: Dies kann z. B. mit einem Scatterplot der Residuen auf der Y-Achse und den vorhergesagten Werten auf der X-Achse überprüft werden. Bei jedem X-Wert sollte die Varianz der Y-Werte (etwa) gleich sein (s. Abbildung \\@ref(fig:tips-preds-resid)).\n\nDie geschätzten (angepassten) Werte kann man über den Befehl `add_predictions()` aus dem Paket `modelr` bekommen. Die Fehlerwerte entsprechend mit dem Befehl `add_residuals`. \n\n```{r tips-preds-resid, fig.cap = \"Vorhergesagte Werte vs. Residualwerte im Datensatz tips\"}\n\nstats_test %>% \n  add_predictions(mein_lm) %>% \n  add_residuals(mein_lm) %>% \n  ggplot() +\n  aes(y = resid, x = pred) +\n  geom_point()\n\n```\n\n\nDie Annahme der konstanten Varianz scheint verletzt zu sein: Die sehr großen vorhersagten Werte können recht genau geschätzt werden; aber die mittleren Werte nur ungenau.\nDie Verletzung dieser Annahme beeinflusst *nicht* die Schätzung der Steigung, sondern die Schätzung des Standardfehlers, also des p-Wertes der Einflusswerte.\n\n- *Extreme Ausreißer*: Extreme Ausreißer scheint es nicht zu geben.\n\n- *Unabhängigkeit der Beobachtungen*: Wenn die Studenten in Lerngruppen lernen, kann es sein, dass die Beobachtungen nicht unabhängig voneinander sind: Wenn ein Mitglied der Lerngruppe gute Noten hat, ist die Wahrscheinlichkeit für ebenfalls gute Noten bei den anderen Mitgliedern der Lerngruppe erhöht. Böse Zungen behaupten, dass 'Abschreiben' eine Gefahr für die Unabhängigkeit der Beobachtungen sei.\n\n\n\n```{block2, tips-uebung, type='rmdexercises', echo = TRUE}\n1.  Wie groß ist der Einfluss des Interesss?\n\n2.  Für wie aussagekräftig halten Sie Ihr Ergebnis aus 1.?\n\n3.  Welcher Einflussfaktor (in unseren Daten) ist am stärksten?\n\n```\n\n\n\n## Regression mit kategorialen Prädiktoren\nVergleichen wir interessierte und nicht interessierte Studenten. Dazu teilen wir die Variable `interest` in zwei Gruppen (1-3 vs. 4-6) auf:\n\n```{r}\nstats_test$interessiert <- stats_test$interest > 3\n```\n\n\nVergleichen wir die Mittelwerte des Klausurerfolgs zwischen den Interessierten und Nicht-Interessierten:\n\n```{r}\nstats_test %>% \n  group_by(interessiert) %>% \n  summarise(score = mean(score)) -> score_interesse\n\nscore_interesse\n```\n\nAha, die Interessierten haben im Schnitt mehr Punkte; aber nicht viel.\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  ggplot() +\n  aes(x = interessiert, y = score) +\n  geom_jitter(width = .1) +\n  geom_point(data = score_interesse, color = \"red\", size = 5) +\n  geom_line(data = score_interesse, group = 1, color = \"red\")\n```\n\nMit `group=1` bekommt man eine Linie, die alle Punkte verbindet. Wir haben in dem Fall nur zwei Punkte, die entsprechend verbunden werden.\n\n\nVisualisieren Sie den Gruppenunterschied auch mit einem Boxplot.\n\n\nUnd als Lineares Modell:\n```{r}\nlm2 <- lm(score ~ interessiert, data = stats_test)\nsummary(lm2)\n```\n\nDer Einfluss von `interessiert` ist statistisch signifikant (p = .03). Der Stärke des Einflusses ist im Schnitt 1.6 Klausurpunkte (zugunsten `interessiertTRUE`). Das ist genau, was wir oben herausgefunden haben.\n\n\n```{block2, tips-uebung2, type='rmdexercises', echo = TRUE}\n3.  Vie ist der Einfluss von `study_time`, auch in zwei Gruppen geteilt?\n\n4.  Wie viel \\% der Variation des Klausurerfolgs können Sie durch das Interesse modellieren?\n\n```\n\n\n\n\n## Multiple Regression\nAber wie wirken sich mehrere Einflussgrößen *zusammen* auf den Klausurerfolg aus?\n\n\n```{r}\nlm3 <- lm(score ~ study_time + interessiert, data = stats_test)\nsummary(lm3)\n```\n\nInteressant ist das *negative* Vorzeichen vor dem Einfluss von `interessiertTRUE`! Die multiple Regression untersucht den 'Nettoeinfluss' jedes Prädiktors. Den Einfluss also, wenn der andere Prädiktor *konstant* gehalten wird. Anders gesagt: Betrachten wir jeden Wert von `study_time` separat, so haben die Interessierten jeweils im Schnitt etwas *weniger* Punkte (jesses). Allerdings ist dieser Unterschied nicht statistisch signifikant.\n\n>   Die multiple Regression zeigt den 'Nettoeinfluss' jedes Prädiktor: Den Einfluss dieses Prädiktor, wenn der andere Prädiktor oder die anderne Prädiktoren konstant gehalten werden.\n\n\nHier haben wir übrigens dem Modell aufgezwungen, dass der Einfluss von Lernzeit auf Klausurerfolg bei den beiden Gruppen gleich groß sein soll  (d.h. bei Interessierten und Nicht-Interessierten ist die Steigung der Regressionsgeraden gleich). Das illustriert sich am einfachsten in einem Diagramm (s. Abbildung \\@ref(fig:no-interakt)).\n\n\n```{r no-interakt, echo = FALSE, fig.cap = \"Eine multivariate Analyse fördert Einsichten zu Tage, die bei einfacheren Analysen verborgen bleiben\", fig.height = 7}\n\nlibrary(viridis)\n\ndf1 <- data_frame(\n  interessiert = c(T, F),\n  slope = c(2.3, 2.3),\n  intercept = c(23.7, 24)\n)\n\n\np1 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=study_time, y = score) +\n  geom_jitter(width = .1) +\n  #facet_wrap(~interessiert) +\n  geom_abline(data = df1, mapping = aes(slope = slope, intercept = intercept, color = interessiert)) +\n  guides(color = FALSE) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(title = \"A\") +\n  coord_cartesian(ylim = c(25,35))\n\n\ndf2 <- data_frame(\n  slope = rep(-.33, 5),\n  intercept = rep(c(24), 5),\n  study_time = 1:5)\n\n\np2 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=interessiert, y = score) +\n  geom_jitter(width = .1) +\n  facet_wrap(~study_time) +\n  geom_abline(data = df2, mapping = aes(slope = slope, intercept = intercept, color = study_time)) +\n  scale_color_viridis() +\n  guides(color = FALSE) +\n  labs(title = \"B\")\n\n\ngridExtra::grid.arrange(p1, p2, nrow = 2)\n```\n\n\nDiese *multivariate*\\index{multivariat} Analyse (mehr als 2 Variablen sind beteiligt) zeigt uns, dass die Regressionsgerade nicht gleich ist in den beiden Gruppen (Interessierte vs. Nicht-Interessierte; s. Abbildung \\@ref(fig:no-interakt)): Im Teildiagramm A sind die Geraden (leicht) versetzt. Analog zeigt Teildiagramm B, dass die Interessierten (`interessiert == TRUE`) geringe Punktewerte haben als die Nicht-Interessierten, wenn man die Werte von `study_time` getrennt betrachtet.\n\n>    Die multivariate Analyse zeigt ein anderes Bild, ein genaueres Bild als die einfachere Analyse. Ein Sachverhalt, der für den ganzen Datensatz gilt, kann in Subgruppen anders sein.\n\nOhne multivariate Analyse hätten wir dies nicht entdeckt. Daher sind multivariate Analysen sinnvoll und sollten gegenüber einfacheren Analysen bevorzugt werden.\n\nMan könnte sich jetzt noch fragen, ob die Regressiongeraden in Abbildung \\@ref(fig:no-interakt) parallel sein müssen. Gerade hat unser R-Befehl sie noch gezwungen, parallel zu sein. Gleich lassen wir hier die Zügel locker. Wenn die Regressionsgerade nicht mehr parallel sind, spricht man von *Interaktionseffekten*.\n\n\nDas Ergebnis des zugrunde-liegenden F-Tests (vgl. Varianzanalyse) wird in der letzten Zeile angegeben (`F-Statistic`). Hier wird $H_0$ also verworfen.\n\n\n\n\n## Interaktionen \n\nEs könnte ja sein, dass die Stärke des Einflusses von Lernzeit auf Klausurerfolg in der Gruppe der Interessierten anders ist als in der Gruppe der Nicht-Interessierten. Wenn man nicht interessiert ist, so könnte man argumentieren, dann bringt eine Stunden Lernen weniger als wenn man interessiert ist. Darum müssten die Steigungen der Regressiongeraden in den beiden Gruppen unterschiedlich sein. Schauen wir uns es an. Um R dazu zu bringen, die Regressiongeraden frei variieren zu lassen, so dass sie nicht mehr parallel sind, nutzen wir das Symbol `*`, dass wir zwischen die betreffenden Prädiktoren schreiben:\n\n```{r}\nlm4 <- lm(score ~ interessiert*study_time, data = stats_test)\nsummary(lm4)\n```\n\nInteressanterweise zeigen die Interessierten nun wiederum - betrachtet man jede Stufe von `study_time` einzeln - bessere Klausurergebnisse als die Nicht-Interessierten. Ansonsten ist noch die Zeile `interessiertTRUE:study_time` neu. Diese Zeile zeigt die Höhe des *Interaktionseffekts*\\index{Interaktionseffekts}. Bei den Interessierten ist die Steigung der Geraden um 0.32 Punkte geringer als bei den Interessierten. Der Effekt ist klein und nicht statistisch signifikant, so dass wir wahrscheinlich Zufallsrauschen überinterpretieren. Aber die reine Zahl sagt, dass bei den Interessierten jede Lernstunde weniger Klausurerfolg bringt als bei den Nicht-Interessierten. Auch hier ist eine Visualisierung wieder hilfreich. \n\n\n\n```{r interakt-stats-test, echo = FALSE, fig.cap = \"Eine Regressionsanalyse mit Interaktionseffekten\"}\n\n\n\ndf1 <- data_frame(\n  interessiert = c(F, T),\n  slope = c(2.44, 2.44-.32),\n  intercept = c(23.6, 23.6+.66)\n)\n\n\np1 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=study_time, y = score) +\n  geom_jitter(width = .1) +\n  #facet_wrap(~interessiert) +\n  geom_abline(data = df1, mapping = aes(slope = slope, intercept = intercept, color = interessiert)) +\n  guides(color = FALSE) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(title = \"A\") +\n  coord_cartesian(ylim = c(25,35))\n\n\n\ndf2 <- stats_test %>% \n  data_grid(\n    study_time = 1:5,\n    interessiert = c(F, T)\n  ) %>% \n  add_predictions(lm4)\n  \n\n\np2 <- stats_test %>% \n  na.omit %>% \n  ggplot +\n  aes(x=interessiert, group = study_time) +\n  geom_jitter(aes(y = score), width = .1) +\n  geom_line(data = df2, aes(y = pred, color = factor(study_time))) +\n  scale_color_viridis(discrete = TRUE) +\n  theme(legend.position = \"bottom\")\n  \n\n\ngridExtra::grid.arrange(p1, p2, nrow = 2)\n```\n\n\nWir wir in Abbildung \\@ref(fig:interakt-stats-test) sehen, ist der Einfluss von `study_time' je nach Gruppe (Wert von `interessiert`) unterschiedlich (Teildiagramm A). Analog ist der Einfluss des Interesses (leicht) unterschiedlich, wenn man die fünf Stufen von `study_time` getrennt betrachtet. \n\n>   Sind die Regressionsgerade nicht parallel, so liegt ein Interaktionseffekt vor. Andernfalls nicht.\n\n\n\n\n\n\n## Fallstudie zu Overfitting {#overfitting-casestudy}\n\nVergleichen wir im ersten Schritt eine Regression, die die Modellgüte anhand der *Trainingsstichprobe* schätzt mit einer Regression, bei der die Modellgüte in einer *Test-Stichprobe* überprüft wird.\n\n\nBetrachten wir nochmal die einfache Regression von oben. Wie lautet das $R^2$?\n\n```{r lm-overfitting1}\n\nlm1 <- lm(score ~ study_time, data = stats_test)\n\n```\n\nEs lautet `round(summary(lm1)$r.squared, 2)`.\n\nIm zweiten Schritt teilen wir die Stichprobe in eine Trainings- und eine Test-Stichprobe auf. Wir \"trainineren\" das Modell anhand der Daten aus der Trainings-Stichprobe:\n\n```{r lm-overfitting2}\ntrain <- stats_test %>% \n  sample_frac(.8, replace = FALSE)  # Stichprobe von 80%, ohne Zurücklegen\n\ntest <- stats_test %>% \n  anti_join(train)  # Alle Zeilen von \"df\", die nicht in \"train\" vorkommen\n\nlm_train <- lm(score ~ study_time, data = train)\n```\n\n\nDann testen wir (die Modellgüte) anhand der *Test*-Stichprobe. Also los, `lm_train`, mach Deine Vorhersage:\n\n```{r lm-overfitting-predict}\nlm2_predict <- predict(lm_train, newdata = test)\n```\n\nDiese Syntax sagt:\n\n```{block2, lm2-predict-block, type='rmdpseudocode', echo = TRUE}\nSpeichere unter dem Namen \"lm2_predict\" das Ergebnis folgender Berechnung:  \nMache eine Vorhersage (\"to predict\") anhand des Modells \"lm2\",   \nwobei frische Daten (\"newdata = test\") verwendet werden sollen. \n\n```\n\nAls Ergebnis bekommen wir einen Vektor, der für jede Beobachtung des Test-Samples den geschätzten (vorhergesagten) Klausurpunktewert speichert.\n\n```{r R2-postresample}\ncaret::postResample(pred = lm2_predict, obs = test$score)\n\n```\n\nDie Funktion `postResample` aus dem Paket `caret` liefert uns zentrale Gütekennzahlen unser Modell. Wir sehen, dass die Modellgüte im Test-Sample deutlich *schlechter* ist als im Trainings-Sample. Ein typischer Fall, der uns warnt, nicht vorschnell optimistisch zu sein!\n\n\n>   Die Modellgüte im in der Test-Stichprobe ist meist schlechter als in der Trainings-Stichprobe. Das warnt uns vor Befunden, die naiv nur die Werte aus der Trainings-Stichprobe berichten.\n\n\n## Aufgaben^[F, R, R, F, F, F, F, F, R]\n\n\n\n```{block2, exercises-regr, type='rmdexercises', echo = TRUE} \nRichtig oder Falsch!?\n\n1. X-Wert: Kriterium; Y-Wert: Prädiktor.\n\n1. Der Y-Wert in der einfachen Regression wird berechnet als Achsenabschnitt plus *x* mal die Geradensteigung.\n\n1. $R^2$ liefert einen *relativen* Vorhersagefehler und MSE einen *absoluten* (relativ im Sinne eines Anteils).\n\n1. Unter 'Ordinary Least Squares' versteht man eine abschätzige Haltung gegenüber Statistik.\n\n5. Zu den Annahmen der Regression gehört Normalverteilung der *Kriteriumswerte*.\n\n1. Die Regression darf nicht bei kategorialen Prädiktoren verwendet werden.\n\n1. Mehrere bivariate Regressionsanalysen (1 Prädiktor, 1 Kriterium) sind einer multivariaten Regression i.d.R. vorzuziehen.\n\n1. Interaktionen erkennt man daran, dass die Regressionsgeraden *nicht* parallel sind.\n\n```\n\n\n## Befehlsübersicht\n\nTabelle \\@ref(tab:befehle-regression) stellt die Befehle dieses Kapitels dar. \n\n\n```{r befehle-regression, echo = FALSE}\n\ndf <- readr::read_csv(\"includes/Befehle_Regression.csv\")\n\n\n\nlibrary(pander)\npander::cache.off()\npanderOptions(\"table.alignment.default\", \"left\")\npander::pander(data.frame(df), caption = \"Befehle des Kapitels 'Regression'\")\n\n# hier `pander`, weil `kable` keine breiten Zellen umbricht.\n\n\n\n```\n\n\n\n\n\n\n",
    "created" : 1497430392251.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1972052874",
    "id" : "5F6149C5",
    "lastKnownWriteTime" : 1497430333,
    "last_content_update" : 1497430333,
    "path" : "~/Documents/Publikationen/In_Arbeit/Praxis_der_Datenanalyse/071_Regression.Rmd",
    "project_path" : "071_Regression.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}