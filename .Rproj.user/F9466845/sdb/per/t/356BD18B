{
    "collab_server" : "",
    "contents" : "```{r include=FALSE, cache=FALSE}\nset.seed(1014)\noptions(digits = 3)\n\nknitr::opts_chunk$set(\n  comment = \"#>\",\n  collapse = TRUE,\n  message = FALSE,\n  warning = FALSE,\n  cache = TRUE,\n  out.width = \"70%\",\n  fig.align = 'center',\n  fig.width = 6,\n  fig.asp = 0.618,  # 1 / phi\n  fig.show = \"hold\"\n)\n\n```\n\n\n\n# Praxisprobleme der Datenaufbereitung\n\n\n\n\n\n```{block2, ziele-typ-probleme, type='rmdcaution', echo = TRUE} \nLernziele:\n\n- Typische Probleme der Datenaufbereitung kennen.\n- Typische Probleme der Datenaufbereitung bearbeiten können.\n\n```\n\n\nLaden wir zuerst die benötigten Pakete; v.a. ist das `dplyr` and friends. Das geht mit dem Paket `tidyverse`. \n\n```{r}\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(gridExtra)\nlibrary(car)\n```\n\n\nStellen wir einige typische Probleme des Datenjudo (genauer: der Datenaufbereitung) zusammen. Probleme heißt hier nicht, dass es etwas Schlimmes passiert ist, sondern es ist gemeint, wir schauen uns ein paar typische Aufgabenstellungen an, die im Rahmen der Datenaufbereitung häufig anfallen. \n\n\n## Datenaufbereitung\n\n\n### Auf fehlende Werte prüfen \nDas geht recht einfach mit `summarise(mein_dataframe)`. Der Befehl liefert für jede Spalte des Dataframe `mein_dataframe` die Anzahl der fehlenden Werte zurück.\n\n\n```{r read-stats-data}\nstats_test <- read.csv(\"data/test_inf_short.csv\")\nsummarise(stats_test)\n```\n\n\n### Fälle mit fehlenden Werte löschen\nWeist eine Variable (Spalte) \"wenig\" fehlende Werte auf, so kann es schlau sein, nichts zu tun. Eine andere Möglichkeit besteht darin, alle entsprechenden Zeilen zu löschen. Man sollte aber schauen, wie viele Zeilen dadurch verloren gehen.\n\n```{r}\n# Unsprünglich Anzahl an Fällen (Zeilen)\nnrow(stats_test)\n\n# Nach Umwandlung in neuen Dataframe\nstats_test %>%\n   na.omit -> stats_test_na_omit\nnrow(stats_test_na_omit)\n\n# Nur die Anzahl der bereinigten Daten\nstats_test %>%\n   na.omit %>%\n   nrow\n\n```\n\n\n```{block2, dplyr-no-brackets, type='rmdcaution', echo = TRUE} \nBei mit der Pfeife verketteten Befehlen darf man für Funktionen die runden Klammern weglassen, wenn man keinen Parameter schreibt. Also ist `nrow` (ohne Klammern) erlaubt bei `dplyr`, wo es eigentlich `nrow()` heißen müsste. Sie dürfen die Klammern natürlich schreiben, aber sie müssen nicht.\n\n```\n\n\n\n\nHier verlieren wir `r nrow(stats_test) - nrow(stats_test_na_omit)` Zeilen, das verschmerzen wir.\n\nWelche Zeilen verlieren wir eigentlich? Lassen wir uns nur die *nicht-*kompletten Fälle anzeigen (und davon nur die ersten paar):\n```{r}\nstats_test %>% \n   filter(!complete.cases(.)) %>% \n  head \n```\n\nMan beachte, dass der Punkt `.` für den Datensatz steht, wie er vom letzten Schritt weitergegeben wurde. Innerhalb einer dplyr-Befehls-Kette können wir den Datensatz, wie er im letzten Schritt beschaffen war, stets mit `.` ansprechen; ganz praktisch, weil schnell zu tippen. Natürlich könnten wir diesen Datensatz jetzt als neues Objekt speichern und damit weiter arbeiten. Das Ausrufezeichen `!` steht für logisches \"Nicht\". Mit `head` bekommt man nur die ersten paar Fälle (6 im Standard) angezeigt, was oft reicht für einen Überblick.\n\nIn Pseudo-Syntax liest es sich so:\n\n```{block2, no-na-filter, type='rmdpseudocode', echo = TRUE}\nNehme den Datensatz `stats_test` UND DANN...  \nfiltere die nicht-kompletten Fälle \n\n```\n\n\n### Fehlende Werte ggf. ersetzen  \nIst die Anzahl der fehlenden Werte zu groß, als dass wir es verkraften könnten, die Zeilen zu löschen, so können wir die fehlenden Werte ersetzen. Allein, das ist ein weites Feld und übersteigt den Anspruch dieses Kurses^[Das sagen Autoren, wenn sie nicht genau wissen, wie etwas funktioniert.]. Eine einfache, aber nicht die beste Möglichkeit, besteht darin, die fehlenden Werte durch einen repräsentativen Wert, z.B. den Mittelwert der Spalte, zu ersetzen.\n\n```{r}\nstats_test$interest <- replace(stats_test$interest, is.na(stats_test$interest),\n                         mean(stats_test$interest, na.rm = TRUE))\n\nsum(is.na(stats_test$interest))\n```\n\n`replace`^[aus dem \"Standard-R\", d.h. Paket \"base\".] ersetzt Werte aus dem Vektor `stats_test$interest` alle Werte, für die `is.na(stats_test$interest)` wahr ist, bei Zeilen mit fehlenden Werten in dieser Spalte also. Diese Werte werden durch den Mittelwert der Spalte ersetzt^[Hier findet sich eine ausführlichere Darstellung: https://sebastiansauer.github.io/checklist_data_cleansing/index.html].\n\n\n\n### Nach Fehlern suchen\nLeicht schleichen sich Tippfehler oder andere Fehler ein. Man sollte darauf prüfen; so könnte man sich ein Histogramm ausgeben lassen pro Variable, um \"ungewöhnliche\" Werte gut zu erkennen. Meist geht das besser als durch das reine Betrachten von Zahlen. Gibt es wenig unterschiedliche Werte, so kann man sich auch die unterschiedlichen Werte ausgeben lassen.\n\n```{r}\nstats_test %>% \n  count(interest) \n```\n\nDa in der Umfrage nur ganze Zahlen von 1 bis 5 abgefragt wurden, ist die `3.21...` auf den ersten Blick suspekt. In diesem Fall ist aber alles ok, da wir diesen Wert selber erzeugt haben.\n\n### Ausreißer identifizieren\n\nÄhnlich zu Fehlern, steht man Ausreißer häufig skeptisch gegenüber. Allerdings kann man nicht pauschal sagen, das Extremwerte entfernt werden sollen: Vielleicht war jemand in der Stichprobe wirklich nur 1.20m groß? Hier gilt es, begründet und nachvollziehbar im Einzelfall zu entscheiden. Histogramme und Boxplots sind wieder ein geeignetes Mittel, um Ausreißer zu finden (vgl. Abb. \\@ref(fig:fig-ausreisser)).\n\n```{r fig-ausreisser, fig.cap = \"Ausreißer identifizieren\"}\nqplot(x = score, data = stats_test, binwidth = 1)\n```\n\nMit `binwidth = 1` sagen wir, dass jeder Balken (bin) eine Breite (width) von 1 haben soll.\n\n### Hochkorrelierte Variablen finden\nHaben zwei Leute die gleiche Meinung, so ist einer von beiden überflüssig - wird behauptet. Ähnlich bei Variablen; sind zwei Variablen sehr hoch korreliert (>.9, als grober (!) Richtwert), so bringt die zweite kaum Informationszuwachs zur ersten. Und kann z.B. ausgeschlossen werden. \n\nNehmen wir dazu den Datensatz `extra` her.\n\n```{r}\nextra <- read.csv(\"data/extra.csv\")\n```\n\n\n```{r}\nextra %>% \n  select(i01:i10) %>% # Wähle die Variablen von i01 bis i10 aus\n  correlate() -> km   # Korrelationsmatrix berechnen\nkm  \n```\n\nIn diesem Beispiel sind keine Variablen sehr hoch korreliert. Wir leiten keine weiteren Schritte ein, abgesehen von einer Visualisierung.\n\n```{r fig-corrr, fig.cap = \"Ein Korrelationsplot\"}\n\nkm %>% \n  shave() %>% # Oberes Dreieck ist redundant, wird \"abrasiert\"\n  rplot()  # Korrelationsplot\n```\n\nDie Funktion `correlate` stammt aus dem Paket `corrr`^[https://github.com/drsimonj/corrr ], welches vorher installiert und geladen sein muss. Hier ist die Korrelation nicht zu groß, so dass wir keine weiteren Schritte unternehmen. Hätten wir eine sehr hohe Korrelation gefunden, so hätten wir eine der beiden beteiligten Variablen aus dem Datensatz löschen können.\n\n\n### z-Standardisieren\nFür eine Reihe von Analysen ist es wichtig, die Skalierung der Variablen zur vereinheitlichen. Die z-Standardisierung ist ein übliches Vorgehen. Dabei wird der Mittelwert auf 0 transformiert und die SD auf 1; man spricht - im Falle von (hinreichend) normalverteilten Variablen - jetzt von der *Standardnormalverteilung*\\index{Standardnormalverteilung}. Unterscheiden sich zwei Objekte A und B in einer standardnormalverteilten Variablen, so sagt dies nur etwas zur relativen Position von A zu B innerhalb ihrer Verteilung aus - im Gegensatz zu den Rohwerten.\n\n```{r}\nextra %>% \n  select(i01, i02r) %>%  \n  scale() %>%  # z-standardisieren\n  head()  # nur die ersten paar Zeilen abdrucken\n```\n\nDieser Befehl liefert z-standardisierte Spalten zurück. Kommoder ist es aber, alle Spalten des Datensatzes zurück zu bekommen, wobei zusätzlich die z-Werte aller numerischen Variablen hinzugekommen sind:\n\n```{r eval = FALSE}\nextra %>% \n  mutate_if(is.numeric, funs(\"z\" = scale)) %>% \n  head\n```\n\nDer Befehl `mutate` berechnet eine neue Spalte; `mutate_if` tut dies nur, wenn die Spalte numerisch ist. Die neue Spalte wird berechnet als z-Transformierung der alten Spalte; zum Spaltenname wird ein \"_z\" hinzugefügt. Natürlich hätten wir auch mit `select` \"händisch\" die relevanten Spalten auswählen können.\n\n\n### Quasi-Konstante finden\nHier suchen wir nach Variablen (Spalten), die nur einen Wert oder zumindest nur sehr wenige verschiedene Werte aufweisen. Oder, ähnlich: Wenn 99.9% der Fälle nur von einem Wert bestritten wird. In diesen Fällen kann man die Variable als \"Quasi-Konstante\" bezeichnen. Quasi-Konstanten sind für die Modellierung von keiner oder nur geringer Bedeutung; sie können in der Regel für weitere Analysen ausgeschlossen werden.\n\nHaben wir z.B. nur Männer im Datensatz, so kann das Geschlecht nicht für Unterschiede im Einkommen verantwortlich sein. Besser ist es, die Variable Geschlecht zu entfernen. Auch hier sind Histogramme oder Boxplots von Nutzen zur Identifikation von (Quasi-)Konstanten. Alternativ kann man sich auch pro die Streuung (numerische Variablen) oder die Anzahl unterschiedlicher Werte (qualitative Variablen) ausgeben lassen:\n\n```{r}\nIQR(extra$n_facebook_friends, na.rm = TRUE)  # keine Konstante\nn_distinct(extra$sex)  # es scheint 3 Geschlechter zu geben...\n```\n\n\n\n### Auf Normalverteilung prüfen\nEinige statistische Verfahren gehen von normalverteilten Variablen aus, daher macht es Sinn, Normalverteilung zu prüfen. *Perfekte* Normalverteilung ist genau so häufig wie *perfekte* Kreise in der Natur. Entsprechend werden Signifikanztests, die ja auf perfekte Normalverteilung prüfen, *immer signifikant* sein, sofern die *Stichprobe groß* genug ist. Daher ist meist zweckmäßiger, einen graphischen \"Test\" durchzuführen: ein Histogramm, ein QQ-Plot oder ein Dichte-Diagramm als \"glatt geschmirgelte\" Variante des Histogramms bieten sich an (s. Abb. \\@ref(fig:fig-norm-check)).\n\n```{r fig-norm-check, echo = FALSE, fig.cap = \"Visuelles Prüfen der Normalverteilung\"}\nextra %>% \n  ggplot() +\n  aes(x = n_facebook_friends) +\n  geom_density() -> p1\n\nextra %>% \n  ggplot() +\n  aes(x = extra_mean) +\n  geom_density() -> p2\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\nWährend die der mittlere Extraversionswert recht gut normalverteilt ist, ist die Anzahl der Facebookfreunde ordentlich (rechts-)schief. Bei schiefen Verteilung können Transformationen Abhilfe schaffen; ein Thema, auf das wir hier nicht weiter eingehen.\n\n\n### Werte umkodieren und partionieren (\"binnen\") \n\n*Umkodieren*\\index{Umkodieren} meint, die Werte zu ändern. Man sieht immer mal wieder, dass die Variable \"gender\" (Geschlecht) mit `1` und `2` kodiert ist. Verwechslungen sind da vorprogrammiert (\"Ich bin mir echt ziemlich sicher, dass ich 1 für Männer kodiert habe, wahrscheinlich...\"). Besser wäre es, die Ausprägungen `male` und `female` (\"Mann\", \"Frau\") o.ä. zu verwenden (vgl. Abb. \\@ref(fig:umkodieren)).\n\n```{r umkodieren, echo = FALSE, fig.cap = \"Sinnbild für Umkodieren\"}\nknitr::include_graphics(\"images/typ_prob/umkodieren_crop.png\")\n```\n\n\nPartionieren\\index{Partionieren) oder *\"Binnen\"*\\index{Binnen} meint, eine kontinuierliche Variablen in einige Bereiche (mindestens 2) zu zerschneiden. Damit macht man aus einer kontinuierlichen Variablen eine diskrete. Ein Bild erläutert das am einfachsten (vgl. Abb. \\@ref(fig:cut-schere)). \n\n```{r cut-schere, echo = FALSE, fig.cap = \"Sinnbild zum 'Binnen'\"}\n\nknitr::include_graphics(\"images/typ_prob/cut_schere_crop.png\")\n\n```\n\n\n\n#### Umkodieren und partionieren mit `car::recode`\n\nManchmal möchte man z.B. negativ gepolte Items umdrehen oder bei kategoriellen Variablen kryptische Bezeichnungen in sprechendere umwandeln. Hier gibt es eine Reihe praktischer Befehle, z.B. `recode` aus dem Paket `car`. Schauen wir uns ein paar Beispiele zum Umkodieren an.\n\n\n```{r}\n\nstats_test <- read.csv(\"data/test_inf_short.csv\")\n\nstats_test$score_fac <- car::recode(stats_test$study_time, \n                        \"5 = 'sehr viel'; 2:4 = 'mittel'; 1 = 'wenig'\",\n                        as.factor.result = TRUE)\nstats_test$score_fac <- car::recode(stats_test$study_time, \n                        \"5 = 'sehr viel'; 2:4 = 'mittel'; 1 = 'wenig'\",\n                        as.factor.result = FALSE)\n\nstats_test$study_time_2 <- car::recode(stats_test$study_time, \n                                       \"5 = 'sehr viel'; 4 = 'wenig'; \n                                       else = 'Hilfe'\", \n                                       as.factor.result = TRUE)\n\nhead(stats_test$study_time_2)\n```\n\nDer Befehle `recode` ist praktisch; mit `:` kann man \"von bis\" ansprechen (das ginge mit `c()` übrigens auch); `else` für \"ansonsten\" ist möglich und mit `as.factor.result` kann man entweder einen Faktor oder eine Text-Variable zurückgeliefert bekommen. Der ganze \"Wechselterm\" steht in Anführungsstrichen (`\"`). Einzelne Teile des Wechselterms sind mit einem Strichpunkt (`;`) voneinander getrennt.\n\n\nDas klassische Umkodieren von Items aus Fragebögen kann man so anstellen; sagen wir `interest` soll umkodiert werden:\n\n```{r}\nstats_test$no_interest <- car::recode(stats_test$interest, \n                                      \"1 = 6; 2 = 5; 3 = 4; 4 = 3; \n                                      5 = 2; 6 = 1; else = NA\")\nglimpse(stats_test$no_interest)\n\n```\n\nBei dem Wechselterm muss man aufpassen, nichts zu verwechseln; die Zahlen sehen alle ähnlich aus...\n\nTesten kann man den Erfolg des Umpolens mit\n\n```{r}\ndplyr::count(stats_test, interest)\ndplyr::count(stats_test, no_interest)\n```\n\nScheint zu passen. Noch praktischer ist, dass man so auch numerische Variablen in Bereiche aufteilen kann (\"binnen\"):\n\n\n```{r}\nstats_test$Ergebnis <- car::recode(stats_test$score, \n                                   \"1:38 = 'durchgefallen'; \n                                   else = 'bestanden'\")\n```\n\n\nNatürlich gibt es auch eine Pfeifen kompatible Version, um Variablen umzukodieren bzw. zu binnen: `dplyr::recode`^[https://blog.rstudio.org/2016/06/27/dplyr-0-5-0/]. Die Syntax ist allerdings etwas weniger komfortabel (da strenger), so dass wir an dieser Stelle bei `car::recode` bleiben.\n\n\n#### Einfaches Umkodieren mit einer Logik-Prüfung\n\nNehmen wir an, wir möchten die Anzahl der Punkte in einer Statistikklausur (`score`) umkodieren in eine Variable \"bestanden\" mit den zwei Ausprägungen \"ja\" und \"nein\"; der griesgrämige Professor beschließt, dass die Klausur ab 25 Punkten (von 40) bestanden sei. Die Umkodierung ist also von der Art \"viele Ausprägungen in zwei Ausprägungen umkodieren\". Das kann man z.B. so erledigen:\n\n```{r}\nstats_test$bestanden <- stats_test$score > 24\n\nhead(stats_test$bestanden)\n```\n\nGenauso könnte man sich die \"Grenzfälle\" - die Bemitleidenswerten mit 24 Punkten - anschauen (knapp daneben ist auch vorbei, so der griesgrämige Professor weiter):\n\n```{r}\nstats_test$Grenzfall <- stats_test$score == 24\n\ncount(stats_test, Grenzfall)\n```\n\nNatürlich könnte man auch hier \"Durchpfeifen\":\n\n```{r}\nstats_test <- \nstats_test %>% \n  mutate(Grenzfall = score == 24)\n\ncount(stats_test, Grenzfall)\n```\n\n\n#### Binnen mit `cut`\nNumerische Werte in Klassen zu gruppieren (\"to bin\", denglisch: \"binnen\") kann mit dem Befehl `cut` (and friends) besorgt werden. \n\nEs lassen sich drei typische Anwendungsformen unterscheiden:\n\nEine numerische Variable ...\n\n1. in *k* gleich große Klassen gruppieren (gleichgroße Intervalle)\n2. so in Klassen gruppieren, dass in jeder Klasse *n* Beobachtungen sind (gleiche Gruppengrößen)\n3. in beliebige Klassen gruppieren\n\n\n##### Gleichgroße Intervalle\n\nNehmen wir an, wir möchten die numerische Variable \"Körpergröße\" in drei Gruppen einteilen: \"klein\", \"mittel\" und \"groß\". Der Range von Körpergröße soll gleichmäßig auf die drei Gruppen aufgeteilt werden, d.h. der Range (Intervall) der drei Gruppen soll gleich groß sein. Dazu kann man `cut_interval` aus `ggplot2` nehmen^[d.h. `ggplot2` muss geladen sein; wenn man `tidyverse` lädt, wird `ggplot2` automatisch auch geladen].\n\n```{r}\nstats_test <- read.csv(\"data/test_inf_short.csv\")\n\n\ntemp <- cut_interval(x = stats_test$score, n = 3)\n\nlevels(temp)\n```\n\n`cut_interval` liefert eine Variable vom Typ `factor` zurück. Hier haben wir das Punktespektrum in drei gleich große Bereiche unterteilt (d.h. mit jeweils gleichem Punkte-Range). \n\n\n##### Gleiche Gruppengrößen\n\n```{r}\ntemp <- cut_number(stats_test$score, n = 2)\nstr(temp)\nmedian(stats_test$score)\n```\n\nMit `cut_number` (aus ggplot2) kann man einen Vektor in `n` Gruppen mit (etwa) gleich viel Observationen einteilen. Hier haben wir `score` am Median geteilt.\n\n>   Teilt man einen Vektor in zwei gleich große Gruppen, so entspricht das einer Aufteilung am Median (Median-Split).\n\n\n##### In beliebige Klassen gruppieren\n\n```{r}\nstats_test$punkte_gruppe <- cut(stats_test$score, \n                             breaks = c(-Inf, 25, 29, 33, 37, 40),\n                             labels = c(\"5\", \"4\", \"3\", \"2\", \"1\"))\n\ncount(stats_test, punkte_gruppe)\n```\n\n`cut` ist im Standard-R (Paket \"base\") enthalten. Mit `breaks` gibt man die Intervallgrenzen an. Zu beachten ist, dass man eine Unter- bzw. Obergrenze angeben muss. D.h. der kleinste Wert in der Stichprobe wird nicht automatisch als unterste Intervallgrenze herangezogen. Anschaulich gesprochen ist `cut` ein Messer, das ein Seil (die kontinuierliche Variable) mit einem oder mehreren Schnitten zerschneidet (vgl. Abb. \\@ref(fig:cut-schere)). Wenn wir 6 Schnitte (`breaks`) tun, haben wir 5 Teile, wie Abb. \\@ref(fig:cut-schere) zeigt. Darum müssen wir auch nur 5 (6-1) `labels` für die Teile vergeben.\n\n\n\n## Deskriptive Statistiken berechnen\n\n\n### Mittelwerte pro Zeile berechnen\n\n#### `rowMeans`\nUm Umfragedaten auszuwerten, will man häufig einen Mittelwert *pro Zeile* berechnen. Normalerweise fasst man eine *Spalte* zu einer Zahl zusammen; aber jetzt, fassen wir eine *Zeile* zu einer Zahl zusammen. Der häufigste Fall ist, wie gesagt, einen Mittelwert zu bilden für jede Person. Nehmen wir an, wir haben eine Befragung zur Extraversion durchgeführt und möchten jetzt den mittleren Extraversions-Wert pro Person (d.h. pro Zeile) berechnen.\n\n```{r}\nextra <- read.csv(\"data/extra.csv\")\n\nextra_items <- extra %>% \n  select(i01:i10)  # `select` ist aus `dplyr`\n\n# oder:\n# select(extra_items, i01:i10)\n\nextra$extra_mw <- rowMeans(extra_items)\n\n```\n\nDa der Datensatz über 28 Spalten verfügt, wir aber nur 10 Spalten heranziehen möchten, um Zeilen auf eine Zahl zusammenzufassen, bilden wir als Zwischenschritt einen \"schmäleren\" Datensatz, `extra_items`. Im Anschluss berechnen wir mit `rowMeans` die Mittelwerte pro Zeile (engl. \"row\").\n\n\n\n### Mittelwerte pro Spalte berechnen\n\n\nEine Möglichkeit ist der Befehl `summary` aus `dplyr`.\n\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  summarise(mean(score),\n            sd(score),\n            median(score),\n            IQR(score))\n```\n\nDie Logik von `dplyr` lässt auch einfach Subgruppenanalysen zu. Z.B. können wir eine Teilmenge des Datensatzes mit `filter` erstellen und dann mit `group_by` Gruppen vergleichen:\n\n```{r}\nstats_test %>% \n  filter(study_time > 1) %>% \n  group_by(interest) %>% \n  summarise(median(score, na.rm = TRUE))\n```\n\n\nWir können auch Gruppierungskriterien unterwegs erstellen:\n\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  filter(study_time > 1) %>% \n  group_by(intessiert = interest > 3) %>% \n  summarise(md_gruppe = median(score))\n```\n\nDie beiden Gruppen von `interessiert` sind \"ja, interessiert\" (`interest > 3` ist `TRUE`) und \"nein, nicht interessiert\" (`interest > 3` ist `FALSE`). Außerdem haben wir der Spalte, die die Mediane zurückliefert einen ansprechenderen Namen gegeben (`md_gruppe`).\n\n\nEtwas expliziter wäre es, `mutate` zu verwenden, um die Variable `interessiert` zu erstellen:\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  filter(study_time > 1) %>% \n  mutate(interessiert = interest > 3) %>% \n  group_by(interessiert) %>% \n  summarise(md_gruppe = median(score),\n            mw_gruppe = mean(score))\n```\n\nDieses Mal haben wir nicht nur eine Spalte mit den Medianwerten, sondern zusätzlich noch mit Mittelwerten berechnet.\n\n```{block2, robust-only, type='rmdcaution', echo = TRUE} \nStatistiken, die auf dem Mittelwert (arithmetisches Mittel) beruhen, sind nicht robust gegenüber Ausreißer: Schon wenige Extremwerte können diese Statistiken so verzerren, dass sie erheblich an Aussagekraft verlieren.\n\nDaher: besser robuste Statistiken verwenden. Der Median, der Modus und der IQR bieten sich an. \n\n```\n\n\n### Korrelationstabellen berechnen\n\nKorrelationen bzw. Korrelationstabellen lassen sich mit dem R-Standardbefehl `cor` berechnen:\n\n```{r cor-demo1}\n\nstats_test %>% \n  select(study_time,interest,score) %>% \n  cor()\n```\n\n\nOh! Lauter NAs! Besser wir löschen Zeilen mit fehlenden Werten bevor wir die Korrelation ausrechnen:\n\n\n```{r cor-demo2}\nstats_test %>% \n  select(study_time:score) %>% \n  na.omit %>% \n  cor()\n```\n\n\nAlternativ zu `cor` kann man auch `corrr:correlate` verwenden:\n\n```{r correlate-demo}\n\nstats_test %>% \n  select(study_time:score) %>% \n  correlate\n```\n\n\n`correlate` hat den Vorteil, dass es bei fehlenden Werten einen Wert ausgibt; die Korrelation wird paarweise mit den verfügbaren (nicht-fehlenden) Werten berechnet. Außerdem wird eine Dataframe (genauer: tibble) zurückgeliefert, was häufig praktischer ist zur Weiterverarbeitung. Wir könnten jetzt die resultierende Korrelationstabelle plotten, vorher \"rasieren\" wir noch das redundanten obere Dreieck ab (da Korrelationstabellen ja symmetrisch sind):\n\n\n```{r rplot-demo}\nstats_test %>% \n  select(study_time:score) %>% \n  correlate %>% \n  shave %>% \n  rplot\n```\n\n\n## Befehlsübersicht\n\nTabelle \\@ref(tab:befehle-praxisprobleme) stellt die Befehle dieses Kapitels dar. \n\n\n\n```{r befehle-praxisprobleme, echo = FALSE}\n\ndf <- readr::read_csv(\"includes/Befehle_Praxisprobleme.csv\")\n\nknitr::kable(df, caption = \"Befehle des Kapitels 'Praxisprobleme'\")\n\n```\n\n",
    "created" : 1497284105788.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2245752063",
    "id" : "356BD18B",
    "lastKnownWriteTime" : 1496658628,
    "last_content_update" : 1496658628,
    "path" : "~/Documents/Publikationen/In_Arbeit/Praxis_der_Datenanalyse/043_Typische_Probleme_Datenanalyse.Rmd",
    "project_path" : "043_Typische_Probleme_Datenanalyse.Rmd",
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}