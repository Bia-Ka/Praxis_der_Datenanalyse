{
    "collab_server" : "",
    "contents" : "\n```{r include=FALSE, cache=FALSE}\nset.seed(1014)\noptions(digits = 3)\n\nknitr::opts_chunk$set(\n  comment = \"#>\",\n  message = FALSE,\n  warning = FALSE,\n  collapse = TRUE,\n  cache = TRUE,\n  out.width = \"70%\",\n  fig.align = 'center',\n  fig.width = 6,\n  fig.asp = 0.618,  # 1 / phi\n  fig.show = \"hold\"\n)\n\noptions(dplyr.print_min = 3, dplyr.print_max = 6)\n```\n\n\n\n\n# Vertiefung: Textmining\n\n\n```{r echo = FALSE, out.width = \"30%\", fig.align = \"center\"}\nknitr::include_graphics(\"images/FOM.jpg\")\n```\n\n```{r echo = FALSE, out.width = \"10%\", fig.align = \"center\"}\nknitr::include_graphics(\"images/licence.png\")\n```\n\n\n\n\nIn diesem Kapitel benötigte R-Pakete:\n```{r}\nlibrary(tidyverse)  # Datenjudo\nlibrary(stringr)  # Textverarbeitung\nlibrary(tidytext)  # Textmining\nlibrary(pdftools)  # PDF einlesen\nlibrary(downloader)  # Daten herunterladen\nlibrary(lsa)  # Stopwörter \nlibrary(SnowballC)  # Wörter trunkieren\nlibrary(wordcloud)  # Wordcloud anzeigen\n```\n\n\n\nEin großer Teil der zur Verfügung stehenden Daten liegt nicht als braves Zahlenmaterial vor, sondern in \"unstrukturierter\" Form, z.B. in Form von Texten. Im Gegensatz zur Analyse von numerischen Daten ist die Analyse von Texten weniger verbreitet bisher. In Anbetracht der Menge und der Informationsreichhaltigkeit von Text erscheint die Analyse von Text als vielversprechend.\n\n\nIn gewisser Weise ist das Textmining ein alternative zu klassischen qualitativen Verfahren der Sozialforschung. Geht es in der qualitativen Sozialforschung primär um das Verstehen eines Textes, so kann man für das Textmining ähnliche Ziele formulieren. Allerdings: Das Textmining ist wesentlich schwächer und beschränkter in der Tiefe des Verstehens. Der Computer ist einfach noch wesentlich *dümmer* als ein Mensch, in dieser Hinsicht. Allerdings ist er auch wesentlich *schneller* als ein Mensch, was das Lesen betrifft. Daher bietet sich das Textmining für das Lesen großer Textmengen an, in denen eine geringe Informationsdichte vermutet wird. Sozusagen maschinelles Sieben im großen Stil. Da fällt viel durch die Maschen, aber es werden Tonnen von Sand bewegt.\n\nIn der Regel wird das Textmining als *gemischte* Methode verwendet: sowohl qualitative als auch qualitative Aspekte spielen eine Rolle. Damit vermittelt das Textmining auf konstruktive Art und Weise zwischen den manchmal antagonierenden Schulen der qualitativ-idiographischen und der quantitativ-nomothetischen Sichtweise auf die Welt. Man könnte es auch als qualitative Forschung mit moderner Technik bezeichnen - mit den skizzierten Einschränkungen wohlgemerkt.\n\n## Einführung\n\n### Grundbegriffe\nDie computergestützte Analyse von Texten speiste (und speist) sich reichhaltig aus Quellen der Linguistik; entsprechende Fachtermini finden Verwendung:  \n\n- Ein *Corpus* bezeichnet die Menge der zu analyisierenden Dokumente; das könnten z.B. alle Reden der Bundeskanzlerin Angela Merkel sein oder alle Tweets von \"\\@realDonaldTrump\".\n\n- Ein *Token* (*Term*) ist ein elementarer Baustein eines Texts, die kleinste Analyseeinheit, häufig ein Wort. \n\n- Unter *tidy text* versteht man einen Dataframe, in dem pro Zeile nur ein Term steht [@Silge2016].\n\n\n\n## Grundlegende Analyse\n\n### Tidy Text Dataframes\n\nBasteln wir uns einen *tidy text* Dataframe. Wir gehen dabei von einem Vektor mit mehreren Text-Elementen aus, das ist ein realistischer Startpunkt. Unser Text-Vektor^[Nach dem Gedicht \"Jahrgang 1899\" von Erich Kästner] besteht aus 4 Elementen.\n\n\n```{r}\ntext <- c(\"Wir haben die Frauen zu Bett gebracht,\",\n          \"als die Männer in Frankreich standen.\",\n          \"Wir hatten uns das viel schöner gedacht.\",\n          \"Wir waren nur Konfirmanden.\")\n```\n\nAls nächstes machen wir daraus einen Dataframe.\n\n```{r}\ntext_df <- data_frame(Zeile = 1:4,\n                      text = text)\n```\n\n```{r echo = FALSE}\nknitr::kable(text_df)\n```\n\nUnd \"dehnen\" diesen Dataframe zu einem *tidy text* Dataframe.\n```{r}\n\ntext_df %>% \n  unnest_tokens(wort, text)\n```\n\nDas `unnest_tokens` kann übersetzt werden als \"entschachtele\" oder \"dehne\" die Tokens - so dass in *jeder Zeile* nur noch *ein Wort* (genauer: Token) steht. Die Syntax ist `unnest_tokens(Ausgabespalte, Eingabespalte)`. Nebenbei werden übrigens alle Buchstaben auf Kleinschreibung getrimmt.\n\nAls nächstes filtern wir die Satzzeichen heraus, da die Wörter für die Analyse wichtiger (oder zumindest einfacher) sind.\n\n```{r}\ntext_df %>% \n  unnest_tokens(wort, text) %>% \n  filter(str_detect(wort, \"[a-z]\"))\n```\n\nDas `\"[a-z]\"` steht für \"alle Buchstaben von a-z\". In Pseudo-Code heißt dieser Abschnitt:\n\n\n```{block2, pseudo-unnest, type='rmdpseudocode', echo = TRUE}\nNehme den Datensatz \"text_df\" UND DANN  \ndehne die einzelnen Elemente der Spalte \"text\", so dass jedes Element seine eigene Spalte bekommt.  \nAch ja: Diese \"gedehnte\" Spalte soll \"Wort\" heißen (weil nur einzelne Wörter drinnen stehen).  \nAch ja 2: Diesees \"dehnen\" wandelt automatisch Groß- in Kleinbuchstaben um. UND DANN   \nfiltere die Spalte \"wort\", so dass nur noch Kleinbuchstaben übrig bleiben. FERTIG.  \n\n```\n\n\n### Text-Daten einlesen\n\nNun lesen wir Text-Daten ein; das können beliebige Daten sein^[Ggf. benötigen Sie Administrator-Rechte, um Dateien auf Ihre Festplatte zu speichern.]. Eine gewisse Reichhaltigkeit ist von Vorteil. Nehmen wir das Parteiprogramm der Partei AfD^[ https://www.alternativefuer.de/wp-content/uploads/sites/7/2016/05/2016-06-27_afd-grundsatzprogramm_web-version.pdf].\n\n\n```{r}\nafd_url <- \"https://www.alternativefuer.de/wp-content/uploads/sites/7/2016/05/2016-06-27_afd-grundsatzprogramm_web-version.pdf\"\n\nafd_pfad <- \"data/afd_programm.pdf\"\n\ndownload(afd_url, afd_pfad)\n\nafd_raw <- pdf_text(afd_pfad)\n\nhead(afd_raw, 1)\n```\n\n\nMit `download` haben wir die Datei mit der Url `afd_url` heruntergeladen und als `afd_pfad` gespeichert. Für uns ist `pdf_text` sehr praktisch, da diese Funktion Text aus einer beliebige PDF-Datei in einen Text-Vektor einliest. `head(afd_raw, 1)` liest das 1. Element (und nur das erste) aus `afd_raw` aus.\n\n\nDer Vektor `afd_raw` hat 96 Elemente (entsprechend der Seitenzahl des Dokzements); zählen wir die Gesamtzahl an Wörtern. Dazu wandeln wir den Vektor in einen tidy text Dataframe um. Auch die Stopwörter entfernen wir wieder wie gehabt.\n\n```{r}\n\nafd_df <- data_frame(Zeile = 1:96, \n                     afd_raw)\n\n\nafd_df %>% \n  unnest_tokens(token, afd_raw) %>% \n  filter(str_detect(token, \"[a-z]\")) -> afd_df\n\ncount(afd_df) \n\n```\n\nEine substanzielle Menge von Text. Was wohl die häufigsten Wörter sind?\n\n\n### Worthäufigkeiten auszählen\n\n```{r}\nafd_df %>% \n  na.omit() %>%  # fehlende Werte löschen\n  count(token, sort = TRUE)\n```\n\nDie häufigsten Wörter sind inhaltsleere Partikel, Präpositionen, Artikel... Solche sogenannten \"Stopwörter\" sollten wir besser herausfischen, um zu den inhaltlich tragenden Wörtern zu kommen. Praktischerweise gibt es frei verfügbare Listen von Stopwörtern, z.B. im Paket `lsa`.\n\n```{r}\ndata(stopwords_de)\n\nstopwords_de <- data_frame(word = stopwords_de)\n\nstopwords_de <- stopwords_de %>% \n  rename(token = word)\n\nafd_df %>% \n  anti_join(stopwords_de) -> afd_df\n\n```\n\n\nUnser Datensatz hat jetzt viel weniger Zeilen; wir haben also durch `anti_join` Zeilen gelöscht (herausgefiltert). Das ist die Funktion von `anti_join`: Die Zeilen, die in beiden Dataframes vorkommen, werden herausgefiltert. Es verbleiben also nicht \"Nicht-Stopwörter\" in unserem Dataframe. Damit wird es schon interessanter, welche Wörter häufig sind.\n\n```{r}\nafd_df %>% \n  count(token, sort = TRUE) -> afd_count\n\n```\n\n\n```{r echo = FALSE}\nafd_count %>% \n  top_n(10) %>% \n  knitr::kable()\n```\n\nGanz interessant; aber es gibt mehrere Varianten des Themas \"deutsch\". Es ist wohl sinnvoller, diese auf den gemeinsamen Wortstamm zurückzuführen und diesen nur einmal zu zählen. Dieses Verfahren nennt man \"stemming\" oder \"trunkieren\".\n\n```{r}\nafd_df %>% \n  mutate(token_stem = wordStem(.$token, language = \"german\")) %>% \n  count(token_stem, sort = TRUE) -> afd_count\n\nafd_count %>% \n  top_n(10) %>% \n  knitr::kable()\n```\n\nDas ist schon informativer. Dem Befehl `SnowballC::wordStem` füttert man einen Vektor an Wörtern ein und gibt die Sprache an (Default ist Englisch). Denken Sie daran, dass `.` bei `dplyr` nur den Datensatz meint, wie er im letzten Schritt definiert war. Mit `.$token` wählen wir also die Variable `token` aus `afd_raw` aus.\n\n\n### Visualisierung\n\n\nZum Abschluss noch eine Visualisierung mit einer \"Wordcloud\" dazu.\n\n\n```{r}\nwordcloud(words = afd_count$token_stem, freq = afd_count$n, max.words = 100, scale = c(2,.5), colors=brewer.pal(6, \"Dark2\"))\n```\n\nMan kann die Anzahl der Wörter, Farben und einige weitere Formatierungen der Wortwolke beeinflussen^[https://cran.r-project.org/web/packages/wordcloud/index.html\n].\n\n \n \nWeniger verspielt ist eine schlichte visualisierte Häufigkeitsauszählung dieser Art, z.B. mit Balkendiagrammen (gedreht).\n\n```{r}\n\nafd_count %>% \n  top_n(30) %>% \n  ggplot() +\n  aes(x = reorder(token_stem, n), y = n) +\n  geom_col() + \n  labs(title = \"mit Trunkierung\") +\n  coord_flip() -> p1\n\nafd_df %>% \n  count(token, sort = TRUE) %>% \n  top_n(30) %>% \n  ggplot() +\n  aes(x = reorder(token, n), y = n) +\n  geom_col() +\n  labs(title = \"ohne Trunkierung\") +\n  coord_flip() -> p2\n\n\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n```\n\nDie beiden Diagramme vergleichen die trunkierten Wörter mit den nicht trunktierten Wörtern. Mit `reorder` ordnen wir die Spalte `token` nach der Spalte `n`. `coord_flip` dreht die Abbildung um 90°, d.h. die Achsen sind vertauscht. `grid.arrange` packt beide Plots in eine Abbildung, welche 2 Spalten (`ncol`) hat.\n\n\n\n## Verweise\n\n- Das Buch *Tidy Text Minig* [@tidytextminig] ist eine hervorragende Quelle vertieftem Wissens zum Textmining mit R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1493219947980.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3987810247",
    "id" : "53C46681",
    "lastKnownWriteTime" : 1493293262,
    "last_content_update" : 1493293262,
    "path" : "~/Documents/Publikationen/In_Arbeit/Praxis_der_Datenanalyse/090_Textmining.Rmd",
    "project_path" : "090_Textmining.Rmd",
    "properties" : {
    },
    "relative_order" : 18,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}