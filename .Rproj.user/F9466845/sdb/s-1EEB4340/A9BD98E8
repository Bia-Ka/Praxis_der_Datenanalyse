{
    "collab_server" : "",
    "contents" : "```{r include=FALSE, cache=FALSE}\nset.seed(1014)\noptions(digits = 3)\n\nknitr::opts_chunk$set(\n  comment = \"#>\",\n  collapse = TRUE,\n  message = FALSE,\n  warning = FALSE,\n  cache = TRUE,\n  out.width = \"70%\",\n  fig.align = 'center',\n  fig.width = 6,\n  fig.asp = 0.618,  # 1 / phi\n  fig.show = \"hold\"\n)\n\n```\n\n\n\n# Praxisprobleme der Datenaufbereitung\n\n\n\n\n\n```{block2, ziele-typ-probleme, type='rmdcaution', echo = TRUE} \nLernziele:\n\n- Typische Probleme der Datenaufbereitung kennen.\n- Typische Probleme der Datenaufbereitung bearbeiten können.\n\n```\n\n\nLaden wir zuerst die benögtigten Pakete; v.a. ist das `dplyr` and friends. Das geht mit dem Paket `tidyverse`. \n```{r}\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(gridExtra)\nlibrary(car)\n```\n\n\nStellen wir einige typische Probleme des Datenjudo (genauer: der Datenaufbereitung) zusammen. Probleme heißt hier nicht, dass es etwas Schlimmes passiert ist, sondern es ist gemeint, wir schauen uns ein paar typische Aufgabenstellungen an, die im Rahmen der Datenaufbereitung häufig anfallen. \n\n\n## Datenaufbereitung\n\n\n### Auf fehlende Werte prüfen \nDas geht recht einfach mit `summarise(mein_dataframe)`. Der Befehl liefert für jede Spalte des Dataframe `mein_dataframe` die Anzahl der fehlenden Werte zurück.\n\n```{r eval = FALSE}\nwo_men <- read.csv(\"https://sebastiansauer.github.io/data/wo_men.csv\")\nglimpse(wo_men)\n```\n\n\n```{r echo = FALSE}\nwo_men <- read.csv(\"data/wo_men.csv\")\nglimpse(wo_men)\n```\n\n\n### Fälle mit fehlenden Werte löschen\nWeist eine Variable (Spalte) \"wenig\" fehlende Werte auf, so kann es schlau sein, nichts zu tun. Eine andere Möglichkeit besteht darin, alle entsprechenden Zeilen zu löschen. Man sollte aber schauen, wie viele Zeilen dadurch verloren gehen.\n\n```{r}\nnrow(wo_men)\nwo_men %>% \n  na.omit %>% \n  nrow\n```\n\n\n```{block2, dplyr-no-brackets, type='rmdcaution', echo = TRUE} \nBei mit der Pfeife verketteten Befehlen darf man für Funktionen die runden Klammern weglassen, wenn man keinen Parameter schreibt. Also ist `nrow` (ohne Klammern) erlaubt bei `dplyr`, wo es eigentlich `nrow()` heißen müsste. Sie dürfen die Klammern natürlich schreiben, aber sie müssen nicht.\n\n```\n\n\n\n\nHier verlieren wir nur 1 Zeile, das verschmerzen wir. Welche eigentlich?\n```{r}\nwo_men %>% \n  rownames_to_column %>%  # Zeilennummer werden eine eigene Spalte\n  filter(!complete.cases(.))  # Nur die nicht-kompletten Fälle filtern\n```\n\nMan beachte, dass der Punkt `.` für den Datensatz steht, wie er vom letzten Schritt weitergegeben wurde. Innerhalb einer dplyr-Befehls-Kette können wir den Datensatz, wie er im letzten Schritt beschaffen war, stets mit `.` ansprechen; ganz praktisch, weil schnell zu tippen. Natürlich könnten wir diesen Datensatz jetzt als neues Objekt speichern und damit weiter arbeiten. Das Ausrufezeichen `!` steht für logisches \"Nicht\".\n\nIn Pseudo-Syntax liest es sich so:\n\n```{block2, no-na-filter, type='rmdpseudocode', echo = TRUE}\nNehme den Datensatz `wo_men` UND DANN...  \nMache aus den Zeilennamen (hier identisch zu Zeilennummer) eine eigene Spalte UND DANN...  \nfiltere die nicht-kompletten Fälle \n\n```\n\n\n### Fehlende Werte ggf. ersetzen  \nIst die Anzahl der fehlenden Werte zu groß, als dass wir es verkraften könnten, die Zeilen zu löschen, so können wir die fehlenden Werte ersetzen. Allein, das ist ein weites Feld und übersteigt den Anspruch dieses Kurses^[Das sagen Autoren, wenn sie nicht genau wissen, wie etwas funktioniert.]. Eine einfache, aber nicht die beste Möglichkeit, besteht darin, die fehlenden Werte durch einen repräsentativen Wert, z.B. den Mittelwert der Spalte, zu ersetzen.\n\n```{r}\nwo_men$height <- replace(wo_men$height, is.na(wo_men$height),\n                         mean(wo_men$height, na.rm = TRUE))\n  \n```\n\n`replace`^[aus dem \"Standard-R\", d.h. Paket \"base\".] ersetzt Werte aus dem Vektor `wo_men$height` alle Werte, für die `is.na(wo_men$height)` wahr ist. Diese Werte werden durch den Mittelwert der Spalte ersetzt^[Hier findet sich eine ausführlichere Darstellung: https://sebastiansauer.github.io/checklist_data_cleansing/index.html].\n\n\n\n### Nach Fehlern suchen\nLeicht schleichen sich Tippfehler oder andere Fehler ein. Man sollte darauf prüfen; so könnte man sich ein Histogramm ausgeben lassen pro Variable, um \"ungewöhnliche\" Werte gut zu erkennen. Meist geht das besser als durch das reine Betrachten von Zahlen. Gibt es wenig unterschiedliche Werte, so kann man sich auch die unterschiedlichen Werte ausgeben lassen.\n\n```{r}\nwo_men %>% \n  count(shoe_size) %>% \n  head  # nur die ersten paar Zeilen\n```\n\n\n### Ausreiser identifizieren\nÄhnlich zu Fehlern, steht man Ausreisern häufig skeptisch gegenüber. Allerdings kann man nicht pauschal sagen, das Extremwerte entfernt werden sollen: Vielleicht war jemand in der Stichprobe wirklich nur 1.20m groß? Hier gilt es, begründet und nachvollziehbar im Einzelfall zu entscheiden. Histogramme und Boxplots sind wieder ein geeignetes Mittel, um Ausreiser zu finden.\n\n```{r echo = FALSE}\np1 <- qplot(x = shoe_size, y = height, data = wo_men, main = \"ungefiltert\")\n\np2 <- wo_men %>% \n  filter(height > 120, height < 210) %>% \n  qplot(x = shoe_size, y = height, data = ., main = \"gefiltert\")\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n\n### Hochkorrelierte Variablen finden\nHaben zwei Leute die gleiche Meinung, so ist einer von beiden überflüssig - wird behauptet. Ähnlich bei Variablen; sind zwei Variablen sehr hoch korreliert (>.9, als grober (!) Richtwert), so bringt die zweite kaum Informationszuwachs zur ersten. Und kann ausgeschlossen werden. Oder man fasst ähnliche Variablen zusammen.\n\n```{r}\nwo_men %>% \n  select(height, shoe_size) %>% \n  correlate() -> km   # Korrelationsmatrix berechnen\nkm  \n\nkm %>% \n  shave() %>% # Oberes Dreieck ist redundant, wird \"abrasiert\"\n  rplot()  # Korrelationsplot\n```\n\nDie Funktion `correlate` stammt aus dem Paket `corrr`^[https://github.com/drsimonj/corrr ], welches vorher installiert und geladen sein muss. Hier ist die Korrelation nicht zu groß, so dass wir keine weiteren Schritte unternehmen.\n\n\n### z-Standardisieren\nFür eine Reihe von Analysen ist es wichtig, die Skalierung der Variablen zur vereinheitlichen. Die z-Standardisierung ist ein übliches Vorgehen. Dabei wird der Mittelwert auf 0 transformiert und die SD auf 1; man spricht - im Falle von (hinreichend) normalverteilten Variablen - jetzt von der *Standardnormalverteilung*\\index{Standardnormalverteilung}. Unterscheiden sich zwei Objekte A und B in einer standardnormalverteilten Variablen, so sagt dies nur etwas zur relativen Position von A zu B innerhalb ihrer Verteilung aus - im Gegensatz zu den Rohwerten.\n\n```{r}\nwo_men %>% \n  select_if(is.numeric) %>%  # Spalte nur auswählen, wenn numerisch\n  scale() %>%  # z-standardisieren\n  head()  # nur die ersten paar Zeilen abdrucken\n```\n\nDieser Befehl liefert zwei z-standardisierte Spalten zurück. Kommoder ist es aber, alle Spalten des Datensatzes zurück zu bekommen, wobei zusätzlich die z-Werte aller numerischen Variablen hinzugekommen sind:\n\n```{r}\nwo_men %>% \n  mutate_if(is.numeric, funs(\"z\" = scale)) %>% \n  head\n```\n\nDer Befehl `mutate` berechnet eine neue Spalte; `mutate_if` tut dies, wenn die Spalte numerisch ist. Die neue Spalte wird berechnet als z-Transformierung der alten Spalte; zum Spaltenname wird ein \"_z\" hinzugefügt. Natürlich hätten wir auch mit `select` \"händisch\" die relevanten Spalten auswählen können.\n\n\n### Quasi-Konstante finden\nHat eine Variable nur einen Wert, so verdient sie die Ehrenbezeichnung \"Variable\" nicht wirklich. Haben wir z.B. nur Männer im Datensatz, so kann das Geschlecht nicht für Unterschiede im Einkommen verantwortlich sein. Besser die Variable Geschlecht dann zu entfernen. Auch hier sind Histogramme oder Boxplots von Nutzen zur Identifiktion von (Quasi-)Konstanten. Alternativ kann man sich auch pro die Streuung (numerische Variablen) oder die Anzahl unterschiedlicher Werte (qualitative Variablen) ausgeben lassen.\n\n\n### Auf Normalverteilung prüfen\nEinige statistische Verfahren gehen von normalverteilten Variablen aus, daher macht es Sinn, Normalverteilung zu prüfen. *Perfekte* Normalverteilung ist genau so häufig wie *perfekte* Kreise in der Natur. Entsprechend werden Signifikanztests, die ja auf perfekte Normalverteilung prüfen, *immer signifikant* sein, sofern die *Stichprobe groß* genug ist. Daher ist meist zweckmäßiger, einen graphischen \"Test\" durchzuführen: ein Histogramm oder ein   Dichte-Diagramm als \"glatt geschmiergelte\" Variante des Histogramms bieten sich an.\n\n```{r echo = FALSE}\nwo_men %>% \n  ggplot() +\n  aes(x = height) +\n  geom_density() -> p1\n\nwo_men %>% \n  ggplot() +\n  aes(x = shoe_size) +\n  geom_density() -> p2\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\nWährend die Körpergröße sehr deutlich normalverteilt ist, ist die Schuhgröße recht schief. Bei schiefen Verteilung können Transformationen Abhilfe schaffen. Hier erscheint die Schiefe noch erträglich, so dass wir keine weiteren Maßnahmen einleiten.\n\n\n### Werte umkodieren und \"binnen\" \n\n*Umkodieren*\\index{Umkodieren} meint, die Werte zu ändern. Man sieht immer mal wieder, dass die Variable \"gender\" (Geschlecht) mit `1` und `2` kodiert ist. Verwechslungen sind da vorpragmmiert (\"Ich bin mir echt ziemlich sicher, dass ich 1 für Männer kodiert habe, wahrscheinlich...\"). Besser wäre es, die Ausprägungen `male` und `female` (\"Mann\", \"Frau\") o.ä. zu verwenden (vgl. Abb. \\@ref(fig:umkodieren)).\n\n```{r umkodieren, echo = FALSE, fig.cap = \"Sinnbild für Umkodieren\"}\nknitr::include_graphics(\"images/typ_prob/umkodieren_crop.pdf\")\n```\n\n\n*Binnen*\\index{Binnen} meint, eine kontinuierliche Variablen in einige Bereiche (mindestens 2) zu zerschneiden. Ein Bild erläutert das am einfachsten (vgl. Abb. \\@ref(fig:cut-schere)). \n\n```{r cut-schere, echo = FALSE, fig.cap = \"Sinnbild zum 'Binnen'\"}\n\nknitr::include_graphics(\"images/typ_prob/cut_schere_crop.pdf\")\n\n```\n\n\n\n#### Umkodieren und binnen mit `car::recode`\n\nManchmal möchte man z.B. negativ gepolte Items umdrehen oder bei kategoriellen Variablen kryptische Bezeichnungen in sprechendere umwandeln. Hier gibt es eine Reihe praktischer Befehle, z.B. `recode` aus dem Paket `car`. Schauen wir uns ein paar Beispiele zum Umkodieren an.\n\n\n```{r}\n\nstats_test <- read.csv(\"data/test_inf_short.csv\")\n\nstats_test$score_fac <- car::recode(stats_test$study_time, \n                        \"5 = 'sehr viel'; 2:4 = 'mittel'; 1 = 'wenig'\",\n                        as.factor.result = TRUE)\nstats_test$score_fac <- car::recode(stats_test$study_time, \n                        \"5 = 'sehr viel'; 2:4 = 'mittel'; 1 = 'wenig'\",\n                        as.factor.result = FALSE)\n\nstats_test$study_time_2 <- car::recode(stats_test$study_time, \n                                       \"5 = 'sehr viel'; 4 = 'wenig'; \n                                       else = 'Hilfe'\", \n                                       as.factor.result = TRUE)\n\nhead(stats_test$study_time_2)\n```\n\nDer Befehle `recode` ist praktisch; mit `:` kann man \"von bis\" ansprechen (das ginge mit `c()` übrigens auch); `else` für \"ansonsten\" ist möglich und mit `as.factor.result` kann man entweder einen Faktor oder eine Text-Variable zurückgeliefert bekommen. Der ganze \"Wechselterm\" steht in Anführungsstrichen (`\"`). Einzelne Teile des Wechselterms sind mit einem Strichpunkt (`;`) voneinander getrennt.\n\n\nDas klassiche Umkodieren von Items aus Fragebögen kann man so anstellen; sagen wir `interest` soll umkodiert werden:\n\n```{r}\nstats_test$no_interest <- car::recode(stats_test$interest, \n                                      \"1 = 6; 2 = 5; 3 = 4; 4 = 3; \n                                      5 = 2; 6 = 1; else = NA\")\nglimpse(stats_test$no_interest)\n\n```\n\nBei dem Wechselterm muss man aufpassen, nichts zu verwechseln; die Zahlen sehen alle ähnlich aus...\n\nTesten kann man den Erfolg des Umpolens mit\n\n```{r}\ndplyr::count(stats_test, interest)\ndplyr::count(stats_test, no_interest)\n```\n\nScheint zu passen. Noch praktischer ist, dass man so auch numerische Variablen in Bereiche aufteilen kann (\"binnen\"):\n\n\n```{r}\nstats_test$Ergebnis <- car::recode(stats_test$score, \n                                   \"1:38 = 'durchgefallen'; \n                                   else = 'bestanden'\")\n```\n\n\nNatürlich gibt es auch eine Pfeifen komptatible Version, um Variablen umzukodieren bzw. zu binnen: `dplyr::recode`^[https://blog.rstudio.org/2016/06/27/dplyr-0-5-0/]. Die Syntax ist allerdings etwas weniger komfortabel (da strenger), so dass wir an dieser Stelle bei `car::recode` bleiben.\n\n\n#### Einfaches Umkodieren mit einer Logik-Prüfung\n\nNehmen wir an, wir möchten die Anzahl der Punkte in einer Statistikklausur (`score`) umkodieren in eine Variable \"bestanden\" mit den zwei Ausprägungen \"ja\" und \"nein\"; der griesgrämige Professor beschließt, dass die Klausur ab 25 Punkten (von 40) bestanden sei. Die Umkodierung ist also von der Art \"viele Ausprägungen in zwei Ausprägungen umkodieren\". Das kann man z.B. so erledigen:\n\n```{r}\nstats_test$bestanden <- stats_test$score > 24\n\nhead(stats_test$bestanden)\n```\n\nGenauso könnte man sich die \"Grenzfälle\" - die Bemitleidenswerten mit 24 Punkten - anschauen (knapp daneben ist auch vorbei, so der griesgrämige Professor weiter):\n\n```{r}\nstats_test$Grenzfall <- stats_test$score == 24\n\ncount(stats_test, Grenzfall)\n```\n\nNatürlich könnte man auch hier \"Durchpfeifen\":\n\n```{r}\nstats_test <- \nstats_test %>% \n  mutate(Grenzfall = score == 24)\n\ncount(stats_test, Grenzfall)\n```\n\n\n#### Binnen mit `cut`\nNumerische Werte in Klassen zu gruppieren (\"to bin\", denglisch: \"binnen\") kann mit dem Befehl `cut` (and friends) besorgt werden. \n\nEs lassen sich drei typische Anwendungsformen unterscheiden:\n\nEine numerische Variable ...\n\n1. in *k* gleich große Klassen grupieren (gleichgroße Intervalle)\n2. so in Klassen gruppieren, dass in jeder Klasse *n* Beobachtungen sind (gleiche Gruppengrößen)\n3. in beliebige Klassen gruppieren\n\n\n##### Gleichgroße Intervalle\n\nNehmen wir an, wir möchten die numerische Variable \"Körpergröße\" in drei Gruppen einteilen: \"klein\", \"mittel\" und \"groß\". Der Range von Körpergröße soll gleichmäßig auf die drei Gruppen aufgeteilt werden, d.h. der Range (Interval) der drei Gruppen soll gleich groß sein. Dazu kann man `cut_interval` aus `ggplot2` nehmen [^d.h. `ggplot2` muss geladen sein; wenn man `tidyverse` lädt, wird `ggplot2` automatisch auch geladen].\n\n```{r}\nwo_men <- read_csv(\"data/wo_men.csv\")\n\nwo_men %>% \n  filter(height > 150, height < 220) -> wo_men2\n\ntemp <- cut_interval(x = wo_men2$height, n = 3)\n\nlevels(temp)\n```\n\n`cut_interval` liefert eine Variabel vom Typ `factor` zurück. \n\n\n##### Gleiche Gruppengrößen\n\n```{r}\ntemp <- cut_number(wo_men2$height, n = 2)\nstr(temp)\n```\n\nMit `cut_number` (aus ggplot2) kann man einen Vektor in `n` Gruppen mit (etwa) gleich viel Observationen einteilen.\n\n>   Teilt man einen Vektor in zwei gleich große Gruppen, so entspricht das einer Aufteilung am Median (Median-Split).\n\n\n##### In beliebige Klassen gruppieren\n\n```{r}\nwo_men$groesse_gruppe <- cut(wo_men$height, \n                             breaks = c(-Inf, 100, 150, 170, 200, 230, Inf))\n\ncount(wo_men, groesse_gruppe)\n```\n\n`cut` ist im Standard-R (Paket \"base\") enthalten. Mit `breaks` gibt man die Intervallgrenzen an. Zu beachten ist, dass man eine Unter- bzw. Obergrenze angeben muss. D.h. der kleinste Wert in der Stichprobe wird nicht automatisch als unterste Intervallgrenze herangezogen. Anschaulich gesprochen ist `cut` ein Messer, das ein Seil (die kontinuierliche Variable) mit einem oder mehreren Schnitten zerschneidet (vgl. Abb. \\@ref(fig:cut-schere)).\n\n\n\n## Deskriptive Statistiken berechnen\n\n\n### Mittelwerte pro Zeile berechnen\n\n#### `rowMeans`\nUm Umfragedaten auszuwerten, will man häufig einen Mittelwert *pro Zeile* berechnen. Normalerweise fasst man eine *Spalte* zu einer Zahl zusammen; aber jetzt, fassen wir eine *Zeile* zu einer Zahl zusammen. Der häufigste Fall ist, wie gesagt, einen Mittelwert zu bilden für jede Person. Nehmen wir an, wir haben eine Befragung zur Extraversion durchgeführt und möchten jetzt den mittleren Extraversions-Wert pro Person (d.h. pro Zeile) berechnen.\n\n```{r}\nextra <- read.csv(\"data/extra.csv\")\n\nextra_items <- extra %>% \n  select(i01:i10)  # `select` ist aus `dplyr`\n\n# oder:\n# select(extra_items, i01:i10)\n\nextra$extra_mw <- rowMeans(extra_items)\n\n```\n\nDa der Datensatz über 28 Spalten verfügt, wir aber nur 10 Spalten heranziehen möchten, um Zeilen auf eine Zahl zusammenzufassen, bilden wir als Zwischenschritt einen \"schmäleren\" Datensatz, `extra_items`. Im Anschluss berechnen wir mit `rowMeans` die Mittelwerte pro Zeile (engl. \"row\").\n\n\n#### Vertiefung: `dplyr`\n\nAlternativ können wir Mittelwerte mit dplyr berechnen:\n\n\n```{r}\nextra_items %>% \n  na.omit %>% \n  rowwise() %>% \n  mutate(mean_row = mean(i01:i10)) %>% \n  select(mean_row) %>% \n  head # nur die ersten paar Zeilen von `mean_row` zeigen\n  \n```\n\n`na.omit` wirft alle Zeilen raus, in denen fehlende Werte vorkommen. Das ist nötig, damit `mean` ein Ergebnis ausgibt (bei fehlenden Werten gibt `mean` sonst `NA` zurück).\n\n`rowwise` gruppiert den Datensatz nach Zeilen (`row_number()`), ist also synonym zu:\n\n```{r}\nextra_items %>% \n  na.omit %>% \n  group_by(row_number()) %>% \n  mutate(mean_row = mean(i01:i10)) %>% \n  select(mean_row) %>% \n  head # nur die ersten paar Zeilen von `mean_row` zeigen\n```\n\n\n### Mittelwerte pro Spalte berechnen\n\n\nEine Möglichkeit ist der Befehl `summary` aus `dplyr`.\n\n```{r load-stats-test-again, echo = FALSE}\nstats_test <- read.csv(\"data/test_inf_short.csv\")\n```\n\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  summarise(mean(score),\n            sd(score),\n            median(score),\n            IQR(score))\n```\n\nDie Logik von `dplyr` lässt auch einfach Subgruppenanalysen zu. Z.B. können wir eine Teilmenge des Datensatzes mit `filter` erstellen und dann mit `group_by` Gruppen vergleichen:\n\n```{r}\nstats_test %>% \n  filter(study_time > 1) %>% \n  group_by(interest) %>% \n  summarise(median(score, na.rm = TRUE))\n```\n\n\nWir können auch Gruppierungskriterien unterwegs erstellen:\n\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  filter(study_time > 1) %>% \n  group_by(intessiert = interest > 3) %>% \n  summarise(median(score))\n```\n\nDie beiden Gruppen von `interessiert` sind \"ja, interessiert\" (`interest > 3` ist `TRUE`) und \"nein, nicht interessiert\" (`interest > 3` ist `FALSE`).\n\n\nEtwas expliziter wäre es, `mutate` zu verwenden, um die Variable `interessiert` zu erstellen:\n\n```{r}\nstats_test %>% \n  na.omit %>% \n  filter(study_time > 1) %>% \n  mutate(interessiert = interest > 3) %>% \n  group_by(interessiert) %>% \n  summarise(median(score))\n```\n\n\n```{block2, robust-only, type='rmdcaution', echo = TRUE} \nStatistiken, die auf dem Mittelwert (arithmetisches Mittel) beruhen, sind nicht robust gegenüber Ausreisern: Schon wenige Extremwerte können diese Statistiken so verzerren, dass sie erheblich an Aussagekraft verlieren.\n\nDaher: besser robuste Statistiken verwenden. Der Median, der Modus und der IQR bieten sich an. \n\n\n```\n\n\n### Korrelationstabellen berechnen\n\nKorrelationen bzw. Korrelationstabellen lassen sich mit dem R-Standardbefehl `cor` berechnen:\n\n```{r cor-demo1}\nstats_test <- read.csv(\"data/test_inf_short.csv\")\n\nstats_test %>% \n  select(study_time,interest,score) %>% \n  cor()\n```\n\n\nOh! Lauter NAs! Besser wir löschen Zeilen mit fehlenden Werten bevor wir die Korrelation ausrechnen:\n\n\n```{r cor-demo2}\nstats_test %>% \n  select(study_time:score) %>% \n  na.omit %>% \n  cor()\n```\n\n\nAlternativ zu `cor` kann man auch `corrr:correlate` verwenden:\n\n```{r correlate-demo}\nstats_test <- read.csv(\"data/test_inf_short.csv\")\n\n\nstats_test %>% \n  select(study_time:score) %>% \n  correlate\n```\n\n\n`correlate` hat den Vorteil, dass es bei fehlenden Werten einen Wert ausgibt; die Korrelation wird paarweise mit den verfügbaren (nicht-fehlenden) Werten berechnet. Außerdme wird eine Dataframe (genauer: tibble) zurückgeliefert, was häufig praktischer ist zur Weiterverarbeitung. Wir könnten jetzt die resultierende Korrelationstabelle plotten, vorher \"rasieren\" wir noch das redundaten obere Dreieck ab (da Korrelationstabellen ja symmetrisch sind):\n\n\n```{r rplot-demo}\nstats_test %>% \n  select(study_time:score) %>% \n  correlate %>% \n  shave %>% \n  rplot\n```\n\n\n## Befehlsübersicht\n\n\nPaket::Funktion        Beschreibung\n-----------------      -------------\nna.omit                Löscht Zeilen, die fehlende Werte enthalten\nnrow                   Liefert die Anzahl der Zeilen des Dataframes zurück  \ncomplete.cases         Gibt die Zeilen ohne fehlenden Werte eines Dataframes zurück\ncar::recode            Kodiert Werte um\ncut                    Schneidet eine kontinuierliche Variable in Wertebereiche\nrowMeans               Berechnet Zeilen-Mittelwerte\ndplyr::rowwise         Gruppiert nach Zeilen\nggplot2::cut_number    Schneidet eine kontinuierliche Variable in *n* gleich große Bereiche\nggplot2::cut_interval  Schneidet eine kontinuierliche Variable in Intervalle der Größe *k*\nhead                   Zeigt nur die ersten Zeilen/Werte eines Dataframes/Vektors an.  \nscale                  z-skaliert eine Variable\ndplyr::select_if       Wählt eine Spalte aus, wenn ein Kriterium erfüllt ist\ndplyr::glimpse         Gibt einen Überblick über einen Dataframe\ndplyr::mutate_if       definiert eine Spalte, wenn eine Kriterium erfüllt ist\n:                      Definiert einen Bereich von ... bis ...\ncorrr:correlate        Berechnet Korrelationtabelle, liefert einen Dataframe zurück\ncor                    Berechnet Korrelationtabelle\nrplot                  Plottet Korrelationsmatrix von `correlate`\nshave                  \"Rasiert\" redundantes Dreick in Korrelationsmatrix ab",
    "created" : 1493236222152.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3840141104",
    "id" : "A9BD98E8",
    "lastKnownWriteTime" : 1493370151,
    "last_content_update" : 1493370151366,
    "path" : "~/Documents/Publikationen/In_Arbeit/Praxis_der_Datenanalyse/043_Typische_Probleme_Datenanalyse.Rmd",
    "project_path" : "043_Typische_Probleme_Datenanalyse.Rmd",
    "properties" : {
    },
    "relative_order" : 19,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}