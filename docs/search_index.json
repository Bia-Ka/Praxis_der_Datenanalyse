[
["index.html", "Praxis der Datenanalyse Kapitel 1 Vorwort", " Praxis der Datenanalyse Skript zum Modul Sebastian Sauer, Matthias Gehrke, Karsten Lübke, Oliver Gansser 2017-04-20 Kapitel 1 Vorwort Statistik heute; was ist das? Sicherlich haben sich die Schwerpunkte von “gestern” zu “heute” verschoben. Wenig überraschend spielt der Computer eine immer größere Rolle; die Daten werden vielseitiger und massiger. Entsprechend sind neue Verfahren nötig - und vorhanden, in Teilen - um auf diese neue Situation einzugehen. Einige Verfahren werden daher weniger wichtig, z.B. der p-Wert oder der t-Test. Allerdings wird vielfach, zumeist, noch die Verfahren gelehrt und verwendet, die für die erste Hälfte des 20. Jahrhunderts entwickelt wurden. Eine Zeit, in der kleine Daten, ohne Hilfe von Computern und basierend auf einer kleinen Theoriefamilie im Rampenlicht standen (Cobb 2007). Die Zeiten haben sich geändert! Zu Themen, die heute zu den dynamischten Gebieten der Datenanalyse gehören, die aber früher keine große Rolle spielten, gehören (Hardin et al. 2015): Nutzung von Datenbanken und anderen Data Warehouses Daten aus dem Internet automatisch einlesen (“scraping”) Genanalysen mit Tausenden von Variablen Gesichtserkennung Sie werden in diesem Kurs einige praktische Aspekte der modernen Datenanalyse lernen. Ziel ist es, Sie - in Grundzügen - mit der Art und Weise vertraut zu machen, wie angewandte Statistik bei führenden Organisationen und Praktikern verwendet wird^[Statistiker, die dabei als Vorbild Pate standen sind: Roger D. Peng: http://www.biostat.jhsph.edu/~rpeng/, Hadley Wickham: http://hadley.nz, Daniel Lakens: http://daniellakens.blogspot.de und andere). Es ist ein Grundlagenkurs; das didaktische Konzept beruht auf einem induktiven, intuitiven Lehr-Lern-Ansatz. Formeln und mathematische Hintergründe such man meist vergebens (tja). Im Gegensatz zu anderen Statistik-Büchern steht hier die Umsetzung mit R stark im Vordergrund. Dies hat pragmatische Gründe: Möchte man Daten einer statistischen Analyse unterziehen, so muss man sie zumeist erst aufbereiten; oft mühselig aufbereiten. Selten kann man den Luxus genießen, einfach “nur”, nach Herzenslust sozusagen, ein Feuerwerk an multivariater Statistik abzubrennen. Zuvor gilt es, die Daten aufzubereiten, umzuformen, zu prüfen und zusammenzufassen. Diesem Teil ist hier recht ausführlich Rechnung getragen. “Statistical thinking” sollte, so eine verbreitete Idee, im Zentrum oder als Ziel einer Statistik-Ausbildung stehen (Wild and Pfannkuch 1999). Es ist die Hoffnung der Autoren dieses Buches, dass das praktische Arbeiten (im Gegensatz zu einer theoretischen Fokus) zur Entwicklung einer Kompetenz im statistischen Denken beiträgt. Außerdem spielt in diesem Kurs die Visualisierung von Daten eine große Rolle. Zum einen könnte der Grund einfach sein, dass Diagramme ansprechen und gefallen (einigen Menschen). Zum anderen bieten Diagramme bei umfangreichen Daten Einsichten, die sonst leicht wortwörtlich überersehen würden. Dieser Kurs zielt auf die praktischen Aspekte der Analyse von Daten ab: “wie mache ich es?”; mathematische und philosophische Hintergründe werden vernachlässigt bzw. auf einschlägige Literatur verwiesen. R-Pseudo-Syntax: R ist (momentan) die führende Umgebung für Datenanalyse. Entsprechend zentral ist R in diesem Kurs. Zugebenermaßen braucht es etwas Zeit, bis man ein paar Brocken “Errisch” spricht. Um den Einstieg zu erleichern, ist Errisch auf Deutsch übersetzt an einigen Stellen, wo mir dies besonders hilfreich erschien. Diese Stellen sind mit diesem Symbol gekennzeichnet (für R-Pseudo-Syntax). Achtung, Falle: Schwierige oder fehlerträchtige Stellen sind mit diesem Symbol markiert. Übungsaufgaben: Das Skript beinhaltet in jedem Kapitel Übungsaufgaben oder/und Testfragen. Auf diese wird mit diesem Icon verwiesen oder die Übungen sind in einem Abschnitt mit einsichtigem Titel zu finden. Love: Wenn Ihnen R diesen Smiley präsentiert, dann sind Sie am Ziel Ihrer Träume: . Dieses Buch hat einige Voraussetzungen, was das Vorwissen der Leser angeht; folgende Themengebiete werden vorausgsetzt: Deskriptive Statistik Grundlagen der Inferenzstatistik Grundagen der Regressionsanalyse Skalenniveaus Grundlagen von R Dieses Skript wurde mit dem Paket bookdown (Xie 2015) erstellt, welches wiederum stark auf den Paketen knitr (Xie 2015) und rmarkdown (Allaire et al. 2016a) beruht. Diese Pakete stellen verblüffende Funktionalität zur Verfügung als freie Software (frei wie in Bier und frei wie in Freiheit). Aus Gründen des Lesbarkeit wird das männliche Generikum verwendet, welches Frauen und Männer in gleichen Maßen ansprechen soll. Die Bildnachweise sind in folgenden Muster aufgebaut: Nummer, Verweis zum Bild, Names des Autors, Titel, Quelle (URL), Lizenz, Abrufdatum. Abb. 10.2, Sebastian Unrau, ohne Titel, https://unsplash.com/photos/CoD2Q92UaEg, CC0, 2017-02-12 Abb. 10.1, Lothar Spurzem, VW 1303 von Wiking in 1:87; Größe des Modells: 47,5 mm, https://de.wikipedia.org/wiki/Modellautomobil#/media/File:Wiking-Modell_VW_1303_(um_1975).JPG, CC-BY-SA 2.0, de. Alle verwendeten Datensätze und R-Pakete finden sich im Literaturverzeichnis; im Text werden Pakete nicht zitiert. Ein Teil dieses Skripts basiert auf Arbeiten von meinen Kollegen Oliver Gansser, Matthias Gehrke und Karsten Lübke. Ohne deren Unterstützung, Ermutigung und Kritik gäbe es diesen Kurs nicht. Gerade von Karsten Lübke habe ich einiges gelernt. Sebastian Sauer Literaturverzeichnis "],
["organisatorisches.html", "Kapitel 2 Organisatorisches 2.1 Modulziele 2.2 Themen pro Termin 2.3 Prüfung 2.4 Literatur", " Kapitel 2 Organisatorisches 2.1 Modulziele Die Studierenden können nach erfolgreichem Abschluss des Moduls: den Ablauf eines Projekts aus der Datenanalyse in wesentlichen Schritten nachvollziehen, Daten aufbereiten und ansprechend visualisieren, Inferenzstatistik anwenden und kritisch hinterfragen, klassische Vorhersagemethoden (Regression) anwenden, moderne Methoden der angewandten Datenanalyse anwenden (z.B. Textmining), betriebswirtschaftliche Fragestellungen mittels datengetriebener Vorhersagemodellen beantworten. 2.2 Themen pro Termin Anahmen: Zeitumfang: 44 UE für Lehre Vorerfahrung: Deskriptive Statistik, Inferenzstatistik, Grundlagen R, Grundlagen Visualisierung Termin Thema/ Kapitel 1 Organisatorisches Einführung Rahmen Daten einlesen 2 Datenjudo 3 Daten visualisieren 4 Fallstudie 5 Daten modellieren Der p-Wert 6 Lineare Regression - metrisch 7 Lineare Regression - kategorial 8 Fallstudie 9 Vertiefung: Textmining und Clusteranalyse 10 Vertiefung: Baumbasierte Verfahren 11 Wiederholung 2.3 Prüfung 2.3.1 Prüfungshinweise Die Prüfung besteht aus zwei Teilen einer Klausur (50% der Teilnote) einer Datenanalyse (50% der Teilnote). Prüfungsrelevant ist der gesamte Stoff aus dem Skript und dem Unterricht mit folgenden Ausnahmen: Inhalte/Abschnitte, die als “nicht klausurrelevant” gekennzeichnet sind, Inhalte/Abschnitte, die als “Vertiefung” gekennzeichnet sind, Fallstudien (nur für Klausuren nicht prüfungslevant), die Inhalte von Links, die Inhalte von Fußnoten. 2.3.2 Klausur Die Klausur besteht fast oder komplett aus Multiple-Choice (MC-)-Aufgaben mit mehreren Antwortoptionen; zumeist ist eine Antwort aus vieren auszuwählen. Die (maximale) Anzahl der richtigen Aussagen ist pro Aufgabe angegeben. Werden mehr Aussagen als “richtig” angekreuzt als angegeben, so wird die Aufgabe mit 0 Punkten beurteilt. Ansonsten werden Teilpunkte für jede Aufgabe vergeben. Jede Aussage gilt ceteris paribus (unter sonst gleichen Umständen). Aussagen der Art “A ist B” (z.B. “Menschen sind sterblich”) sind nur dann als richtig auszuwählen, wenn die Aussage immer richtig ist. Im Zweifel ist eine Aussage auf den Stoff, so wie im Unterricht behandelt, zu beziehen. Werden in Aussagen Zahlen abgefragt, so sind Antworten auch dann richtig, wenn die vorgeschlagene Antwort ab der 1. Dezimale von der wahren Antwort abweicht (einigermaßen genaue Aussagen werden als richtig akzeptiert). Bei Fragen zu R-Syntax spielen Aspekte wie Enter-Taste o.ä. bei der Beantwortung der Frage keine Rolle; diese Aspekte dürfen zu ignorieren. Jede Aussage einer MC-Aufgabe ist entweder richtig oder falsch (aber nicht beides oder keines). Die MC-Aufgaben sind nur mit Kreuzen zu beantworten; Text wird bei der Korrektur nicht berücksichtigt. Bei Nachholklausuren gelten die selben Inhalte (inkl. Schwerpunkte) wie bei der Standard-Klausur, sofern nicht anderweitig angegeben. I.d.R. sind nur Klausurpapier und ein nicht-programmierbarer Taschenrechner als Hilfsmittel zulässig. Die Musterlösungen zu offenen Fragen sind elektronisch hinterlegt. 2.3.3 Datenanalyse Wenden Sie die passenden, im Modul eingeführten statistischen Verfahren an. Werten Sie die Daten mit R aus; R-Syntax soll verwendet und im Hauptteil dokumentiert werden. In der Wahl des Datensatzes sind Sie frei, mit folgender Ausnahme: Im Unterricht besprochene Datensätze dürfen nicht als Prüfungsleistung eingereicht werden (vgl. Abschnitt 3.1.7). Der (Original-)Name des Datensatzes (sowie ggf. Link) ist bei der Anmeldung anzugeben. Gruppenarbeiten sind nicht zulässig. Hat sich jemand schon für einen Datensatz angemeldet, so darf dieser Datensatz nicht mehr gewählt werden (“first come, first serve”). Fundorte für Datensätze sind z.B. hier, hier und hier; im Internet finden sich viele Datensätze1. Schreiben Sie Ihre Ergebnisse in einer Ausarbeitung zusammen; der Umfang der Ausarbeitung umfasst ca. 1000-1500 Wörter (nur Hauptteil; d.h. exklusive Deckblatt, Verzeichnisse, Anhang etc.). Untersuchen Sie 2-3 Hypothesen. Denken Sie daran, Name, Matrikelnummer, Modulname etc. anzugeben (Deckblatt). Bei der Gestaltung des Layout entscheiden Sie selbständig bitte nach Zweckmäßigkeit (und Ästhetik). Fügen Sie keine Erklärungen oder Definitionen von statistischen Verfahren an. 2.3.4 Gliederungsvorschlag zur Datenanalyse Datensatz Beschreibung Name Hintergrund (Themengebiet, Theorien, Relevanz), ca. 100 Wörter Dimension (Zeilen*Spalten) Zitation (wenn vorhanden) sonstige Hinweise (z.B. Datenqualität, Entstehung des Datensatzes) Variablendeskription (nur für Variablen der Hypothese) Skalenniveaus Kontinuität (nur bei metrischen Variablen) R-Datentyp Anzahl Fälle und fehlende Werte Erläuterung der Variablen Deduktive Analyse Hypothese(n) Beschreiben Sie die Vermutung(en), die Sie prüfen möchten, möglichst exakt. Deskriptive Statistiken Berichten Sie deskriptive Statistiken für alle Variablen der Hypothesen. Berichten Sie aber nur univariate Statistiken sowie Subgruppenanalysen dazu. Berichten Sie ggf. Effektstärken. Diagramme Visualisieren Sie Ihre Hypothese(n) bzw. die Daten dazu, gerne aus mehreren Blickwinkeln. Signifikanztest Explorative Analyse Eörtern Sie interessante Einblicke, die über Ihre vorab getroffenen Hypothesen hinausgehen. Diagramme können hier eine zentrale Rolle spielen. Diskussion Zentrale Ergebnisse Fassen Sie das zentrale Ergebnisse zusammen. Interpretation Interpretieren Sie die Ergebnisse: Was bedeuten die Zahlen/Fakten, die die Rechnungen ergeben haben? Grenzen der Analyse Schildern Sie etwaige Schwachpunkte oder Einschränkungen der Analyse. Geben Sie Anregungen für weiterführende Analysen dieses Datensatzes. 2.4 Literatur Zum Bestehen der Prüfung ist keine weitere Literatur fomal notwendig; allerdings ist es hilfreich, den Stoff aus unterschiedlichen Blickwinkeln aufzuarbeiten. Dazu ist am ehesten das Buch von Wickham und Grolemund (Wickham and Grolemund 2016) hilreich, obwohl es deutlich tiefer geht als dieses Skript. Literaturverzeichnis "],
["i-explorieren.html", "I EXPLORIEREN", " I EXPLORIEREN "],
["rahmen.html", "Kapitel 3 Rahmen 3.1 Software installieren 3.2 ERRRstkontakt 3.3 Was ist Statistik? Wozu ist sie gut? 3.4 Befehlsübersicht 3.5 Verweise", " Kapitel 3 Rahmen Lernziele: Einen Überblick über die fünf wesentliche Schritte der Datenanalyse gewinnen. R und RStudio installieren können. Einige häufige technische Probleme zu lösen wissen. R-Pakete installieren können. Einige grundlegende R-Funktionalitäten verstehen. Auf die Frage “Was ist Statistik?” eine Antwort geben können. In diesem Skript geht es um die Praxis der Datenanalyse. Mit Rahmen ist das “Drumherum” oder der Kontext der eigentlichen Datenanalyse gemeint. Dazu gehören einige praktische Vorbereitungen und ein paar Überlegungen. Zum Beispiel brauchen wir einen Überblick über das Thema. Voilà (Abb. 3.1): Abbildung 3.1: Der Prozess der Datenanalyse Datenanalyse, praktisch betrachtet, kann man in fünf Schritte einteilen (Wickham and Grolemund 2016). Zuerst muss man die Daten einlesen, die Daten also in R (oder einer anderen Software) verfügbar machen (laden). Fügen wir hinzu: In schöner Form verfügbar machen; man nennt dies auch tidy data[hört sich cooler an]. Sobald die Daten in geeigneter Form in R geladen sind, folgt das Aufbereiten. Das beinhaltet Zusammenfassen, Umformen oder Anreichern je nach Bedarf. Ein nächster wesentlicher Schritt ist das Visualisieren der Daten. Ein Bild sagt bekanntlich mehr als viele Worte. Schließlich folgt das Modellieren oder das Hypothesen prüfen: Man überlegt sich, wie sich die Daten erklären lassen könnten. Zu beachten ist, dass diese drei Schritte - Aufbereiten, Visualisieren, Modellieren - keine starre Abfolge sind, sondern eher ein munteres Hin-und-Her-Springen, ein aufbauendes Abwechseln. Der letzte Schritt ist das Kommunizieren der Ergebnisse der Analyse - nicht der Daten. Niemand ist an Zahlenwüsten interessiert; es gilt, spannende Einblicke zu vermitteln. Der Prozess der Datenanalyse vollzieht sich nicht im luftleeren Raum, sondern ist in einem Rahmen eingebettet. Dieser beinhaltet praktische Aspekte - wie Software, Datensätze - und grundsätzliche Überlegungen - wie Ziele und Grundannahmen. 3.1 Software installieren Als Haupt-Analysewerkzeug nutzen wir R; daneben wird uns die sog. “Entwicklungsumgebung” RStudio einiges an komfortabler Funktionalität bescheren. Eine Reihe von R-Paketen (“Packages”; d.h. Erweiterungen) werden wir auch nutzen. R ist eine recht alte Sprache; viele Neuerungen finden in Paketen Niederschlag, da der “harte Kern” von R lieber nicht so stark geändert wird. Stellen Sie sich vor: Seit 29 Jahren nutzen Sie eine Befehl, der Ihnen einen Mittelwert ausrechnet, sagen wir die mittlere Anzahl von Tassen Kaffee am Tag. Und auf einmal wird der Mittelwert anders berechnet?! Eine Welt stürzt ein! Naja, vielleicht nicht ganz so tragisch in dem Beispiel, aber grundsätzlich sind Änderungen in viel benutzen Befehlen potenziell problematisch. Das ist wohl ein Grund, warum sich am “R-Kern” nicht so viel ändert. Die Innovationen in R passieren in den Paketen. Und es gibt viele davon; als ich diese Zeilen schreibe, sind es fast schon 10.000! Genauer: 9937 nach dieser Quelle: https://cran.r-project.org/web/packages/. 3.1.1 R und RStudio installieren Sie können R unter https://cran.r-project.org herunterladen und installieren (für Windows, Mac oder Linux). RStudio finden Sie auf der gleichnamigen Homepage: https://www.rstudio.com; laden Sie die “Desktop-Version” für Ihr Betriebssystem herunter. Die Oberfläche von R, die “Console”, sieht so aus: Die Oberfläche von RStudio sieht (unter allen Betriebssystemen etwa gleich) so aus: Das Skript-Fenster ähnelt einem normalem Text-Editor; praktischerweise finden Sie aber einen Button “run”, der die aktuelle Zeile oder die Auswahl “abschickt”, d.h. in die Konsole gibt, wo die Syntax ausgeführt wird. Wenn Sie ein Skript-Fenster öffnen möchten, so können Sie das Icon klicken2. Aus dem Fenster der Konsole spricht R zu uns bzw. wir mit ihm (ihr?). Wird ein Befehl hier eingegeben, so führt R ihn aus. Es ist aber viel praktischer, Befehle in das Skript-Fenster einzugeben, als in die Konsole. Behalten Sie dieses Fenster im Blick, wenn Sie Antwort von R erwarten. Im Fenster Umgebung (engl. “environment”) zeigt R, welche Variablen (Objekte) vorhanden sind. Stellen Sie sich die Umgebung wie einen Karpfenteich vor, in dem die Datensätze und andere Objekte herumschwimmen. Was nicht in der Umgebung angezeigt wird, existiert nicht für R. Im Fenster rechts unten werden mehrere Informationen bereit gestellt, z.B. werden Diagramme (Plots) dort ausgegeben. Klicken Sie mal die anderen Reiter im Fenster rechts unten durch. Wer Shortcuts mag, wird in RStudio überschwänglich beschenkt; der Shortcut für die Shortcuts ist Shift-Alt-K. 3.1.2 Hilfe! R tut nicht so wie ich das will Manntje, Manntje, Timpe Te, Buttje, Buttje inne See, myne Fru de Ilsebill will nich so, as ik wol will. Gebrüder Grimm, Märchen vom Fischer und seiner Frau3 Ihr R startet nicht oder nicht richtig? Die drei wichtigsten Heilmittel sind: Schließen Sie die Augen für eine Minute. Denken Sie an etwas Schönes und was Rs Problem sein könnte. Schalten Sie den Rechner aus und probieren Sie es morgen noch einmal. Googeln. Sorry für die schnoddrigen Tipps. Aber: Es passiert allzu leicht, dass man Fehler wie diese macht: install.packages(dplyr) install.packages(&quot;dliar&quot;) install.packages(&quot;derpyler&quot;) install.packages(&quot;dplyr&quot;) # dependencies vergessen Keine Internet-Verbindung library(dplyr) # ohne vorher zu installieren Wenn R oder RStudio dann immer noch nicht starten oder nicht richtig laufen, probieren Sie dieses: Sehen Sie eine Fehlermeldung, die von einem fehlenden Paket spricht (z.B. “Package ‘Rcpp’ not available”) oder davon spricht, dass ein Paket nicht installiert werden konnte (z.B. “Package ‘Rcpp’ could not be installed” oder “es gibt kein Paket namens ‘Rcpp’” oder “unable to move temporary installation XXX to YYY”), dann tun Sie folgendes: Schließen Sie R und starten Sie es neu. Installieren Sie das oder die angesprochenen Pakete mit install.packages(&quot;name_des_pakets&quot;, dependencies = TRUE) oder mit dem entsprechenden Klick in RStudio. Starten Sie das entsprechende Paket mit library(paket_name). Gerade bei Windows 10 scheinen die Schreibrechte für R (und damit RStudio oder RCommander) eingeschränkt zu sein. Ohne Schreibrechte kann R aber nicht die Pakete (“packages”) installieren, die Sie für bestimmte R-Funktionen benötigen. Daher schließen Sie R bzw. RStudio und suchen Sie das Icon von R oder wenn Sie RStudio verwenden von RStudio. Rechtsklicken Sie das Icon und wählen Sie “als Administrator ausführen”. Damit geben Sie dem Programm Schreibrechte. Jetzt können Sie etwaige fehlende Pakete installieren. Ein weiterer Grund, warum R bzw. RStudio die Schreibrechte verwehrt werden könnten (und damit die Installation von Paketen), ist ein Virenscanner. Der Virenscanner sagt, nicht ganz zu Unrecht: “Moment, einfach hier Software zu installieren, das geht nicht, zu gefährlich”. Grundsätzlich gut, in diesem Fall unnötig. Schließen Sie R/RStudio und schalten Sie dann den Virenscanner komplett (!) aus. Öffnen Sie dann R/RStudio wieder und versuchen Sie fehlende Pakete zu installieren. 3.1.3 Hier werden Sie geholfen Es ist keine Schande, nicht alle Befehle der ca. 10,000 R-Pakete auswendig zu wissen. Schlauer ist, zu wissen, wo man Antworten findet. Hier eine Auswahl: Zu diesen Paketen gibt es gute “Spickzettel” (cheatsheets): ggplot2, RMarkdown, dplyr, tidyr. Klicken Sie dazu in RStudio auf Help &gt; Cheatsheets &gt; … oder gehen Sie auf https://www.rstudio.com/resources/cheatsheets/. In RStudio gibt es eine Reihe (viele) von Tastaturkürzeln (Shortcuts), die Sie hier finden: Tools &gt; Keyboard Shortcuts Help. Für jeden Befehl (d.i. Funktion) können Sie mit ? Hilfe erhalten; probieren Sie z.B. ?mean. Im Internet finden sich zuhauf Tutorials. Die bekannteste Seite, um Fragen rund um R zu diskutieren ist http://stackoverflow.com. 3.1.4 Die Denk- und Gefühlswelt von R Wenn Sie RStudio starten, startet R automatisch auch. Starten Sie daher, wenn Sie RStudio gestartet haben, nicht noch extra R. Damit hätten Sie sonst zwei Instanzen von R laufen, was zu Verwirrungen (bei R und beim Nutzer) führen kann. 3.1.4.1 R-Skript-Dateien Ein neues R-Skript im RStudio können Sie z.B. öffnen mit File-New File-R Script. Schreiben Sie dort Ihre R-Befehle; Sie können die Skriptdatei speichern, öffnen, ausdrucken, übers Bett hängen… R-Skripte können Sie speichern (File-Save) und öffnen. R-Skripte sind einfache Textdateien, die jeder Texteditor verarbeiten kann. Nur statt der Endung .txt, sind R-Skripte stolzer Träger der Endung .R. Es bleibt aber eine schnöde Textdatei. Geben Sie Ihren R-Skript-Dateien die Endung “.R”, damit erkennt RStudio, dass es sich um ein R-Skript handelt und bietet ein paar praktische Funktionen wie den “Run-Button”. 3.1.4.2 Stolpersteine beim Errisch lernen I Errr, therefore I am… Verwenden Sie möglichst die neueste Version von R, RStudio und Ihres Betriebssystems. Ältere Versionen führen u.U. zu Problemen; je älter, desto Problem… Updaten Sie Ihre Packages regelmäßig z.B. mit update.packages() oder dem Button “Update” bei RStudio (Reiter Packages). R zu lernen kann hart sein. Ich weiß, wovon ich spreche. Wahrscheinlich eine spirituelle Prüfung in Geduld und Hartnäckigkeit… Tolle Gelegenheit, sich in diesen Tugenden zu trainieren :-) 3.1.5 Pakete installieren Ein R-Paket, welches für die praktische Datenanalyse praktisch ist, heißt dplyr. Wir werden viel mit diesem Paket arbeiten. Bitte installieren Sie es schon einmal, sofern noch nicht geschehen: install.packages(&quot;dplyr&quot;, dependencies = TRUE) Beim Installieren von R-Paketen könnten Sie gefragt werden, welchen “Mirror” Sie verwenden möchten. Das hat folgenden Hintergrund: R-Pakete sind in einer Art “App-Store”, mit Namen CRAN (Comprehense R Archive Network) gespeichert. Damit nicht ein armer, kleiner Server überlastet wird, wenn alle Studis dieser Welt just gerade beschließen, ein Paket herunterzuladen, gibt es viele Kopien dieses Servers - die “Mirrors”, Spiegelbilder. Suchen Sie sich einfach einen aus, der in der Nähe ist. Bei der Installation von Paketen mit install.packages(&quot;name_des_pakets&quot;) sollte stets der Parameter dependencies = TRUE angefügt werden. Also install.packages(&quot;name_des_pakets&quot;, dependencies = TRUE). Hintergrund ist: Falls das zu installierende Paket seinerseits Pakete benötigt, die noch nicht installiert sind (gut möglich), dann werden diese sog. “dependencies” gleich mitinstalliert (wenn Sie dependencies = TRUE setzen). Sie müssen online sein, um Packages zu installieren. Nicht vergessen: Installieren muss man eine Software nur einmal; starten (laden) muss man sie jedes Mal, wenn man sie vorher geschlossen hat und wieder nutzen möchte: library(dplyr) Der Befehl bedeutet sinngemäß: “Hey R, geh in die Bücherei (library) und hole das Buch (package) dplyr!”. Wann benutzt man bei R Anführungszeichen? Das ist etwas verwirrend im Detail, aber die Grundegel lautet: wenn man Text anspricht. Im Beispiel oben “library(dplyr)” ist “dplyr” hier erst mal für R nichts Bekanntes, weil noch nicht geladen. Demnach müssten eigentlich Anführungsstriche stehen. Allerdings meinte ein Programmierer, dass es doch so bequemer ist. Hat er Recht. Aber bedenken Sie, dass es sich um die Ausnahme einer Regel handelt. Sie können also auch schreiben: library(“dplyr”) oder library(‘dplyr’); geht beides. Das Installieren und Starten anderer Pakete läuft genauso ab. Am besten installieren Sie alle Pakete, die wir in diesem Buch benötigen auf einmal, dann haben Sie Ruhe. 3.1.6 R-Pakete für dieses Buch In diesem Buch verwenden wir die folgenden R-Pakete; diese müssen installiert4 sein und geladen: Pakete #&gt; [1] &quot;tidyverse&quot; &quot;readr&quot; &quot;knitr&quot; &quot;stringr&quot; #&gt; [5] &quot;car&quot; &quot;nycflights13&quot; &quot;ISLR&quot; &quot;pdftools&quot; #&gt; [9] &quot;downloader&quot; &quot;ggdendro&quot; &quot;gridExtra&quot; &quot;tm&quot; #&gt; [13] &quot;tidytext&quot; &quot;lsa&quot; &quot;SnowballC&quot; &quot;wordcloud&quot; #&gt; [17] &quot;RColorBrewer&quot; &quot;okcupiddata&quot; &quot;reshape2&quot; &quot;wesanderson&quot; #&gt; [21] &quot;GGally&quot; &quot;titanic&quot; &quot;compute.es&quot; &quot;corrr&quot; #&gt; [25] &quot;rpart&quot; &quot;rpart.plot&quot; &quot;MASS&quot; &quot;titanic&quot; #&gt; [29] &quot;arules&quot; &quot;arulesViz&quot; &quot;SDMTools&quot; &quot;corrplot&quot; #&gt; [33] &quot;gplots&quot; &quot;corrplot&quot; &quot;scatterplot3d&quot; &quot;BaylorEdPsych&quot; #&gt; [37] &quot;nFactors&quot; &quot;rmarkdown&quot; &quot;methods&quot; 3.1.7 Datensätze Datensatz profiles aus dem R-Paket {okcupiddata} (Kim and Escobedo-Land 2015); es handelt sich um Daten von einer Online-Singlebörse Datensatz Wage aus dem R-Paket {ISLR} (James, Witten, Hastie, and Tibshirani 2013b); es handelt sich um Gehaltsdaten von US-amerikanischen Männern Datensatz inf_test_short, hier herunterzuladen: &lt;osf.io/sjhu&gt; (Sauer 2017a); es handelt sich um Ergebnisse einer Statistikklausur Datensatz flights aus dem R-Paket {nycflights13} (RITA 2013); es handelt sich um Abflüge von den New Yorker Flughäfen Datensatz ’wo_men`, hier herunterzuladen: &lt;osf.io/ja9dw&gt; (Sauer 2017b); es handelt sich um Körper- und Schuhgröße von Studierenden Datensatz tips aus dem R-Paket {reshape2} (Bryant and Smith 1995); es handelt sich um Trinkgelder in einem Restaurant Datensatz extra, hier herunterzuladen: &lt;osf.io/4kgzh&gt; (Sauer 2016); es handelt sich die Ergebnisse einer Umfrage zu Extraversion Wir verwenden zwei Methoden, um Datensätze in R zu laden. Zum einen laden wir Datensätze aus R-Paketen, z.B. aus dem Paket okcupiddata. Dazu muss das entsprechende Paket installiert und geladen sein. Mit dem Befehl data(name_des_datensatzes, package = &quot;name_des_paketes&quot;), kann man dann die Daten laden. Das Laden eines Pakets lädt noch nicht die Daten des Paketes; dafür ist der Befehl data zuständig. library(okcupiddata) data(profiles, package = &quot;okcupiddata&quot;) Alternativ kann man die Daten als CSV- oder als XLS(X)-Datei importieren. Die Datei darf dabei sowohl auf einer Webseite als auch lokal (Festplatte, Stick…) liegen. Daten &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/tips.csv&quot;) Wir werden mit beiden Methoden arbeiten und “on the job” Details besprechen. 3.1.8 Übungen Öffnen Sie das Cheatsheet für RStudio und machen Sie sich mit dem Cheatsheet vertraut. Sichten Sie kurz die übrigen Cheatsheets; später werden die Ihnen vielleicht von Nutzen sein. 3.2 ERRRstkontakt 3.2.1 Hinweise Unser erster Kontakt mit R! Ein paar Anmerkungen vorweg: R unterscheidet zwischen Groß- und Kleinbuchstaben, d.h. Oma und oma sind zwei verschiedene Dinge für R! R verwendet den Punkt . als Dezimaltrennzeichen. Fehlende Werte werden in R durch NA kodiert. Kommentare werden mit dem Rautezeichen # eingeleitet; der Rest der Zeile von von R dann ignoriert. Hilfe zu einem Befehl erhält man über ein vorgestelltes Fragezeichen ?. Zusätzliche Funktionalität kann über Zusatzpakete hinzugeladen werden. Diese müssen ggf. zunächst installiert werden. Variablennamen (synonym: Objekte) in R müssen mit Buchstaben beginnen; ansonsten dürfen nur Zahlen, Unterstriche - und Minuszeichen - enthalten sein. Leerzeichen sind nicht erlaubt. Variablen einen Namen zu geben, ist nicht leicht, aber wichtig. Namen sollten knapp, aber aussagekräftig sein. # so nicht: var x dummy objekt dieser_name_ist_etwas_lang_vielleicht # gut: tips_mw lm1 Um den Inhalt einer Variablen auszulesen, geben wir einfach den Namen des Objekts ein (und schicken den Befehl ab). 3.2.2 R als Taschenrechner Auch wenn Statistik nicht Mathe ist, so kann man mit R auch rechnen. Geben Sie zum Üben die Befehle in der R Konsole hinter der Eingabeaufforderung &gt; ein und beenden Sie die Eingabe mit Return bzw. Enter. 4+2 #&gt; [1] 6 Das Ergebnis wird direkt angezeigt. Bei x &lt;- 4+2 erscheint zunächst kein Ergebnis. Über &lt;- wird der Variable x der Wert 4+2 zugewiesen. Wenn Sie jetzt x eingeben, wird das Ergebnis #&gt; [1] 6 angezeigt. Sie können jetzt auch mit x weiterrechnen, z.B.: x/4 #&gt; [1] 1.5 Vielleicht fragen Sie sich was die [1] vor dem Ergebnis bedeutet. R arbeitet vektororientiert, und die [1] zeigt an, dass es sich um das erste (und hier auch letzte) Element des Vektors handelt. 3.2.3 Text und Variablen zuweisen Man kann einer Variablen auch Text zuweisen (im Gegensatz zu Zahlen): y &lt;- &quot;Hallo R!&quot; Man kann auch einer Variablen eine andere zuweisen: y &lt;- x Wird jetzt y mit dem Inhalt von x überschrieben oder umgekehrt? Der Zuweisungspfeil &lt;- macht die Richtung der Zuweisung ganz klar. Zwar ist in R das Gleichheitszeichen synonym zum Zuweisungspfeil erlaubt, aber der Zuweisungspfeil macht die Sache glasklar und sollte daher bevorzugt werden. Man kann auch einer Variablen mehr als einen Wert zuweisen: x &lt;- c(1, 2, 3) Dieser Befehl erzeugt eine “Spalte” (einen Vektor). Will man einer Variablen mehr als einen Wert zuweisen, muss man die Werte erst in einen Vektor “zusammen binden”; das geht mit dem Befehl c (wie combine). 3.2.4 Funktionen aufrufen Um einen “Befehl” (präziser: eine Funktion) aufzurufen, geben wir ihren Namen an und definieren sog. “Parameter” in einer runden Klammer, z.B. so: wo_men &lt;- read.csv(&quot;data/wo_men.csv&quot;) Allgemein gesprochen: funktionsname(parametername1 = wert1, parametername2 = wert2, ...) Die drei Punkte ... sollen andeuten, dass evtl. weitere Parameter zu übergeben wären. Die Reihenfolge der Parameter ist egal - wenn man die Parameternamen anführt. Ansonsten muss man sich an die Standard-Reihenfolge, die eine Funktion vorgibt halten: #ok: wo_men &lt;- read.csv(file = &quot;data/wo_men.csv&quot;, header = TRUE, sep = &quot;,&quot;) wo_men &lt;- read.csv(&quot;data/wo_men.csv&quot;, TRUE, &quot;,&quot;) wo_men &lt;- read.csv(header = TRUE, sep = &quot;,&quot;, file = &quot;data/wo_men.csv&quot;) # ohno: wo_men &lt;- read.csv(TRUE, &quot;data/wo_men.csv&quot;, &quot;,&quot;) 3.2.5 Übungen Führen Sie diese Syntax aus: meine_coole_variable &lt;- 10 meine_coole_var1able Woher rührt der Fehler? Korrigieren Sie die Syntax: install.packages(dplyer) y &lt;- Hallo R! Hallo R &lt;- 1 Hallo_R &lt; - 1 3.3 Was ist Statistik? Wozu ist sie gut? Zwei Fragen bieten sich sich am Anfang der Beschäftigung mit jedem Thema an: Was ist die Essenz des Themas? Warum ist das Thema (oder die Beschäftigung damit) wichtig? Was ist Statistik? Eine Antwort dazu ist, dass Statistik die Wissenschaft von Sammlung, Analyse, Interpretation und Kommunikation von Daten ist mithilfe mathematischer Verfahren ist und zur Entscheidungshilfe beitragen solle (The Oxford Dictionary of Statistical Terms 2006; Romeijn 2016). Damit hätten wir auch den Unterschied zur schnöden Datenanalyse (ein Teil der Statistik) herausgemeißelt. Statistik wird häufig in die zwei Gebiete deskriptive und inferierende Statistik eingeteilt. Erstere fasst viele Zahlen zusammen, so dass wir den Wald statt vieler Bäume sehen. Letztere verallgemeinert von den vorliegenden (sog. “Stichproben-”)Daten auf eine zugrunde liegende Grundmenge (Population). Dabei spielt die Wahrscheinlichkeitsrechnung (Stochastik) eine große Rolle. Aufgabe der deskriptiven Statistik ist es primär, Daten prägnant zusammenzufassen. Aufgabe der Inferenzstatistik ist es, zu prüfen, ob Daten einer Stichprobe auf eine Grundgesamtheit verallgemeinert werden können. Dabei lässt sich der Begriff “Statistik” als Überbegriff von “Datenanalyse” verstehen, wenn diese Sicht auch nicht von allen geteilt wird (Grolemund and Wickham 2014). In diesem Buch steht die Aufbereitung, Analyse, Interpretation und Kommunikation von Daten im Vordergrund. Liegt der Schwerpunkt dieser Aktivitäten bei computerintensiven Methoden, so wird auch von Data Science gesprochen, wobei der Begriff nicht einheitlich verwendet wird (Wickham and Grolemund 2016; Hardin et al. 2015) Daten kann man definieren als Informationen, die in einem Kontext stehen (Moore 1990), wobei eine numerische Konnotation mitschwingt. Modellieren kann man als zentrale Aufgabe von Statistik begreifen (Cobb 2007; Grolemund and Wickham 2014). Einfach gesprochen, bedeutet Modellieren in diesem Sinne, ein mathematisches Narrativ (“Geschichte”) zu finden, welches als Erklärung für gewisse Muster in den Daten fungiert; vgl. Kap. 10. Statistisches Modellieren läuft gewöhnlich nach folgendem Muster ab (Grolemund and Wickham 2014): Prämisse 1: Wenn Modell M wahr ist, dann sollten die Daten das Muster D aufweisen. Prämisse 2: Die Daten weisen das Muster D auf. --- Konklusion: Daher muss das Modell M wahr sein. Die Konklusion ist nicht zwangsläufig richtig. Es ist falsch zu sagen, dass dieses Argumentationsmuster - Abduktion (Peirce 1955) genannt - wahre, sichere Schlüsse (Konklusionen) liefert. Die Konklusion kann, muss aber nicht, zutreffen. Ein Beispiel: Auf dem Nachhauseweg eines langen Arbeitstags wartet, in einer dunklen Ecke, ein Mann, der sich als Statistik-Professor vorstellt und Sie zu einem Glücksspiel einlädt. Sofort sagen Sie zu. Der Statistiker will 10 Mal eine Münze werfen, er setzt auf Zahl (versteht sich). Wenn er gewinnt, bekommt er 10€ von Ihnen; gewinnen Sie, bekommen Sie 11€ von ihm. Hört sich gut an, oder? Nun wirft er die Münze zehn Mal. Was passiert? Er gewinnt 10 Mal, natürlich (so will es die Geschichte). Sollten wir glauben, dass er ein Betrüger ist? Ein Modell, welches wir hier verwenden könnten, lautet: Wenn die Münze gezinkt ist (Modell M zutrifft), dann wäre diese Datenlage D (10 von 10 Treffern) wahrscheinlich - Prämisse 1. Datenlage D ist tatsächlich der Fall; der Statistiker hat 10 von 10 Treffer erzielt - Prämisse 2. Die Daten D “passen” also zum Modell M; man entscheidet sich, dass der Professor ein Falschspieler ist. Wichtig zu erkennen ist, dass Abduktion mit dem Wörtchen wenn beginnt. Also davon ausgeht, dass ein Modell M der Fall ist (der Professor also tatsächlich ein Betrüger ist). Dass, worüber wir entscheiden wollen, wird also bereits vorausgesetzt. Gilt also M, wie gut passen dann die Daten dazu? Wie gut passen die Daten D zum Modell M? Das ist die Frage, die hier tatsächlich gestellt bzw. beantwortet wird. Natürlich ist es keineswegs sicher, dass das Modell gilt. Darüber macht die Abduktion auch keine Aussage. Es könnte also sein, dass ein anderes Modell zutrifft: Der Professor könnte ein Heiliger sein, der uns auf etwas merkwürdige Art versucht, Geld zuzuschanzen… Oder er hat einfach Glück gehabt. Statistische Modelle beantworten i.d.R. nicht, wie wahrsheinlich es ist, dass ein Modell gilt. Statistische Modelle beurteilen, wie gut Daten zu einem Modell passen. Häufig trifft ein Modell eine Reihe von Annahmen, die nicht immer explizit gemacht werden, aber die klar sein sollten. Z.B. sind die Münzwürfe unabhängig voneinander? Oder kann es sein, dass sich die Münze “einschießt” auf eine Seite? Dann wären die Münzwürfe nicht unabhängig voneinander. In diesem Fall klingt das reichlich unplausibel; in anderen Fällen kann dies eher der Fall sein5. Auch wenn die Münzwürfe unabhängig voneinander sind, ist die Wahrscheinlichkeit für Zahl jedes Mal gleich? Hier ist es wiederum unwahrscheinlich, dass sich die Münze verändert, ihre Masse verlagert, so dass eine Seite Unwucht bekommt. In anderen Situationen können sich Untersuchungsobjekte verändern (Menschen lernen manchmal etwas, sagt man), so dass die Wahrscheinlichkeiten für ein Ereignis unterschiedlich sein können, man dies aber nicht berücksichtigt. 3.4 Befehlsübersicht Funktion Beschreibung install.packages installiert ein Paket library lädt ein Paket &lt;- Weist einer Variablen einen Wert zu c erstellt eine Spalte/ einen Vektor Diese Befehle “wohnen” alle im Standard-R; es ist für diese Befehle nicht nötig, zusätzliche Pakete zu installieren/ laden. 3.5 Verweise Chester Ismay erläutert einige Grundlagen von R und RStudio, die für Datenanalyse hilfreich sind: https://bookdown.org/chesterismay/rbasics/. Roger Peng und Kollegen bieten hier einen Einstieg in Data Science mit R: https://bookdown.org/rdpeng/artofdatascience/ Wickam und Grolemund (2016) geben einen hervorragenden Überblick in das Thema dieses Buches; ihr Buch ist sehr zu empfehlen. Wer einen stärker an der Statistik orientierten Zugang sucht, aber “mathematisch sanft” behandelt werden möchte, wird bei James et al. (2013b) glücklich oder zumindest fündig werden. Literaturverzeichnis "],
["daten-einlesen.html", "Kapitel 4 Daten einlesen 4.1 Daten in R importieren 4.2 Normalform einer Tabelle 4.3 Vertiefung 4.4 Befehlsübersicht 4.5 Übungen10 4.6 Verweise", " Kapitel 4 Daten einlesen Lernziele: Wissen, was eine CSV-Datei ist. Wissen, was UTF-8 bedeutet. Erläutern können, was R unter dem “working directory” versteht. Erkennen können, ob eine Tabelle in Normalform vorliegt. Daten aus R hinauskriegen (exportieren). Dieses Kapitel beantwortet eine Frage: “Wie kriege ich Daten in vernünftiger Form in R hinein?”. Abbildung 4.1: Daten sauber einlesen 4.1 Daten in R importieren In R kann man ohne Weiteres verschiedene, gebräuchliche (Excel oder CSV) oder weniger gebräuchliche (Feather6) Datenformate einlesen. In RStudio lässt sich dies z.B. durch einen schnellen Klick auf Import Dataset im Reiter Environment erledigen7. 4.1.1 Excel-Dateien importieren Am einfachsten ist es, eine Excel-Datei (.xls oder .xlsx) über die RStudio-Oberfläche zu importieren; das ist mit ein paar Klicks geschehen8: Abbildung 4.2: Daten einlesen (importieren) mit RStudio Es ist für bestimmte Zwecke sinnvoll, nicht zu klicken, sondern die Syntax einzutippen. Zum Beispiel: Wenn Sie die komplette Analyse als Syntax in einer Datei haben (eine sog. “Skriptdatei”), dann brauchen Sie (in RStudio) nur alles auszuwählen und auf Run zu klicken, und die komplette Analyse läuft durch! Die Erfahrung zeigt, dass das ein praktisches Vorgehen ist. Daten (CSV, Excel,…) können Sie nicht öffnen über File &gt; Open File .... Dieser Weg ist Skript-Dateien vorbehalten. 4.1.2 CSV-Dateien importieren Die gebräuchlichste Form von Daten für statistische Analysen ist wahrscheinlich das CSV-Format. Das ist ein einfaches Format, basierend auf einer Textdatei. Schauen Sie sich mal diesen Auszug aus einer CSV-Datei an. &quot;ID&quot;,&quot;time&quot;,&quot;sex&quot;,&quot;height&quot;,&quot;shoe_size&quot; &quot;1&quot;,&quot;04.10.2016 17:58:51&quot;,NA,160.1,40 &quot;2&quot;,&quot;04.10.2016 17:58:59&quot;,&quot;woman&quot;,171.2,39 &quot;3&quot;,&quot;04.10.2016 18:00:15&quot;,&quot;woman&quot;,174.2,39 &quot;4&quot;,&quot;04.10.2016 18:01:17&quot;,&quot;woman&quot;,176.4,40 &quot;5&quot;,&quot;04.10.2016 18:01:22&quot;,&quot;man&quot;,195.2,46 Erkennen Sie das Muster? Die erste Zeile gibt die “Spaltenköpfe” wieder, also die Namen der Variablen. Hier sind es 5 Spalten; die vierte heißt “shoe_size”. Die Spalten sind offenbar durch Komma , voneinander getrennt. Dezimalstellen sind in amerikanischer Manier mit einem Punkt . dargestellt. Die Daten sind “rechteckig”; alle Spalten haben gleich viele Zeilen und umgekehrt alle Spalten gleich viele Zeilen. Man kann sich diese Tabelle gut als Excel-Tabelle mit Zellen vorstellen, in denen z.B. “ID” (Zelle oben links) oder “46” (Zelle unten rechts) steht. An einer Stelle steht NA. Das ist Errisch für “fehlender Wert”. Häufig wird die Zelle auch leer gelassen, um auszudrücken, dass ein Wert hier fehlt (hört sich nicht ganz doof an). Aber man findet alle möglichen Ideen, um fehlende Werte darzustellen. Ich rate von allen anderen ab; führt nur zu Verwirrung. Lesen wir diese Daten jetzt ein: daten &lt;- read.csv(&quot;data/wo_men.csv&quot;) Der Befehl read.csv liest also eine CSV-Datei, was uns jetzt nicht übermäßig überrascht. Aber Achtung: Wenn Sie aus einem Excel mit deutscher Einstellung eine CSV-Datei exportieren, wird diese CSV-Datei als Trennzeichen ; (Strichpunkt) und als Dezimaltrennzeichen , verwenden. Da der Befehl read.csv als Standard mit Komma und Punkt arbeitet, müssen wir die deutschen Sonderlocken explizit angeben, z.B. so: # nicht ausführen: daten_deutsch &lt;- read.csv(&quot;daten_deutsch.csv&quot;, sep = &quot;;&quot;, dec = &quot;.&quot;) Dabei steht sep (separator) für das Trennzeichen zwischen den Spalten und dec für das Dezimaltrennzeichen. R bietet eine Kurzfassung für read.csv mit diesen Parametern: read.csv2(&quot;daten_deutsch.csv&quot;). 4.1.3 Vertiefung: Einlesen mit Prüfung #&gt; X time sex height shoe_size #&gt; 1 1 04.10.2016 17:58:51 woman 160 40 #&gt; 2 2 04.10.2016 17:58:59 woman 171 39 #&gt; 3 3 04.10.2016 18:00:15 woman 174 39 #&gt; 4 4 04.10.2016 18:01:17 woman 176 40 #&gt; 5 5 04.10.2016 18:01:22 man 195 46 #&gt; 6 6 04.10.2016 18:01:53 woman 157 37 Wir haben zuerst geprüft, ob die Datei (wo_men.csv) im entsprechenden Ordner existiert oder nicht (das !-Zeichen heißt auf Errisch “nicht”). Falls die Datei nicht im Ordner existiert, laden wir sie mit read.csv herunter und direkt ins R hinein. Andernfalls (else) lesen wir sie direkt ins R hinein. 4.1.4 Das Arbeitsverzeichnis Übrigens: Wenn Sie keinen Pfad angeben, so geht R davon aus, dass die Daten im aktuellen Verzeichnis (dem working directory) liegen. Das aktuelle Verzeichnis (Arbeitsverzeichnis; “working directory”) kann man mit getwd() erfragen und mit setwd() einstellen. Komfortabler ist es aber, das aktuelle Verzeichnis per Menü zu ändern. In RStudio: Session &gt; Set Working Directory &gt; Choose Directory ... (oder per Shortcut, der dort angezeigt wird). Es ist praktisch, das Arbeitsverzeichnis festzulegen, denn dann kann man z.B. eine Datendatei einlesen, ohne den Pfad eingeben zu müssen: # nicht ausführen: daten_deutsch &lt;- read.csv(&quot;daten_deutsch.csv&quot;, sep = &quot;;&quot;, dec = &quot;.&quot;) R geht dann davon aus, dass sich die Datei daten_deutsch.csv im Arbeitsverzeichnis befindet. 4.2 Normalform einer Tabelle Tabellen in R werden als data frames (“Dataframe” auf Denglisch; moderner: als tibble, Tibble kurz für “Table-df”) bezeichnet. Tabellen sollten in “Normalform” vorliegen (“tidy”), bevor wir weitere Analysen starten. Unter Normalform verstehen sich folgende Punkte: Es handelt sich um einen Dataframe, also um eine Tabelle mit Spalten mit Namen und gleicher Länge; eine Datentabelle in rechteckiger Form und die Spalten haben einen Namen. In jeder Zeile steht eine Beobachtung, in jeder Spalte eine Variable. Fehlende Werte sollten sich in leeren Zellen niederschlagen. Daten sollten nicht mit Farbmarkierungen o.ä. kodiert werden. Es gibt keine Leerzeilen und keine Leerspalten. In jeder Zelle steht ein Wert. Am besten verwendet man keine Sonderzeichen verwenden und keine Leerzeichen in Variablennamen und -werten, sondern nur Ziffern und Buchstaben und Unterstriche. Variablennamen dürfen nicht mit einer Zahl beginnen. Abbildung 4.3 visualisiert die Bestimmungsstücke eines Dataframes (Wickham and Grolemund 2016): Abbildung 4.3: Schematische Darstellung eines Dataframes in Normalform Der Punkt “Jede Zeile eine Beobachtung, jede Spalte eine Variable” verdient besondere Beachtung. Betrachten Sie dieses Beispiel: Abbildung 4.4: Dieselben Daten - einmal breit, einmal lang In der rechten Tabelle sind die Variablen Quartal und Umsatz klar getrennt; jede hat ihre eigene Spalte. In der linken Tabelle hingegen sind die beiden Variablen vermischt. Sie haben nicht mehr ihre eigene Spalte, sondern sind über vier Spalten verteilt. Die rechte Tabelle ist ein Beispiel für eine Tabelle in Normalform, die linke nicht. Abbildung 4.5: Illustration eines Datensatzes in Normalform 4.3 Vertiefung 4.3.1 Tabelle in Normalform bringen Eine der ersten Aktionen einer Datenanalyse sollte also die “Normalisierung” Ihrer Tabelle sein. In R bietet sich dazu das Paket tidyr an, mit dem die Tabelle von Breit- auf Langformat (und wieder zurück) geschoben werden kann. Ein Beispiel dazu: meindf &lt;- read.csv(&quot;http://stanford.edu/~ejdemyr/r-tutorials/data/unicef-u5mr.csv&quot;) df_lang &lt;- gather(meindf, year, u5mr, U5MR.1950:U5MR.2015) df_lang &lt;- separate(df_lang, year, into = c(&quot;U5MR&quot;, &quot;year&quot;), sep = &quot;.&quot;) Die erste Zeile liest die Daten aus einer CSV-Datei ein; praktischerweise direkt von einer Webseite. Die zweite Zeile gather formt die Daten von breit nach lang um. Die neuen Spalten, nach der Umformung heißen dann year und u5mr (Sterblichkeit bei Kindern unter fünf Jahren). In die Umformung werden die Spalten U5MR 1950 bis U5MR 2015 einbezogen. Die dritte Zeile separate entzerrt die Werte der Spalte year; hier stehen die ehemaligen Spaltenköpfe. Man nennt sie auch key Spalte daher. Steht in einer Zelle von year bspw. U5MR 1950, so wird U5MR in eine Spalte mit Namen U5MR und 1950 in eine Spalte mit Namen year geschrieben. 4.3.2 Textkodierung Öffnet man eine Textdatei mit einem Texteditor seiner Wahl, so sieht man… Text und sonst nichts, also keine Formatierung etc. Eine Textdatei besteht aus Text und sonst nichts (daher der Name…). Auch eine R-Skript-Datei (Coole_Syntax.R) ist eine Textdatei. Technisch gesprochen werden nur die Textzeichen gespeichert, sonst nichts; im Gegensatz dazu speichert eine Word-Datei noch mehr, z.B. Formatierung. Ein bestimmtes Zeichen wie “A” bekommt einen bestimmten Code wie “41”. Mit etwas Glück weiß der Computer jetzt, dass er das Zeichen “41” auf den Bildschirm ausgeben soll. Es stellt sich jetzt die Frage, welche Code-Tabelle der Computer nutzt? Welchem Code wird “A” (bzw. ein beliebiges Zeichen) zugeordnet? Mehrere solcher Kodierungstafeln existieren. Die gebräuchlichste im Internet heißt UTF-89. Leider benutzen unterschiedliche Betriebssysteme unterschiedliche Kodierungstafeln, was zu Verwirrung führt. Ich empfehle, ihre Textdateien als UTF-8 zu kodieren. RStudio fragt sie, wie eine Textdatei kodiert werden soll. Sie können auch unter File &gt; Save with Encoding... die Kodierung einer Textdatei festlegen. Speichern Sie R-Textdateien wie Skripte stets mit UTF-8-Kodierung ab. 4.3.3 Daten exportieren Wie bekommt man seine Daten wieder aus R raus (“ich will zu Excel zurück!”)? Eine Möglichkeit bietet die Funktion write.csv; sie schreibt eine CSV-Datei: write.csv(name_der_tabelle, &quot;Dateiname.csv&quot;) Mit help(write.csv) bekommt man mehr Hinweise dazu. Beachten Sie, dass immer in das aktuelle Arbeitsverzeichnis geschrieben wird. 4.4 Befehlsübersicht Paket::Funktion Beschreibung read.csv Liest eine CSV-Datei ein. write.csv Schreibt einen Dateframe in eine CSV-Datei. readr::gather Macht aus einem “breiten” Dataframe einen “langen”. readr::separate “Zieht” Spalten auseinander. 4.5 Übungen10 Richtig oder Falsch!? In CSV-Dateien dürfen Spalten nie durch Komma getrennt sein. RStudio bietet die Möglichkeit, CSV-Dateien per Klick zu importieren. RStudio bietet nicht die Möglichkeit, CSV-Dateien per Klick zu importieren. “Deutsche” CSV-Dateien verwenden als Spalten-Trennzeichen einen Strichpunkt. In einer Tabelle in Normalform stehen in jeder Zeile eine Beobachtung. In einer Tabelle in Normalform stehen in jeder Spalte eine Variable. R stellt fehlende Werte mit einem Fragezeichen ? dar. Um Excel-Dateien zu importieren, kann man den Befehl read.csv verwenden. 4.6 Verweise R for Data Science bietet umfangreiche Unterstützung zu diesem Thema (Wickham and Grolemund 2016). Literaturverzeichnis "],
["datenjudo.html", "Kapitel 5 Datenjudo 5.1 Typische Probleme 5.2 Daten aufbereiten mit dplyr 5.3 Die Pfeife 5.4 Befehlsübersicht 5.5 Verweise", " Kapitel 5 Datenjudo Lernziele: Typische Probleme der Datenanalyse schildern können. Zentrale dplyr-Befehle anwenden können. dplyr-Befehle kombinieren können. Die Pfeife anwenden können. Werte umkodieren und “binnen” können. Abbildung 5.1: Daten aufbereiten In diesem Kapitel benötigte Pakete: library(tidyverse) # Datenjudo library(stringr) # Texte bearbeiten library(car) # für &#39;recode&#39; Das Paket tidyverse lädt dplyr, ggplot2 und weitere Pakete11. Daher ist es komfortabler, tidyverse zu laden, damit spart man sich Tipparbeit. Die eigentliche Funktionalität, die wir in diesem Kapitel nutzen, kommt aus dem Paket dplyr. Mit Datenjudo ist gemeint, die Daten für die eigentliche Analyse “aufzubereiten”. Unter Aufbereiten ist hier das Umformen, Prüfen, Bereinigen, Gruppieren und Zusammenfassen von Daten gemeint. Die deskriptive Statistik fällt unter die Rubrik Aufbereiten. Kurz gesagt: Alles, wan tut, nachdem die Daten “da” sind und bevor man mit anspruchsvoller(er) Modellierung beginnt. Ist das Aufbereiten von Daten auch nicht statistisch anspruchsvoll, so ist es trotzdem von großer Bedeutung und häufig recht zeitintensiv. Eine Anekdote zur Relevanz der Datenaufbereitung, die (so will es die Geschichte) mir an einer Bar nach einer einschlägigen Konferenz erzählt wurde (daher keine Quellenangebe, Sie verstehen…). Eine Computerwissenschaftlerin aus den USA (deutschen Ursprungs) hatte einen beeindruckenden “Track Record” an Siegen in Wettkämpfen der Datenanalyse. Tatsächlich hatte sie keine besonderen, raffinierten Modellierungstechniken eingesetzt; klassische Regression war ihre Methode der Wahl. Bei einem Wettkampf, bei dem es darum ging, Krebsfälle aus Krankendaten vorherzusagen (z.B. von Röntgenbildern) fand sie nach langem Datenjudo heraus, dass in die “ID-Variablen” Information gesickert war, die dort nicht hingehörte und die sie nutzen konnte für überraschend (aus Sicht der Mitstreiter) gute Vorhersagen zu Krebsfällen. Wie war das möglich? Die Daten stammten aus mehreren Kliniken, jede Klinik verwendete ein anderes System, um IDs für Patienten zu erstellen. Überall waren die IDs stark genug, um die Anonymität der Patienten sicherzustellen, aber gleich wohl konnte man (nach einigem Judo) unterscheiden, welche ID von welcher Klinik stammte. Was das bringt? Einige Kliniken waren reine Screening-Zentren, die die Normalbevölkerung versorgte. Dort sind wenig Krebsfälle zu erwarten. Andere Kliniken jedoch waren Onkologie-Zentren für bereits bekannte Patienten oder für Patienten mit besonderer Risikolage. Wenig überraschen, dass man dann höhere Krebsraten vorhersagen kann. Eigentlich ganz einfach; besondere Mathe steht hier (zumindest in dieser Geschichte) nicht dahinter. Und, wenn man den Trick kennt, ganz einfach. Aber wie so oft ist es nicht leicht, den Trick zu finden. Sorgfältiges Datenjudo hat hier den Schlüssel zum Erfolg gebracht. 5.1 Typische Probleme Bevor man seine Statistik-Trickkiste so richtig schön aufmachen kann, muss man die Daten häufig erst noch in Form bringen. Das ist nicht schwierig in dem Sinne, dass es um komplizierte Mathe ginge. Allerdings braucht es mitunter recht viel Zeit und ein paar (oder viele) handwerkliche Tricks sind hilfreich. Hier soll das folgende Kapitel helfen. Typische Probleme, die immer wieder auftreten, sind: Fehlende Werte: Irgend jemand hat auf eine meiner schönen Fragen in der Umfrage nicht geantwortet! Unerwartete Daten: Auf die Frage, wie viele Facebook-Freunde er oder sie habe, schrieb die Person “I like you a lot”. Was tun??? Daten müssen umgeformt werden: Für jede der beiden Gruppen seiner Studie hat Joachim einen Google-Forms-Fragebogen aufgesetzt. Jetzt hat er zwei Tabellen, die er “verheiraten” möchte. Geht das? Neue Variablen (Spalten) berechnen: Ein Student fragt nach der Anzahl der richtigen Aufgaben in der Statistik-Probeklausur. Wir wollen helfen und im entsprechenden Datensatz eine Spalte erzeugen, in der pro Person die Anzahl der richtig beantworteten Fragen steht. 5.2 Daten aufbereiten mit dplyr Es gibt viele Möglichkeiten, Daten mit R aufzubereiten; dplyr12 ist ein populäres Paket dafür. Eine zentrale Idee von dplyr ist, dass es nur ein paar wenige Grundbausteine geben sollte, die sich gut kombinieren lassen. Sprich: Wenige grundlegende Funktionen mit eng umgrenzter Funktionalität. Der Autor, Hadley Wickham, sprach einmal in einem Forum (citation needed), dass diese Befehle wenig können, das Wenige aber gut. Ein Nachteil dieser Konzeption kann sein, dass man recht viele dieser Bausteine kombinieren muss, um zum gewünschten Ergebnis zu kommen. Außerdem muss man die Logik des Baukastens gut verstanden habe - die Lernkurve ist also erstmal steiler. Dafür ist man dann nicht darauf angewiesen, dass es irgendwo “Mrs Right” gibt, die genau das kann, was ich will. Außerdem braucht man sich auch nicht viele Funktionen merken. Es reicht einen kleinen Satz an Funktionen zu kennen (die praktischerweise konsistent in Syntax und Methodik sind). Willkommen in der Welt von dyplr! dplyr hat seinen Namen, weil es sich ausschließlich um Dataframes bemüht; es erwartet einen Dataframe als Eingabe und gibt einen Dataframe zurück (zumindest bei den meisten Befehlen). Diese Bausteine sind typische Tätigkeiten im Umgang mit Daten; nichts Überraschendes. Schauen wir uns diese Bausteine näher an. 5.2.1 Zeilen filtern mit filter Häufig will man bestimmte Zeilen aus einer Tabelle filtern; filter. Zum Beispiel man arbeitet für die Zigarettenindustrie und ist nur an den Rauchern interessiert (die im Übrigen unser Gesundheitssystem retten (Krämer 2011)), nicht an Nicht-Rauchern; es sollen die nur Umsatzzahlen des letzten Quartals untersucht werden, nicht die vorherigen Quartale; es sollen nur die Daten aus Labor X (nicht Labor Y) ausgewertet werden etc. Ein Sinnbild: Abbildung 5.2: Zeilen filtern Merke: Die Funktion filter filtert Zeilen aus einem Dataframe. Schauen wir uns einige Beispiel an; zuerst die Daten laden nicht vergessen. Achtung: “Wohnen” die Daten in einem Paket, muss dieses Paket installiert sein, damit man auf die Daten zugreifen kann. data(profiles, package = &quot;okcupiddata&quot;) # Das Paket muss installiert sein df_frauen &lt;- filter(profiles, sex == &quot;f&quot;) # nur die Frauen df_alt &lt;- filter(profiles, age &gt; 70) # nur die alten df_alte_frauen &lt;- filter(profiles, age &gt; 70, sex == &quot;f&quot;) # nur die alten Frauen, d.h. UND-Verknüpfung df_nosmoke_nodrinks &lt;- filter(profiles, smokes == &quot;no&quot; | drinks == &quot;not at all&quot;) # liefert alle Personen, die Nicht-Raucher *oder* Nicht-Trinker sind Gar nicht so schwer, oder? Allgemeiner gesprochen werden diejenigen Zeilen gefiltert (also behalten bzw. zurückgeliefert), für die das Filterkriterium TRUE ist. Manche Befehle wie filter haben einen Allerweltsnamen; gut möglich, dass ein Befehl mit gleichem Namen in einem anderen (geladenen) Paket existiert. Das kann dann zu Verwirrungen führen - und kryptischen Fehlern. Im Zweifel den Namen des richtigen Pakets ergänzen, und zwar zum Beispiel so: dplyr::filter(...). 5.2.1.1 Aufgaben13 Richtig oder Falsch!? filter filtert Spalten. filter ist eine Funktion aus dem Paket dplyr. filter erwartet als ersten Parameter das Filterkriterium. filter lässt nur ein Filterkriterium zu. Möchte man aus dem Datensatz profiles (okcupiddata) die Frauen filtern, so ist folgende Syntax korrekt: `filter(profiles, sex == “f”)´. 5.2.1.2 Vertiefung: Fortgeschrittene Beispiele für filter Einige fortgeschrittene Beispiele für filter: Man kann alle Elemente (Zeilen) filtern, die zu einer Menge gehören und zwar mit diesem Operator: %in%: filter(profiles, body_type %in% c(&quot;a little extra&quot;, &quot;average&quot;)) Besonders Textdaten laden zu einigen Extra-Überlegungen ein; sagen wir, wir wollen alle Personen filtern, die Katzen bei den Haustieren erwähnen. Es soll reichen, wenn cat ein Teil des Textes ist; also likes dogs and likes cats wäre OK (soll gefiltert werden). Dazu nutzen wir ein Paket zur Bearbeitung von Strings (Textdaten): filter(profiles, str_detect(pets, &quot;cats&quot;)) Ein häufiger Fall ist, Zeilen ohne fehlende Werte (NAs) zu filtern. Das geht einfach: profiles_keine_nas &lt;- na.omit(profiles) Aber was ist, wenn wir nur bei bestimmten Spalten wegen fehlender Werte besorgt sind? Sagen wir bei income und bei sex: filter(profiles, !is.na(income) | !is.na(sex)) 5.2.2 Spalten wählen mit select Das Gegenstück zu filter ist select; dieser Befehl liefert die gewählten Spalten zurück. Das ist häufig praktisch, wenn der Datensatz sehr “breit” ist, also viele Spalten enthält. Dann kann es übersichtlicher sein, sich nur die relevanten auszuwählen. Das Sinnbild für diesen Befehl: Abbildung 5.3: Spalten auswählen Merke: Die Funktion select wählt Spalten aus einem Dataframe aus. Laden wir als ersten einen Datensatz. stats_test &lt;- read.csv(&quot;data/test_inf_short.csv&quot;) Dieser Datensatz beinhaltet Daten zu einer Statistikklausur. Beachten Sie, dass diese Syntax davon ausgeht, dass sich die Daten in einem Unterordner mit dem Namen data befinden, welcher sich im Arbeitsverzeichnis befindet14. select(stats_test, score) # Spalte `score` auswählen select(stats_test, score, study_time) # Splaten `score` und `study_time` auswählen select(stats_test, score:study_time) # dito select(stats_test, 5:6) Spalten 5 bis 6 auswählen Tatsächlich ist der Befehl select sehr flexibel; es gibt viele Möglichkeiten, Spalten auszuwählen. Im dplyr-Cheatsheet findet sich ein guter Überblick dazu. 5.2.2.1 Aufgaben15 Richtig oder Falsch!? select wählt Zeilen aus. select ist eine Funktion aus dem Paket knitr. Möchte man zwei Spalten auswählen, so ist folgende Syntax prinzipiell korrekt: select(df, spalte1, spalte2). Möchte man Spalten 1 bis 10 auswählen, so ist folgende Syntax prinzipiell korrekt: `select(df, spalte1:spalte10) Mit select können Spalten nur bei ihrem Namen, aber nicht bei ihrer Nummer aufgerufen werden. 5.2.3 Zeilen sortieren mit arrange Man kann zwei Arten des Umgangs mit R unterscheiden: Zum einen der “interaktive Gebrauch” und zum anderen “richtiges Programmieren”. Im interaktiven Gebrauch geht es uns darum, die Fragen zum aktuell vorliegenden Datensatz (schnell) zu beantworten. Es geht nicht darum, eine allgemeine Lösung zu entwickeln, die wir in die Welt verschicken können und die dort ein bestimmtes Problem löst, ohne dass der Entwickler (wir) dabei Hilfestellung geben muss. “Richtige” Software, wie ein R-Paket oder Microsoft Powerpoint, muss diese Erwartung erfüllen; “richtiges Programmieren” ist dazu vonnöten. Natürlich sind in diesem Fall die Ansprüche an die Syntax (der “Code”, hört sich cooler an) viel höher. In dem Fall muss man alle Eventualitäten voraussehen und sicherstellen, dass das Programm auch beim merkwürdigsten Nutzer brav seinen Dienst tut. Wir haben hier, beim interaktiven Gebrauch, niedrigere Ansprüche bzw. andere Ziele. Beim interaktiven Gebrauch von R (oder beliebigen Analyseprogrammen) ist das Sortieren von Zeilen eine recht häufige Tätigkeit. Typisches Beispiel wäre der Lehrer, der eine Tabelle mit Noten hat und wissen will, welche Schüler die schlechtesten oder die besten sind in einem bestimmten Fach. Oder bei der Prüfung der Umsätze nach Filialen möchten wir die umsatzstärksten sowie -schwächsten Niederlassungen kennen. Ein R-Befehl hierzu ist arrange; einige Beispiele zeigen die Funktionsweise am besten: arrange(stats_test, score) # liefert die *schlechtesten* Noten zuerst zurück arrange(stats_test, -score) # liefert die *besten* Noten zuerst zurück arrange(stats_test, interest, score) #&gt; X V_1 study_time self_eval interest score #&gt; 1 234 23.01.2017 18:13:15 3 1 1 17 #&gt; 2 4 06.01.2017 09:58:05 2 3 2 18 #&gt; 3 131 19.01.2017 18:03:45 2 3 4 18 #&gt; 4 142 19.01.2017 19:02:12 3 4 1 18 #&gt; 5 35 12.01.2017 19:04:43 1 2 3 19 #&gt; 6 71 15.01.2017 15:03:29 3 3 3 20 #&gt; X V_1 study_time self_eval interest score #&gt; 1 3 05.01.2017 23:33:47 5 10 6 40 #&gt; 2 7 06.01.2017 14:25:49 NA NA NA 40 #&gt; 3 29 12.01.2017 09:48:16 4 10 3 40 #&gt; 4 41 13.01.2017 12:07:29 4 10 3 40 #&gt; 5 58 14.01.2017 15:43:01 3 8 2 40 #&gt; 6 83 16.01.2017 10:16:52 NA NA NA 40 #&gt; X V_1 study_time self_eval interest score #&gt; 1 234 23.01.2017 18:13:15 3 1 1 17 #&gt; 2 142 19.01.2017 19:02:12 3 4 1 18 #&gt; 3 221 23.01.2017 11:40:30 1 1 1 23 #&gt; 4 230 23.01.2017 16:27:49 1 1 1 23 #&gt; 5 92 17.01.2017 17:18:55 1 1 1 24 #&gt; 6 107 18.01.2017 16:01:36 3 2 1 24 Einige Anmerkungen. Die generelle Syntax lautet arrange(df, Spalte1, ...), wobei df den Dataframe bezeichnet und Spalte1 die erste zu sortierende Spalte; die Punkte ... geben an, dass man weitere Parameter übergeben kann. Man kann sowohl numerische Spalten als auch Textspalten sortieren. Am wichtigsten ist hier, dass man weitere Spalten übergeben kann. Dazu gleich mehr. Standardmäßig sortiert arrange aufsteigend (weil kleine Zahlen im Zahlenstrahl vor den großen Zahlen kommen). Möchte man diese Reihenfolge umdrehen (große Werte zuert, d.h. absteigend), so kann man ein Minuszeichen vor den Namen der Spalte setzen. Gibt man zwei oder mehr Spalten an, so werden pro Wert von Spalte1 die Werte von Spalte2 sortiert etc; man betrachte den Output des Beispiels oben dazu. %`? Diese sogenannte \"Pfeife\" lässt sich mit \"und dann\" ins Deutsce übersetzen. Also: --> Merke: Die Funktion arrange sortiert die Zeilen eines Datafames. Ein Sinnbild zur Verdeutlichung: Abbildung 5.4: Spalten sortieren Ein ähnliches Ergebnis erhält mit man top_n(), welches die n größten Ränge widergibt: top_n(stats_test, 3) #&gt; X V_1 study_time self_eval interest score #&gt; 1 3 05.01.2017 23:33:47 5 10 6 40 #&gt; 2 7 06.01.2017 14:25:49 NA NA NA 40 #&gt; 3 29 12.01.2017 09:48:16 4 10 3 40 #&gt; 4 41 13.01.2017 12:07:29 4 10 3 40 #&gt; 5 58 14.01.2017 15:43:01 3 8 2 40 #&gt; 6 83 16.01.2017 10:16:52 NA NA NA 40 #&gt; 7 116 18.01.2017 23:07:32 4 8 5 40 #&gt; 8 119 19.01.2017 09:05:01 NA NA NA 40 #&gt; 9 132 19.01.2017 18:22:32 NA NA NA 40 #&gt; 10 175 20.01.2017 23:03:36 5 10 5 40 #&gt; 11 179 21.01.2017 07:40:05 5 9 1 40 #&gt; 12 185 21.01.2017 15:01:26 4 10 5 40 #&gt; 13 196 22.01.2017 13:38:56 4 10 5 40 #&gt; 14 197 22.01.2017 14:55:17 4 10 5 40 #&gt; 15 248 24.01.2017 16:29:45 2 10 2 40 #&gt; 16 249 24.01.2017 17:19:54 NA NA NA 40 #&gt; 17 257 25.01.2017 10:44:34 2 9 3 40 #&gt; 18 306 27.01.2017 11:29:48 4 9 3 40 top_n(stats_test, 3, interest) #&gt; X V_1 study_time self_eval interest score #&gt; 1 3 05.01.2017 23:33:47 5 10 6 40 #&gt; 2 5 06.01.2017 14:13:08 4 8 6 34 #&gt; 3 43 13.01.2017 14:14:16 4 8 6 36 #&gt; 4 65 15.01.2017 12:41:27 3 6 6 22 #&gt; 5 110 18.01.2017 18:53:02 5 8 6 37 #&gt; 6 136 19.01.2017 18:22:57 3 1 6 39 #&gt; 7 172 20.01.2017 20:42:46 5 10 6 34 #&gt; 8 214 22.01.2017 21:57:36 2 6 6 31 #&gt; 9 301 27.01.2017 08:17:59 4 8 6 33 Gibt man keine Spalte an, so bezieht sich top_n auf die letzte Spalte im Datensatz. Da sich hier mehrere Personen den größten Rang (Wert 40) teilen, bekommen wir nicht 3 Zeilen zurückgeliefert, sondern entsprechend mehr. 5.2.3.1 Aufgaben16 Richtig oder Falsch!? arrange arrangiert Spalten. arrange sortiert im Standard absteigend. arrange lässt nur ein Sortierkriterium zu. arrange kann numerische Werte, aber nicht Zeichenketten sortieren. top_n(5) liefert die fünf kleinsten Ränge. 5.2.4 Datensatz gruppieren mit group_by Einen Datensatz zu gruppieren ist eine häufige Angelegenheit: Was ist der mittlere Umsatz in Region X im Vergleich zu Region Y? Ist die Reaktionszeit in der Experimentalgruppe kleiner als in der Kontrollgruppe? Können Männer schneller ausparken als Frauen? Man sieht, dass das Gruppieren v.a. in Verbindung mit Mittelwerten oder anderen Zusammenfassungen sinnvol ist; dazu im nächsten Abschnitt mehr. Gruppieren meint, einen Datensatz anhand einer diskreten Variablen (z.B. Geschlecht) so aufzuteilen, dass Teil-Datensätze entstehen - pro Gruppe ein Teil-Datensatz (z.B. Mann vs. Frau). Abbildung 5.5: Datensätze nach Subgruppen aufteilen In der Abbildung wurde der Datensatz anhand der Spalte Fach in mehrere Gruppen geteilt. Wir könnten uns als nächstes z.B. Mittelwerte pro Fach - d.h. pro Gruppe (pro Ausprägung von Fach) - ausgeben lassen; in diesem Fall vier Gruppen (Fach A bis D). test_gruppiert &lt;- group_by(stats_test, interest) test_gruppiert #&gt; Source: local data frame [306 x 6] #&gt; Groups: interest [7] #&gt; #&gt; X V_1 study_time self_eval interest score #&gt; &lt;int&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 05.01.2017 13:57:01 5 8 5 29 #&gt; 2 2 05.01.2017 21:07:56 3 7 3 29 #&gt; 3 3 05.01.2017 23:33:47 5 10 6 40 #&gt; 4 4 06.01.2017 09:58:05 2 3 2 18 #&gt; 5 5 06.01.2017 14:13:08 4 8 6 34 #&gt; 6 6 06.01.2017 14:21:18 NA NA NA 39 #&gt; 7 7 06.01.2017 14:25:49 NA NA NA 40 #&gt; 8 8 06.01.2017 17:24:53 2 5 3 24 #&gt; 9 9 07.01.2017 10:11:17 2 3 5 25 #&gt; 10 10 07.01.2017 18:10:05 4 5 5 33 #&gt; # ... with 296 more rows Schaut man sich nun den Datensatz an, sieht man erstmal wenig Effekt der Gruppierung. R teilt uns lediglich mit Groups: interest [7], dass es 7 Gruppen gibt, aber es gibt keine extra Spalte oder sonstige Anzeichen der Gruppierung. Aber keine Sorge, wenn wir gleich einen Mittelwert ausrechnen, bekommen wir den Mittelwert pro Gruppe! Ein paar Hinweise: Source: local data frame [306 x 6] will sagen, dass die Ausgabe sich auf einen tibble bezieht17, also eine bestimmte Art von Dataframe. Groups: interest [7] zeigt, dass der Tibble in 7 Gruppen - entsprechend der Werte von interest aufgeteilt ist. group_by an sich ist nicht wirklich nützlich. Nützlich wird es erst, wenn man weitere Funktionen auf den gruppierten Datensatz anwendet - z.B. Mittelwerte ausrechnet (z.B mit summarise, s. unten). Die nachfolgenden Funktionen (wenn sie aus dplyr kommen), berücksichtigen nämlich die Gruppierung. So kann man einfach Mittelwerte pro Gruppe ausrechnen. dplyr kombiniert dann die Zusammenfassungen (z.B. Mittelwerte) der einzelnen Gruppen in einen Dataframe und gibt diesen dann aus. Die Idee des “Gruppieren - Zusammenfassen - Kombinieren” ist flexibel; man kann sie häufig brauchen. Es lohnt sich, diese Idee zu lernen (vgl. Abb. 5.6). Abbildung 5.6: Schematische Darstellung des ‘Gruppieren - Zusammenfassen - Kombinieren’ 5.2.4.1 Aufgaben18 Richtig oder Falsch!? Mit group_by gruppiert man einen Datensatz. group_by lässt nur ein Gruppierungskriterium zu. Die Gruppierung durch group_by wird nur von Funktionen aus dplyr erkannt. group_by ist sinnvoll mit summarise zu kombinieren. Merke: Mit group_by teilt man einen Datensatz in Gruppen ein, entsprechend der Werte einer mehrerer Spalten. 5.2.5 Eine Spalte zusammenfassen mit summarise Vielleicht die wichtigste oder häufigte Tätigkeit in der Analyse von Daten ist es, eine Spalte zu einem Wert zusammenzufassen; summarise leistet dies. Anders gesagt: Einen Mittelwert berechnen, den größten (kleinsten) Wert heraussuchen, die Korrelation berechnen oder eine beliebige andere Statistik ausgeben lassen. Die Gemeinsamkeit dieser Operaitonen ist, dass sie eine Spalte zu einem Wert zusammenfassen, “aus Spalte mach Zahl”, sozusagen. Daher ist der Name des Befehls summarise ganz passend. Genauer gesagt fasst dieser Befehl eine Spalte zu einer Zahl zusammen anhand einer Funktion wie mean oder max. Hierbei ist jede Funktion erlaubt, die eine Spalte als Input verlangt und eine Zahl zurückgibt; andere Funktionen sind bei summarise nicht erlaubt. Abbildung 5.7: Spalten zu einer Zahl zusammenfassen summarise(stats_test, mean(score)) #&gt; mean(score) #&gt; 1 31.1 Man könnte diesen Befehl so ins Deutsche übersetzen: Fasse aus Tabelle stats_test die Spalte score anhand des Mittelwerts zusammen. Nicht vergessen, wenn die Spalte score fehlende Werte hat, wird der Befehl mean standardmäßig dies mit NA quittieren. Ergänzt man den Parameter nr.rm = TRUE, so ignoriert R fehlende Werte und der Befehl mean liefert ein Ergebnis zurück. Jetzt können wir auch die Gruppierung nutzen: test_gruppiert &lt;- group_by(stats_test, interest) summarise(test_gruppiert, mean(score, na.rm = TRUE)) #&gt; # A tibble: 7 × 2 #&gt; interest `mean(score, na.rm = TRUE)` #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 28.3 #&gt; 2 2 29.7 #&gt; 3 3 30.8 #&gt; 4 4 29.9 #&gt; 5 5 32.5 #&gt; 6 6 34.0 #&gt; 7 NA 33.1 Der Befehl summarise erkennt also, wenn eine (mit group_by) gruppierte Tabelle vorliegt. Jegliche Zusammenfassung, die wir anfordern, wird anhand der Gruppierungsinformation aufgeteilt werden. In dem Beispiel bekommen wir einen Mittelwert für jeden Wert von interest. Interessanterweise sehen wir, dass der Mittelwert tendenziell größer wird, je größer interest wird. Alle diese dplyr-Befehle geben einen Dataframe zurück, was praktisch ist für weitere Verarbeitung. In diesem Fall heißen die Spalten interst und mean(score). Zweiter Name ist nicht so schön, daher ändern wir den wie folgt: Jetzt können wir auch die Gruppierung nutzen: test_gruppiert &lt;- group_by(stats_test, interest) summarise(test_gruppiert, mw_pro_gruppe = mean(score, na.rm = TRUE)) #&gt; # A tibble: 7 × 2 #&gt; interest mw_pro_gruppe #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 28.3 #&gt; 2 2 29.7 #&gt; 3 3 30.8 #&gt; 4 4 29.9 #&gt; 5 5 32.5 #&gt; 6 6 34.0 #&gt; 7 NA 33.1 Nun heißt die zweite Spalte mw_pro_Gruppe. na.rm = TRUE veranlasst, bei fehlenden Werten trotzdem einen Mittelwert zurückzuliefern (die Zeilen mit fehlenden Werten werden in dem Fall ignoriert). Grundsätzlich ist die Philosophie der dplyr-Befehle: “Mach nur eine Sache, aber die dafür gut”. Entsprechend kann summarise nur Spalten zusammenfassen, aber keine Zeilen. Merke: Mit summarise kann man eine Spalte eines Dataframes zu einem Wert zusammenfassen. 5.2.5.1 Deskriptive Statistik mit summarise Die deskriptive Statistik hat zwei Haupt-Bereiche: Lagemaße und Streuungsmaße. Lagemaße geben den “typischen”, “mittleren” oder “repräsentativen” Vertreter der Verteilung an. Bei den Lagemaßen denkt man sofort an das arithmetische Mittel (synonym: Mittelwert; häufig als \\(\\bar{X}\\) abgekürzt; mean). Ein Nachteil von Mittelwerten ist, dass sie nicht robust gegenüber Extremwerte sind: Schon ein vergleichsweise großer Einzelwert kann den Mittelwert deutlich verändern und damit die Repräsentativität des Mittelwerts für die Gesamtmenge der Daten in Frage stellen. Eine robuste Variante ist der Median (Md; median). Ist die Anzahl der (unterschiedlichen) Ausprägungen nicht zu groß im Verhältnis zur Fallzahl, so ist der Modus eine sinnvolle Statistik; er gibt die häufigste Ausprägung an19. Streuungsmaße geben die Unterschiedlichkeit in den Daten wieder; mit anderen Worten: sind die Daten sich ähnlich oder unterscheiden sich die Werte deutlich? Zentrale Statistiken sind der mittlere Absolutabstand (MAA; MAD),20 die Standardabweichung (sd; sd), die Varianz (Var; var) und der Interquartilsabstand (IQR; IQR). Da nur der IQR nicht auf dem Mittelwert basiert, ist er am robustesten. Beliebige Quantile bekommt man mit dem R-Befehl quantile. Der Befehl summarise eignet sich, um deskriptive Statistiken auszurechnen. summarise(stats_test, mean(score)) #&gt; mean(score) #&gt; 1 31.1 summarise(stats_test, sd(score)) #&gt; sd(score) #&gt; 1 5.74 Natürlich könnte man auch einfacher schreiben: mean(stats_test$score) #&gt; [1] 31.1 median(stats_test$score) #&gt; [1] 31 summarise liefert aber im Unterschied zu mean etc. immer einen Dataframe zurück. Da der Dataframe die typische Datenstruktur ist, ist es häufig praktisch, wenn man einen Dataframe zurückbekommt, mit dem man weiterarbeiten kann. Außerdem lassen mean etc. keine Gruppierungsoperationen zu; über group_by kann man dies aber bei dplyr erreichen. 5.2.5.2 Aufgaben21 Richtig oder Falsch!? Möchte man aus der Tabelle stats_test den Mittelwert für die Spalte score berechnen, so ist folgende Syntax korrekt: summarise(stats_test, mean(score)). summarise liefert eine Tabelle, genauer: einen Tibble, zurück. Die Tabelle, die diese Funktion zurückliefert: summarise(stats_test, mean(score)), hat eine Spalte mit dem Namen mean(score). summarise lässt zu, dass die zu berechnende Spalte einen Namen vom Nutzer zugewiesen bekommt. summarise darf nur verwendet werden, wenn eine Spalte zu einem Wert zusammengefasst werden soll. (Fortgeschritten) Bauen Sie einen eigenen Weg, um den mittleren Absolutabstand auszurechnen! Gehen Sie der Einfachheit halber (zuerst) von einem Vektor mit den Werten (1,2,3) aus! Lösung: x &lt;- c(1, 2, 3) x_mw &lt;- mean(x) x_delta &lt;- x - x_mw x_delta &lt;- abs(x_delta) mad &lt;- mean(x_delta) mad #&gt; [1] 0.667 ``` 5.2.6 Zeilen zählen mit n und count Ebenfalls nützlich ist es, Zeilen zu zählen. Im Gegensatz zum Standardbefehl22 nrow versteht der dyplr-Befehl n auch Gruppierungen. n darf nur innerhalb von summarise oder ähnlichen dplyr-Befehlen verwendet werden. summarise(stats_test, n()) #&gt; n() #&gt; 1 306 summarise(test_gruppiert, n()) #&gt; # A tibble: 7 × 2 #&gt; interest `n()` #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 30 #&gt; 2 2 47 #&gt; 3 3 66 #&gt; 4 4 41 #&gt; 5 5 45 #&gt; 6 6 9 #&gt; 7 NA 68 nrow(stats_test) #&gt; [1] 306 Außerhalb von gruppierten Datensätzen ist nrow meist praktischer. Praktischer ist der Befehl count, der nichts anderes ist als die Hintereinanderschaltung von group_by und n. Mit count zählen wir die Häufigkeiten nach Gruppen; Gruppen sind hier zumeist die Werte einer auszuzählenden Variablen (oder mehrerer auszuzählender Variablen). Das macht count zu einem wichtigen Helfer bei der Analyse von Häufigkeitsdaten. dplyr::count(stats_test, interest) #&gt; # A tibble: 7 × 2 #&gt; interest n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 30 #&gt; 2 2 47 #&gt; 3 3 66 #&gt; 4 4 41 #&gt; 5 5 45 #&gt; 6 6 9 #&gt; 7 NA 68 dplyr::count(stats_test, study_time) #&gt; # A tibble: 6 × 2 #&gt; study_time n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 31 #&gt; 2 2 49 #&gt; 3 3 85 #&gt; 4 4 56 #&gt; 5 5 17 #&gt; 6 NA 68 dplyr::count(stats_test, interest, study_time) #&gt; Source: local data frame [29 x 3] #&gt; Groups: interest [?] #&gt; #&gt; interest study_time n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 12 #&gt; 2 1 2 7 #&gt; 3 1 3 8 #&gt; 4 1 4 2 #&gt; 5 1 5 1 #&gt; 6 2 1 9 #&gt; 7 2 2 15 #&gt; 8 2 3 16 #&gt; 9 2 4 6 #&gt; 10 2 5 1 #&gt; # ... with 19 more rows Allgemeiner formuliert lautet die Syntax: count(df, Spalte1, ...), wobei df der Dataframe ist und Spalte1 die erste (es können mehrere sein) auszuzählende Spalte. Gibt man z.B. zwei Spalten an, so wird pro Wert der 1. Spalte die Häufigkeiten der 2. Spalte ausgegeben. Merke: n und count zählen die Anzahl der Zeilen, d.h. die Anzahl der Fälle. 5.2.6.1 Aufgaben23 Richtig oder Falsch!? Mit count kann man Zeilen zählen. count ist ähnlich (oder identisch) zu einer Kombination von group_by und n(). Mit count kann man nur nur eine Gruppe beim Zählen berücksichtigen. count darf nicht bei nominalskalierten Variablen verwendet werden. Bauen Sie sich einen Weg, um den Modus mithilfe von count und arrange zu bekommen! stats_count &lt;- count(stats_test, score) stats_count_sortiert &lt;- arrange(stats_count, -n) head(stats_count_sortiert,1) #&gt; # A tibble: 1 × 2 #&gt; score n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 34 22 Ah! Der Score 34 ist der häufigste! 5.3 Die Pfeife Die zweite Idee kann man salopp als “Durchpfeifen” oder die “Idee der Pfeife bezeichnen; ikonographisch mit einem Pfeifen ähnlichen Symbol dargestellt %&gt;%. Der Begriff”Durchpfeifen&quot; ist frei vom Englischen “to pipe” übernommen. Das berühmte Bild von René Magritte stand dabei Pate. Abbildung 5.8: La trahison des images [Ceci n’est pas une pipe], René Magritte, 1929, © C. Herscovici, Brussels / Artists Rights Society (ARS), New York, http://collections.lacma.org/node/239578 Hierbei ist gemeint, einen Datensatz sozusagen auf ein Fließband zu legen und an jedem Arbeitsplatz einen Arbeitsschritt auszuführen. Der springende Punkt ist, dass ein Dataframe als “Rohstoff” eingegeben wird und jeder Arbeitsschritt seinerseits wieder einen Datafram ausgiebt. Damit kann man sehr schön, einen “Flow” an Verarbeitung erreichen, außerdem spart man sich Tipparbeit und die Syntax wird lesbarer. Damit das Durchpfeifen funktioniert, benötigt man Befehle, die als Eingabe einen Dataframe erwarten und wieder einen Dataframe zurückliefern. Das Schaubild verdeutlich beispielhaft eine Abfolge des Durchpfeifens. Abbildung 5.9: Das ‘Durchpeifen’ Die sog. “Pfeife” (pipe: %&gt;%) in Anspielung an das berühmte Bild von René Magritte, verkettet Befehle hintereinander. Das ist praktisch, da es die Syntax vereinfacht. Vergleichen Sie mal diese Syntax filter(summarise(group_by(filter(stats_test, !is.na(score)), interest), mw = mean(score)), mw &gt; 30) mit dieser stats_test %&gt;% filter(!is.na(score)) %&gt;% group_by(interest) %&gt;% summarise(mw = mean(score)) %&gt;% filter(mw &gt; 30) #&gt; # A tibble: 4 × 2 #&gt; interest mw #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 3 30.8 #&gt; 2 5 32.5 #&gt; 3 6 34.0 #&gt; 4 NA 33.1 Es ist hilfreich, diese “Pfeifen-Syntax” in deutschen Pseudo-Code zu übersetzen. Nimm die Tabelle “stats_test” UND DANN filtere alle nicht-fehlenden Werte UND DANN gruppiere die verbleibenden Werte nach “interest” UND DANN bilde den Mittelwert (pro Gruppe) für “score” UND DANN liefere nur die Werte größer als 30 zurück. Die zweite Syntax, in “Pfeifenform” ist viel einfacher zu verstehen als die erste! Die erste Syntax ist verschachelt, man muss sie von innen nach außen lesen. Das ist kompliziert. Die Pfeife in der 2. Syntax macht es viel einfacher, die Snytax zu verstehen, da die Befehle “hintereinander” gestellt (sequenziell organisiert) sind. Die Pfeife zerlegt die “russische Puppe”, also ineinander verschachelteten Code, in sequenzielle Schritte und zwar in der richtigen Reihenfolge (entsprechend der Abarbeitung). Wir müssen den Code nicht mehr von innen nach außen lesen (wie das bei einer mathematischen Formel der Fall ist), sondern können wie bei einem Kochrezept “erstens …, zweitens .., drittens …” lesen. Die Pfeife macht die Syntax einfacher. Natürlich hätten wir die verschachtelte Syntax in viele einzelne Befehle zerlegen können und jeweils eine Zwischenergebnis speichern mit dem Zuweisungspfeil &lt;- und das Zwischenergebnis dann explizit an den nächsten Befehl weitergeben. Eigentlich macht die Pfeife genau das - nur mit weniger Tipparbeit. Und auch einfacher zu lesen. Flow! Wenn Sie Befehle verketten mit der Pfeife, sind nur Befehle erlaubt, die einen Datensatz als Eingabe verlangen und einen Datensatz ausgeben. Das ist bei den hier vorgestellten Funktionen der Fall. Viele andere Funktionen erfüllen dieses Kriterium aber nicht; in dem Fall liefert dplyr eine Fehlermeldung. 5.3.1 Spalten berechnen mit mutate Wenn man die Pfeife benutzt, ist der Befehl mutate ganz praktisch: Er berechnet eine Spalte. Normalerweise kann man einfach eine Spalte berechnen mit dem Zuweisungsoperator: Zum Beispiel so: df$neue_spalte &lt;- df$spalte1 + df$spalte2 Innerhalb einer Pfeifen-Syntax geht das aber nicht (so gut). Da ist man mit der Funtion mutate besser beraten; mutate leistest just dasselbe wie die Pseudo-Syntax oben: df %&gt;% mutate(neue_spalte = spalte1 + spalte2) In Worten: Nimm die Tabelle “df” UND DANN bilde eine neue Spalte mit dem Namen neue_spalte, die sich berechnet als Summe von spalte1 und spalte2. Allerdings berücksichtigt mutate auch Gruppierungen. Der Hauptvorteil ist die bessere Lesbarkeit durch Auflösen der Verschachtelungen. Ein konkretes Beispiel: stats_test %&gt;% mutate(bestanden = score &gt; 25) %&gt;% head() #&gt; X V_1 study_time self_eval interest score bestanden #&gt; 1 1 05.01.2017 13:57:01 5 8 5 29 TRUE #&gt; 2 2 05.01.2017 21:07:56 3 7 3 29 TRUE #&gt; 3 3 05.01.2017 23:33:47 5 10 6 40 TRUE #&gt; 4 4 06.01.2017 09:58:05 2 3 2 18 FALSE #&gt; 5 5 06.01.2017 14:13:08 4 8 6 34 TRUE #&gt; 6 6 06.01.2017 14:21:18 NA NA NA 39 TRUE Diese Syntax erzeugt eine neue Spalte innerhalb von stats_test; diese Spalte prüft pro Persion, ob score &gt; 25 ist. Falls ja (TRUE), dann ist bestanden TRUE, ansonsten ist bestanden FALSE (Pech). head zeigt die ersten 6 Zeilen des resultierenden Dataframes an. Ein Sinnbild für mutate: 5.3.2 Aufgaben Entschlüsseln Sie dieses Ungetüm! Übersetzen Sie diese Syntax auf Deutsch: library(nycflights13) data(flights) verspaetung &lt;- filter( summarise( group_by(filter(flights, !is.na(dep_delay), month)), delay = mean(dep_delay), n = n()), n &gt; 10) Entschlüsseln Sie jetzt diese Syntax bzw. übersetzen Sie sie ins Deutsche: verspaetung &lt;- flights %&gt;% filter(!is.na(dep_delay)) %&gt;% group_by(month) %&gt;% summarise(delay = mean(dep_delay), n = n()) %&gt;% filter(n &gt; 10) (schwierig) Die Pfeife bei arr_delay Übersetzen Sie die folgende Pseudo-Syntax ins ERRRische! Nehme den Datensatz flights UND DANN… Wähle daraus die Spalte arr_delay UND DANN… Berechne den Mittelwert der Spalte UND DANN… ziehe vom Mittelwert die Spalte ab UND DANN… quadriere die einzelnen Differenzen UND DANN… bilde davon den Mittelwert. Lösung: flights %&gt;% select(arr_delay) %&gt;% mutate(arr_delay_delta = arr_delay - mean(flights$arr_delay, na.rm = TRUE)) %&gt;% mutate(arr_delay_delta_quadrat = arr_delay_delta^2) %&gt;% summarise(arr_delay_var = mean(arr_delay_delta_quadrat, na.rm = TRUE)) %&gt;% summarise(sqrt(arr_delay_var)) #&gt; # A tibble: 1 × 1 #&gt; `sqrt(arr_delay_var)` #&gt; &lt;dbl&gt; #&gt; 1 44.6 Berechnen Sie die sd von arr_delay in flights! Vergleichen Sie sie mit dem Ergebnis der vorherigen Aufgabe!24 Was hat die Pfeifen-Syntax oben berechnet?25 5.4 Befehlsübersicht Paket::Funktion Beschreibung dplyr::arrange Sortiert Spalten dplyr::filter Filtert Zeilen dplyr::select Wählt Spalten dplyr::group_by gruppiert einen Dataframe dplyr::n zählt Zeilen dplyr::count zählt Zeilen nach Untergruppen %&gt;% (dplyr) verkettet Befehle dplyr::mutate erzeugt/berechnet Spalten 5.5 Verweise Die offizielle Dokumentation von dplyr findet sich hier26. Eine schöne Demonstration der Mächtigkeit von dplyr findet sich hier27. Die GUI “exploratory” ist ein “klickbare” Umsetzung von dplyr, mächtig, modern und sieht cool aus: https://exploratory.io. R for Data Science bietet umfangreiche Unterstützung zu diesem Thema (Wickham and Grolemund 2016). Literaturverzeichnis "],
["praxisprobleme-der-datenaufbereitung.html", "Kapitel 6 Praxisprobleme der Datenaufbereitung 6.1 Datenaufbereitung 6.2 Deskriptive Statistiken berechnen 6.3 Befehlsübersicht", " Kapitel 6 Praxisprobleme der Datenaufbereitung Lernziele: Typische Probleme der Datenaufbereitung kennen. Typische Probleme der Datenaufbereitung bearbeiten können. Laden wir zuerst die benögtigten Pakete; v.a. ist das dplyr and friends. Das geht mit dem Paket tidyverse. library(tidyverse) library(corrr) library(gridExtra) library(car) Stellen wir einige typische Probleme des Datenjudo (genauer: der Datenaufbereitung) zusammen. Probleme heißt hier nicht, dass es etwas Schlimmes passiert ist, sondern es ist gemeint, wir schauen uns ein paar typische Aufgabenstellungen an, die im Rahmen der Datenaufbereitung häufig anfallen. 6.1 Datenaufbereitung 6.1.1 Auf fehlende Werte prüfen Das geht recht einfach mit summarise(mein_dataframe). Der Befehl liefert für jede Spalte des Dataframe mein_dataframe die Anzahl der fehlenden Werte zurück. wo_men &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/wo_men.csv&quot;) glimpse(wo_men) #&gt; Observations: 101 #&gt; Variables: 4 #&gt; $ time &lt;fctr&gt; 04.10.2016 17:58:51, 04.10.2016 17:58:59, 04.10.201... #&gt; $ sex &lt;fctr&gt; woman, woman, woman, woman, man, woman, woman, woma... #&gt; $ height &lt;dbl&gt; 160, 171, 174, 176, 195, 157, 160, 178, 168, 171, 16... #&gt; $ shoe_size &lt;dbl&gt; 40, 39, 39, 40, 46, 37, 38, 39, 38, 41, 39, 44, 38, ... 6.1.2 Fälle mit fehlenden Werte löschen Weist eine Variable (Spalte) “wenig” fehlende Werte auf, so kann es schlau sein, nichts zu tun. Eine andere Möglichkeit besteht darin, alle entsprechenden Zeilen zu löschen. Man sollte aber schauen, wie viele Zeilen dadurch verloren gehen. nrow(wo_men) #&gt; [1] 101 wo_men %&gt;% na.omit %&gt;% nrow #&gt; [1] 100 Bei mit der Pfeife verketteten Befehlen darf man für Funktionen die runden Klammern weglassen, wenn man keinen Parameter schreibt. Also nrow ist erlaubt bei dplyr, wo es eigentlich nrow() heißen müsste. Sie dürfen die Klammern natürlich schreiben, aber sie müssen nicht. Hier verlieren wir nur 1 Zeile, das verschmerzen wir. Welche eigentlich? wo_men %&gt;% rownames_to_column %&gt;% # Zeilennummer werden eine eigene Spalte filter(!complete.cases(.)) # Nur die nicht-kompletten Fälle filtern #&gt; rowname time sex height shoe_size #&gt; 1 86 11.10.2016 12:44:06 &lt;NA&gt; NA NA Man beachte, dass der Punkt . für den Datensatz steht, wie er vom letzten Schritt weitergegeben wurde. Innerhalb einer dplyr-Befehls-Kette können wir den Datensatz, wie er im letzten Schritt beschaffen war, stets mit . ansprechen; ganz praktisch, weil schnell zu tippen. Natürlich könnten wir diesen Datensatz jetzt als neues Objekt speichern und damit weiter arbeiten. Das Ausrufezeichen ! steht für logisches “Nicht”. In Pseudo-Syntax liest es sich so: Nehme den Datensatz wo_men UND DANN… Mache aus den Zeilennamen (hier identisch zu Zeilennummer) eine eigene Spalte UND DANN… filtere die nicht-kompletten Fälle 6.1.3 Fehlende Werte ggf. ersetzen Ist die Anzahl der fehlenden Werte zu groß, als dass wir es verkraften könnten, die Zeilen zu löschen, so können wir die fehlenden Werte ersetzen. Allein, das ist ein weites Feld und übersteigt den Anspruch dieses Kurses28. Eine einfache, aber nicht die beste Möglichkeit, besteht darin, die fehlenden Werte durch einen repräsentativen Wert, z.B. den Mittelwert der Spalte, zu ersetzen. wo_men$height &lt;- replace(wo_men$height, is.na(wo_men$height), mean(wo_men$height, na.rm = TRUE)) replace29 ersetzt Werte aus dem Vektor wo_men$height alle Werte, für die is.na(wo_men$height) wahr ist. Diese Werte werden durch den Mittelwert der Spalte ersetzt30. 6.1.4 Nach Fehlern suchen Leicht schleichen sich Tippfehler oder andere Fehler ein. Man sollte darauf prüfen; so könnte man sich ein Histogramm ausgeben lassen pro Variable, um “ungewöhnliche” Werte gut zu erkennen. Meist geht das besser als durch das reine Betrachten von Zahlen. Gibt es wenig unterschiedliche Werte, so kann man sich auch die unterschiedlichen Werte ausgeben lassen. wo_men %&gt;% count(shoe_size) %&gt;% head # nur die ersten paar Zeilen #&gt; # A tibble: 6 × 2 #&gt; shoe_size n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 35.0 1 #&gt; 2 36.0 6 #&gt; 3 36.5 1 #&gt; 4 37.0 14 #&gt; 5 38.0 26 #&gt; 6 39.0 18 6.1.5 Ausreiser identifizieren Ähnlich zu Fehlern, steht man Ausreisern häufig skeptisch gegenüber. Allerdings kann man nicht pauschal sagen, das Extremwerte entfernt werden sollen: Vielleicht war jemand in der Stichprobe wirklich nur 1.20m groß? Hier gilt es, begründet und nachvollziehbar im Einzelfall zu entscheiden. Histogramme und Boxplots sind wieder ein geeignetes Mittel, um Ausreiser zu finden. 6.1.6 Hochkorrelierte Variablen finden Haben zwei Leute die gleiche Meinung, so ist einer von beiden überflüssig - wird behauptet. Ähnlich bei Variablen; sind zwei Variablen sehr hoch korreliert (&gt;.9, als grober (!) Richtwert), so bringt die zweite kaum Informationszuwachs zur ersten. Und kann ausgeschlossen werden. Oder man fasst ähnliche Variablen zusammen. wo_men %&gt;% select(height, shoe_size) %&gt;% correlate() -&gt; km # Korrelationsmatrix berechnen km #&gt; # A tibble: 2 × 3 #&gt; rowname height shoe_size #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 height NA 0.553 #&gt; 2 shoe_size 0.553 NA km %&gt;% shave() %&gt;% # Oberes Dreieck ist redundant, wird &quot;abrasiert&quot; rplot() # Korrelationsplot Die Funktion correlate stammt aus dem Paket corrr31, welches vorher installiert und geladen sein muss. Hier ist die Korrelation nicht zu groß, so dass wir keine weiteren Schritte unternehmen. 6.1.7 z-Standardisieren Für eine Reihe von Analysen ist es wichtig, die Skalierung der Variablen zur vereinheitlichen. Die z-Standardisierung ist ein übliches Vorgehen. Dabei wird der Mittelwert auf 0 transformiert und die SD auf 1; man spricht - im Falle von (hinreichend) normalverteilten Variablen - jetzt von der Standardnormalverteilung. Unterscheiden sich zwei Objekte A und B in einer standardnormalverteilten Variablen, so sagt dies nur etwas zur relativen Position von A zu B innerhalb ihrer Verteilung aus - im Gegensatz zu den Rohwerten. wo_men %&gt;% select_if(is.numeric) %&gt;% # Spalte nur auswählen, wenn numerisch scale() %&gt;% # z-standardisieren head() # nur die ersten paar Zeilen abdrucken #&gt; height shoe_size #&gt; [1,] -0.132 0.0405 #&gt; [2,] 0.146 -0.1395 #&gt; [3,] 0.221 -0.1395 #&gt; [4,] 0.272 0.0405 #&gt; [5,] 0.751 1.1204 #&gt; [6,] -0.208 -0.4994 Dieser Befehl liefert zwei z-standardisierte Spalten zurück. Kommoder ist es aber, alle Spalten des Datensatzes zurück zu bekommen, wobei zusätzlich die z-Werte aller numerischen Variablen hinzugekommen sind: wo_men %&gt;% mutate_if(is.numeric, funs(&quot;z&quot; = scale)) %&gt;% head #&gt; time sex height shoe_size height_z shoe_size_z #&gt; 1 04.10.2016 17:58:51 woman 160 40 -0.132 0.0405 #&gt; 2 04.10.2016 17:58:59 woman 171 39 0.146 -0.1395 #&gt; 3 04.10.2016 18:00:15 woman 174 39 0.221 -0.1395 #&gt; 4 04.10.2016 18:01:17 woman 176 40 0.272 0.0405 #&gt; 5 04.10.2016 18:01:22 man 195 46 0.751 1.1204 #&gt; 6 04.10.2016 18:01:53 woman 157 37 -0.208 -0.4994 Der Befehl mutate berechnet eine neue Spalte; mutate_if tut dies, wenn die Spalte numerisch ist. Die neue Spalte wird berechnet als z-Transformierung der alten Spalte; zum Spaltenname wird ein “_z&quot; hinzugefügt. Natürlich hätten wir auch mit select “händisch” die relevanten Spalten auswählen können. 6.1.8 Quasi-Konstante finden Hat eine Variable nur einen Wert, so verdient sie die Ehrenbezeichnung “Variable” nicht wirklich. Haben wir z.B. nur Männer im Datensatz, so kann das Geschlecht nicht für Unterschiede im Einkommen verantwortlich sein. Besser die Variable Geschlecht dann zu entfernen. Auch hier sind Histogramme oder Boxplots von Nutzen zur Identifiktion von (Quasi-)Konstanten. Alternativ kann man sich auch pro die Streuung (numerische Variablen) oder die Anzahl unterschiedlicher Werte (qualitative Variablen) ausgeben lassen. 6.1.9 Auf Normalverteilung prüfen Einige statistische Verfahren gehen von normalverteilten Variablen aus, daher macht es Sinn, Normalverteilung zu prüfen. Perfekte Normalverteilung ist genau so häufig wie perfekte Kreise in der Natur. Entsprechend werden Signifikanztests, die ja auf perfekte Normalverteilung prüfen, immer signifikant sein, sofern die Stichprobe groß genug ist. Daher ist meist zweckmäßiger, einen graphischen “Test” durchzuführen: ein Histogramm oder ein Dichte-Diagramm als “glatt geschmiergelte” Variante des Histogramms bieten sich an. Während die Körpergröße sehr deutlich normalverteilt ist, ist die Schuhgröße recht schief. Bei schiefen Verteilung können Transformationen Abhilfe schaffen. Hier erscheint die Schiefe noch erträglich, so dass wir keine weiteren Maßnahmen einleiten. 6.1.10 Werte umkodieren und “binnen” Umkodieren meint, die Werte zu ändern. Man sieht immer mal wieder, dass die Variable “gender” (Geschlecht) mit 1 und 2 kodiert ist. Verwechslungen sind da vorpragmmiert (“Ich bin mir echt ziemlich sicher, dass ich 1 für Männer kodiert habe, wahrscheinlich…”). Besser wäre es, die Ausprägungen male und female (“Mann”, “Frau”) o.ä. zu verwenden (vgl. Abb. 6.1). Abbildung 6.1: Sinnbild für Umkodieren Binnen meint, eine kontinuierliche Variablen in einige Bereiche (mindestens 2) zu zerschneiden. Ein Bild erläutert das am einfachsten (vgl. Abb. 6.2). Abbildung 6.2: Sinnbild zum ‘Binnen’ 6.1.10.1 Umkodieren und binnen mit car::recode Manchmal möchte man z.B. negativ gepolte Items umdrehen oder bei kategoriellen Variablen kryptische Bezeichnungen in sprechendere umwandeln. Hier gibt es eine Reihe praktischer Befehle, z.B. recode aus dem Paket car. Schauen wir uns ein paar Beispiele zum Umkodieren an. stats_test &lt;- read.csv(&quot;data/test_inf_short.csv&quot;) stats_test$score_fac &lt;- car::recode(stats_test$study_time, &quot;5 = &#39;sehr viel&#39;; 2:4 = &#39;mittel&#39;; 1 = &#39;wenig&#39;&quot;, as.factor.result = TRUE) stats_test$score_fac &lt;- car::recode(stats_test$study_time, &quot;5 = &#39;sehr viel&#39;; 2:4 = &#39;mittel&#39;; 1 = &#39;wenig&#39;&quot;, as.factor.result = FALSE) stats_test$study_time_2 &lt;- car::recode(stats_test$study_time, &quot;5 = &#39;sehr viel&#39;; 4 = &#39;wenig&#39;; else = &#39;Hilfe&#39;&quot;, as.factor.result = TRUE) head(stats_test$study_time_2) #&gt; [1] sehr viel Hilfe sehr viel Hilfe wenig Hilfe #&gt; Levels: Hilfe sehr viel wenig Der Befehle recode ist praktisch; mit : kann man “von bis” ansprechen (das ginge mit c() übrigens auch); else für “ansonsten” ist möglich und mit as.factor.result kann man entweder einen Faktor oder eine Text-Variable zurückgeliefert bekommen. Der ganze “Wechselterm” steht in Anführungsstrichen (&quot;). Einzelne Teile des Wechselterms sind mit einem Strichpunkt (;) voneinander getrennt. Das klassiche Umkodieren von Items aus Fragebögen kann man so anstellen; sagen wir interest soll umkodiert werden: stats_test$no_interest &lt;- car::recode(stats_test$interest, &quot;1 = 6; 2 = 5; 3 = 4; 4 = 3; 5 = 2; 6 = 1; else = NA&quot;) glimpse(stats_test$no_interest) #&gt; num [1:306] 2 4 1 5 1 NA NA 4 2 2 ... Bei dem Wechselterm muss man aufpassen, nichts zu verwechseln; die Zahlen sehen alle ähnlich aus… Testen kann man den Erfolg des Umpolens mit dplyr::count(stats_test, interest) #&gt; # A tibble: 7 × 2 #&gt; interest n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 30 #&gt; 2 2 47 #&gt; 3 3 66 #&gt; 4 4 41 #&gt; 5 5 45 #&gt; 6 6 9 #&gt; 7 NA 68 dplyr::count(stats_test, no_interest) #&gt; # A tibble: 7 × 2 #&gt; no_interest n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 9 #&gt; 2 2 45 #&gt; 3 3 41 #&gt; 4 4 66 #&gt; 5 5 47 #&gt; 6 6 30 #&gt; 7 NA 68 Scheint zu passen. Noch praktischer ist, dass man so auch numerische Variablen in Bereiche aufteilen kann (“binnen”): stats_test$Ergebnis &lt;- car::recode(stats_test$score, &quot;1:38 = &#39;durchgefallen&#39;; else = &#39;bestanden&#39;&quot;) Natürlich gibt es auch eine Pfeifen komptatible Version, um Variablen umzukodieren bzw. zu binnen: dplyr::recode32. Die Syntax ist allerdings etwas weniger komfortabel (da strenger), so dass wir an dieser Stelle bei car::recode bleiben. 6.1.10.2 Einfaches Umkodieren mit einer Logik-Prüfung Nehmen wir an, wir möchten die Anzahl der Punkte in einer Statistikklausur (score) umkodieren in eine Variable “bestanden” mit den zwei Ausprägungen “ja” und “nein”; der griesgrämige Professor beschließt, dass die Klausur ab 25 Punkten (von 40) bestanden sei. Die Umkodierung ist also von der Art “viele Ausprägungen in zwei Ausprägungen umkodieren”. Das kann man z.B. so erledigen: stats_test$bestanden &lt;- stats_test$score &gt; 24 head(stats_test$bestanden) #&gt; [1] TRUE TRUE TRUE FALSE TRUE TRUE Genauso könnte man sich die “Grenzfälle” - die Bemitleidenswerten mit 24 Punkten - anschauen (knapp daneben ist auch vorbei, so der griesgrämige Professor weiter): stats_test$Grenzfall &lt;- stats_test$score == 24 count(stats_test, Grenzfall) #&gt; # A tibble: 2 × 2 #&gt; Grenzfall n #&gt; &lt;lgl&gt; &lt;int&gt; #&gt; 1 FALSE 294 #&gt; 2 TRUE 12 Natürlich könnte man auch hier “Durchpfeifen”: stats_test &lt;- stats_test %&gt;% mutate(Grenzfall = score == 24) count(stats_test, Grenzfall) #&gt; # A tibble: 2 × 2 #&gt; Grenzfall n #&gt; &lt;lgl&gt; &lt;int&gt; #&gt; 1 FALSE 294 #&gt; 2 TRUE 12 6.1.10.3 Binnen mit cut Numerische Werte in Klassen zu gruppieren (“to bin”, denglisch: “binnen”) kann mit dem Befehl cut (and friends) besorgt werden. Es lassen sich drei typische Anwendungsformen unterscheiden: Eine numerische Variable … in k gleich große Klassen grupieren (gleichgroße Intervalle) so in Klassen gruppieren, dass in jeder Klasse n Beobachtungen sind (gleiche Gruppengrößen) in beliebige Klassen gruppieren 6.1.10.3.1 Gleichgroße Intervalle Nehmen wir an, wir möchten die numerische Variable “Körpergröße” in drei Gruppen einteilen: “klein”, “mittel” und “groß”. Der Range von Körpergröße soll gleichmäßig auf die drei Gruppen aufgeteilt werden, d.h. der Range (Interval) der drei Gruppen soll gleich groß sein. Dazu kann man cut_interval aus ggplot2 nehmen [^d.h. ggplot2 muss geladen sein; wenn man tidyverse lädt, wird ggplot2 automatisch auch geladen]. wo_men &lt;- read_csv(&quot;data/wo_men.csv&quot;) wo_men %&gt;% filter(height &gt; 150, height &lt; 220) -&gt; wo_men2 temp &lt;- cut_interval(x = wo_men2$height, n = 3) levels(temp) #&gt; [1] &quot;[155,172]&quot; &quot;(172,189]&quot; &quot;(189,206]&quot; cut_interval liefert eine Variabel vom Typ factor zurück. 6.1.10.3.2 Gleiche Gruppengrößen temp &lt;- cut_number(wo_men2$height, n = 2) str(temp) #&gt; Factor w/ 2 levels &quot;[155,169]&quot;,&quot;(169,206]&quot;: 1 2 2 2 2 1 1 2 1 2 ... Mit cut_number (aus ggplot2) kann man einen Vektor in n Gruppen mit (etwa) gleich viel Observationen einteilen. Teilt man einen Vektor in zwei gleich große Gruppen, so entspricht das einer Aufteilung am Median (Median-Split). 6.1.10.3.3 In beliebige Klassen gruppieren wo_men$groesse_gruppe &lt;- cut(wo_men$height, breaks = c(-Inf, 100, 150, 170, 200, 230, Inf)) count(wo_men, groesse_gruppe) #&gt; # A tibble: 6 × 2 #&gt; groesse_gruppe n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 (-Inf,100] 4 #&gt; 2 (150,170] 55 #&gt; 3 (170,200] 38 #&gt; 4 (200,230] 2 #&gt; 5 (230, Inf] 1 #&gt; 6 NA 1 cut ist im Standard-R (Paket “base”) enthalten. Mit breaks gibt man die Intervallgrenzen an. Zu beachten ist, dass man eine Unter- bzw. Obergrenze angeben muss. D.h. der kleinste Wert in der Stichprobe wird nicht automatisch als unterste Intervallgrenze herangezogen. Anschaulich gesprochen ist cut ein Messer, das ein Seil (die kontinuierliche Variable) mit einem oder mehreren Schnitten zerschneidet (vgl. Abb. 6.2). 6.2 Deskriptive Statistiken berechnen 6.2.1 Mittelwerte pro Zeile berechnen 6.2.1.1 rowMeans Um Umfragedaten auszuwerten, will man häufig einen Mittelwert pro Zeile berechnen. Normalerweise fasst man eine Spalte zu einer Zahl zusammen; aber jetzt, fassen wir eine Zeile zu einer Zahl zusammen. Der häufigste Fall ist, wie gesagt, einen Mittelwert zu bilden für jede Person. Nehmen wir an, wir haben eine Befragung zur Extraversion durchgeführt und möchten jetzt den mittleren Extraversions-Wert pro Person (d.h. pro Zeile) berechnen. extra &lt;- read.csv(&quot;data/extra.csv&quot;) extra_items &lt;- extra %&gt;% select(i01:i10) # `select` ist aus `dplyr` # oder: # select(extra_items, i01:i10) extra$extra_mw &lt;- rowMeans(extra_items) Da der Datensatz über 28 Spalten verfügt, wir aber nur 10 Spalten heranziehen möchten, um Zeilen auf eine Zahl zusammenzufassen, bilden wir als Zwischenschritt einen “schmäleren” Datensatz, extra_items. Im Anschluss berechnen wir mit rowMeans die Mittelwerte pro Zeile (engl. “row”). 6.2.1.2 Vertiefung: dpyr Alternativ können wir Mittelwerte mit dplyr berechnen: extra_items %&gt;% na.omit %&gt;% rowwise() %&gt;% mutate(mean_row = mean(i01:i10)) %&gt;% select(mean_row) %&gt;% head # nur die ersten paar Zeilen von `mean_row` zeigen #&gt; # A tibble: 6 × 1 #&gt; mean_row #&gt; &lt;dbl&gt; #&gt; 1 2.0 #&gt; 2 1.5 #&gt; 3 2.0 #&gt; 4 2.5 #&gt; 5 4.0 #&gt; 6 3.0 na.omit wirft alle Zeilen raus, in denen fehlende Werte vorkommen. Das ist nötig, damit mean ein Ergebnis ausgibt (bei fehlenden Werten gibt mean sonst NA zurück). rowwise gruppiert den Datensatz nach Zeilen (row_number()), ist also synonym zu: extra_items %&gt;% na.omit %&gt;% group_by(row_number()) %&gt;% mutate(mean_row = mean(i01:i10)) %&gt;% select(mean_row) %&gt;% head # nur die ersten paar Zeilen von `mean_row` zeigen #&gt; Source: local data frame [6 x 2] #&gt; Groups: row_number() [6] #&gt; #&gt; `row_number()` mean_row #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2.0 #&gt; 2 2 1.5 #&gt; 3 3 2.0 #&gt; 4 4 2.5 #&gt; 5 5 4.0 #&gt; 6 6 3.0 6.2.2 Mittelwerte pro Spalte berechnen Eine Möglichkeit ist der Befehl summary aus dplyr. stats_test %&gt;% na.omit %&gt;% summarise(mean(score), sd(score), median(score), IQR(score)) #&gt; mean(score) sd(score) median(score) IQR(score) #&gt; 1 30.6 5.72 31 9 Die Logik von dplyr lässt auch einfach Subgruppenanalysen zu. Z.B. können wir eine Teilmenge des Datensatzes mit filter erstellen und dann mit group_by Gruppen vergleichen: stats_test %&gt;% filter(study_time &gt; 1) %&gt;% group_by(interest) %&gt;% summarise(median(score, na.rm = TRUE)) #&gt; # A tibble: 6 × 2 #&gt; interest `median(score, na.rm = TRUE)` #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 28 #&gt; 2 2 30 #&gt; 3 3 33 #&gt; 4 4 31 #&gt; 5 5 34 #&gt; 6 6 34 Wir können auch Gruppierungskriterien unterwegs erstellen: stats_test %&gt;% na.omit %&gt;% filter(study_time &gt; 1) %&gt;% group_by(intessiert = interest &gt; 3) %&gt;% summarise(median(score)) #&gt; # A tibble: 2 × 2 #&gt; intessiert `median(score)` #&gt; &lt;lgl&gt; &lt;dbl&gt; #&gt; 1 FALSE 30 #&gt; 2 TRUE 32 Die beiden Gruppen von interessiert sind “ja, interessiert” (interest &gt; 3 ist TRUE) und “nein, nicht interessiert” (interest &gt; 3 ist FALSE). Etwas expliziter wäre es, mutate zu verwenden, um die Variable interessiert zu erstellen: stats_test %&gt;% na.omit %&gt;% filter(study_time &gt; 1) %&gt;% mutate(interessiert = interest &gt; 3) %&gt;% group_by(interessiert) %&gt;% summarise(median(score)) #&gt; # A tibble: 2 × 2 #&gt; interessiert `median(score)` #&gt; &lt;lgl&gt; &lt;dbl&gt; #&gt; 1 FALSE 30 #&gt; 2 TRUE 32 Statistiken, die auf dem Mittelwert (arithmetisches Mittel) beruhen, sind nicht robust gegenüber Ausreisern: Schon wenige Extremwerte können diese Statistiken so verzerren, dass sie erheblich an Aussagekraft verlieren. Daher: besser robuste Statistiken verwenden. Der Median, der Modus und der IQR bieten sich an. 6.2.3 Korrelationstabellen berechnen Korrelationen bzw. Korrelationstabellen lassen sich mit dem R-Standardbefehl cor berechnen: stats_test &lt;- read.csv(&quot;data/test_inf_short.csv&quot;) stats_test %&gt;% select(study_time,interest,score) %&gt;% cor() #&gt; study_time interest score #&gt; study_time 1 NA NA #&gt; interest NA 1 NA #&gt; score NA NA 1 Oh! Lauter NAs! Besser wir löschen Zeilen mit fehlenden Werten bevor wir die Korrelation ausrechnen: stats_test %&gt;% select(study_time:score) %&gt;% na.omit %&gt;% cor() #&gt; study_time self_eval interest score #&gt; study_time 1.000 0.559 0.461 0.441 #&gt; self_eval 0.559 1.000 0.360 0.628 #&gt; interest 0.461 0.360 1.000 0.223 #&gt; score 0.441 0.628 0.223 1.000 Alternativ zu cor kann man auch corrr:correlate verwenden: stats_test &lt;- read.csv(&quot;data/test_inf_short.csv&quot;) stats_test %&gt;% select(study_time:score) %&gt;% correlate #&gt; # A tibble: 4 × 5 #&gt; rowname study_time self_eval interest score #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 study_time NA 0.559 0.461 0.441 #&gt; 2 self_eval 0.559 NA 0.360 0.628 #&gt; 3 interest 0.461 0.360 NA 0.223 #&gt; 4 score 0.441 0.628 0.223 NA correlate hat den Vorteil, dass es bei fehlenden Werten einen Wert ausgibt; die Korrelation wird paarweise mit den verfügbaren (nicht-fehlenden) Werten berechnet. Außerdme wird eine Dataframe (genauer: tibble) zurückgeliefert, was häufig praktischer ist zur Weiterverarbeitung. Wir könnten jetzt die resultierende Korrelationstabelle plotten, vorher “rasieren” wir noch das redundaten obere Dreieck ab (da Korrelationstabellen ja symmetrisch sind): stats_test %&gt;% select(study_time:score) %&gt;% correlate %&gt;% shave %&gt;% rplot 6.3 Befehlsübersicht Paket::Funktion Beschreibung na.omit Löscht Zeilen, die fehlende Werte enthalten nrow Liefert die Anzahl der Zeilen des Dataframes zurück complete.cases Gibt die Zeilen ohne fehlenden Werte eines Dataframes zurück car::recode Kodiert Werte um cut Schneidet eine kontinuierliche Variable in Wertebereiche rowMeans Berechnet Zeilen-Mittelwerte dplyr::rowwise Gruppiert nach Zeilen ggplot2::cut_number Schneidet eine kontinuierliche Variable in n gleich große Bereiche ggplot2::cut_interval Schneidet eine kontinuierliche Variable in Intervalle der Größe k head Zeigt nur die ersten Zeilen/Werte eines Dataframes/Vektors an. scale z-skaliert eine Variable dplyr::select_if Wählt eine Spalte aus, wenn ein Kriterium erfüllt ist dplyr::glimpse Gibt einen Überblick über einen Dataframe dplyr::mutate_if definiert eine Spalte, wenn eine Kriterium erfüllt ist : Definiert einen Bereich von … bis … corrr:correlate Berechnet Korrelationtabelle, liefert einen Dataframe zurück cor Berechnet Korrelationtabelle rplot Plottet Korrelationsmatrix von correlate shave “Rasiert” redundantes Dreick in Korrelationsmatrix ab Das sagen Autoren, wenn sie nicht genau wissen, wie etwas funktioniert.↩ aus dem “Standard-R”, d.h. Paket “base”.↩ Hier findet sich eine ausführlichere Darstellung: https://sebastiansauer.github.io/checklist_data_cleansing/index.html↩ https://github.com/drsimonj/corrr↩ https://blog.rstudio.org/2016/06/27/dplyr-0-5-0/↩ "],
["fallstudie-zum-datenjudo.html", "Kapitel 7 Fallstudie zum Datenjudo 7.1 Achtung, Fallstudie 7.2 Befehlsübersicht 7.3 Verweise", " Kapitel 7 Fallstudie zum Datenjudo Lernziele: Grundlegende Funktionen von dplyr andwenden können. Das Konzept der Pfeife in einem echten Datensatz anwenden können. Auch mit relativ großen Daten sicher hantieren können. Schauen wir uns einige Beispiele der Datenaufbereitung mittels dplyr anhand einer Fallstudie an. Wir verwenden hier den Datensatz flightsaus dem Package nycflights13. Der Datensatz ist recht groß (~300.000 Zeilen und 19 Spalten); wenn man ihn als Excel importiert, kann eine alte Möhre von Computer schon in die Knie gehen. Beim Import als CSV habe ich noch nie von Problemen gehört; beim Import via Package ebenfalls nicht. Werfen wir einen ersten Blick in die Daten: Laden wir zuerst dplyr and friends. Das geht mit dem Paket tidyverse, welches diese Pakete lädt: library(tidyverse) library(nycflights13) # für die Daten Dann laden wir die Daten aus dem Paket nycflights13 und werfen eine Blick hinein (“to glimpse”). glimpse zeigt uns einen Überblick über den Dataframe. data(flights) glimpse(flights) #&gt; Observations: 336,776 #&gt; Variables: 19 #&gt; $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... #&gt; $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... #&gt; $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... #&gt; $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 55... #&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 60... #&gt; $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2... #&gt; $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 7... #&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 7... #&gt; $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -... #&gt; $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;,... #&gt; $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79... #&gt; $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN... #&gt; $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;... #&gt; $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;... #&gt; $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138... #&gt; $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 94... #&gt; $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5,... #&gt; $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, ... #&gt; $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013... Der Befehl data lädt Daten aus einem zuvor gestarteten Paket. 7.1 Achtung, Fallstudie Sie sind der/die Assistent_in des Chefs der New Yorker Flughäfen. Ihr Chef kommt gut gelaunt ins Büro und sagt, dass er diesen Schnarchnasen einheizen wolle und sagt, sie sollen ihm mal schnell die Flüge mit der größten Verspätung raussuchen. Nix schickes, aber zacki-zacki… flights %&gt;% arrange(arr_delay) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 5 7 1715 1729 -14 1944 #&gt; 2 2013 5 20 719 735 -16 951 #&gt; 3 2013 5 2 1947 1949 -2 2209 #&gt; 4 2013 5 6 1826 1830 -4 2045 #&gt; 5 2013 5 4 1816 1820 -4 2017 #&gt; 6 2013 5 2 1926 1929 -3 2157 #&gt; 7 2013 5 6 1753 1755 -2 2004 #&gt; 8 2013 5 7 2054 2055 -1 2317 #&gt; 9 2013 5 13 657 700 -3 908 #&gt; 10 2013 1 4 1026 1030 -4 1305 #&gt; # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Hm, übersichtlicher wäre es wahrscheinlich, wenn wir weniger Spalten anschauen müssten. Am besten neben der Verspätung nur die Information, die wir zur Identifizierung der Schuldigen… will sagen der gesuchten Flüge benötigen flights %&gt;% arrange(arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) #&gt; # A tibble: 336,776 × 8 #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 -86 VX 5 7 1715 N843VA 193 SFO #&gt; 2 -79 VX 5 20 719 N840VA 11 SFO #&gt; 3 -75 UA 5 2 1947 N851UA 612 LAX #&gt; 4 -75 AA 5 6 1826 N3KCAA 269 SEA #&gt; 5 -74 AS 5 4 1816 N551AS 7 SEA #&gt; 6 -73 UA 5 2 1926 N24212 1628 SFO #&gt; 7 -71 DL 5 6 1753 N3760C 1394 PDX #&gt; 8 -71 UA 5 7 2054 N806UA 622 SFO #&gt; 9 -71 B6 5 13 657 N805JB 671 LAX #&gt; 10 -70 VX 1 4 1026 N855VA 23 SFO #&gt; # ... with 336,766 more rows Da Zahlen in ihrer natürlichen Form von klein nach groß sortiert sind, sortiert arrange in ebendieser Richtung. Wir können das umdrehen mit einem Minuszeichen vor der zu sortierenden Spalte: flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) #&gt; # A tibble: 336,776 × 8 #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1272 HA 1 9 641 N384HA 51 HNL #&gt; 2 1127 MQ 6 15 1432 N504MQ 3535 CMH #&gt; 3 1109 MQ 1 10 1121 N517MQ 3695 ORD #&gt; 4 1007 AA 9 20 1139 N338AA 177 SFO #&gt; 5 989 MQ 7 22 845 N665MQ 3075 CVG #&gt; 6 931 DL 4 10 1100 N959DL 2391 TPA #&gt; 7 915 DL 3 17 2321 N927DA 2119 MSP #&gt; 8 895 DL 7 22 2257 N6716C 2047 ATL #&gt; 9 878 AA 12 5 756 N5DMAA 172 MIA #&gt; 10 875 MQ 5 3 1133 N523MQ 3744 ORD #&gt; # ... with 336,766 more rows Oh halt, wir wollen keine Tabelle mit 300.000 Zeilen (der Chef ist kein Freund von Details). Also begrenzen wir die Ausgabe auf die ersten 10 Plätze. flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% filter(row_number() &lt; 11) #&gt; # A tibble: 10 × 8 #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1272 HA 1 9 641 N384HA 51 HNL #&gt; 2 1127 MQ 6 15 1432 N504MQ 3535 CMH #&gt; 3 1109 MQ 1 10 1121 N517MQ 3695 ORD #&gt; 4 1007 AA 9 20 1139 N338AA 177 SFO #&gt; 5 989 MQ 7 22 845 N665MQ 3075 CVG #&gt; 6 931 DL 4 10 1100 N959DL 2391 TPA #&gt; 7 915 DL 3 17 2321 N927DA 2119 MSP #&gt; 8 895 DL 7 22 2257 N6716C 2047 ATL #&gt; 9 878 AA 12 5 756 N5DMAA 172 MIA #&gt; 10 875 MQ 5 3 1133 N523MQ 3744 ORD “Geht doch”, war die Antwort des Chefs, als sie die Tabelle rübergeben (er mag auch keine Emails). “Ach ja”, raunt der Chef, als Sie das Zimmer verlassen wollen, “hatte ich erwähnt, dass ich die gleiche Auswertung für jeden Carrier brauche? Reicht bis in einer halben Stunde”. Wir gruppieren also den Datensatz nach der Fluggesellschaft (carrier) und filtern dann die ersten 3 Zeilen (damit die Tabelle für den Chef nicht zu groß wird). Wie jeder dplyr-Befehl wird die vorherige Gruppierung berücksichtigt und daher die Top-3-Zeilen pro Gruppe, d.h. pro Fluggesellschaft, ausgegeben. flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% group_by(carrier) %&gt;% filter(row_number() &lt; 4) #&gt; Source: local data frame [48 x 8] #&gt; Groups: carrier [16] #&gt; #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1272 HA 1 9 641 N384HA 51 HNL #&gt; 2 1127 MQ 6 15 1432 N504MQ 3535 CMH #&gt; 3 1109 MQ 1 10 1121 N517MQ 3695 ORD #&gt; 4 1007 AA 9 20 1139 N338AA 177 SFO #&gt; 5 989 MQ 7 22 845 N665MQ 3075 CVG #&gt; 6 931 DL 4 10 1100 N959DL 2391 TPA #&gt; 7 915 DL 3 17 2321 N927DA 2119 MSP #&gt; 8 895 DL 7 22 2257 N6716C 2047 ATL #&gt; 9 878 AA 12 5 756 N5DMAA 172 MIA #&gt; 10 852 AA 5 19 713 N3HEAA 257 LAS #&gt; # ... with 38 more rows Vielleicht gefällt dem Chef diese Darstellung (sortiert nach carrier) besser: flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% group_by(carrier) %&gt;% filter(row_number() &lt; 4) %&gt;% arrange(carrier) #&gt; Source: local data frame [48 x 8] #&gt; Groups: carrier [16] #&gt; #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 744 9E 2 16 757 N8940E 3798 CLT #&gt; 2 458 9E 7 24 1525 N927XJ 3538 MSP #&gt; 3 421 9E 7 10 2054 N937XJ 3325 DFW #&gt; 4 1007 AA 9 20 1139 N338AA 177 SFO #&gt; 5 878 AA 12 5 756 N5DMAA 172 MIA #&gt; 6 852 AA 5 19 713 N3HEAA 257 LAS #&gt; 7 198 AS 3 8 1012 N587AS 11 SEA #&gt; 8 196 AS 1 20 2157 N305AS 7 SEA #&gt; 9 188 AS 5 23 2205 N516AS 7 SEA #&gt; 10 497 B6 1 16 1622 N661JB 517 MCO #&gt; # ... with 38 more rows Da Sie den Chef gut kennen, berechnen Sie gleich noch die durchschnittliche Verspätung pro Fluggesellschaft. flights %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% group_by(carrier) %&gt;% summarise(delay_mean = mean(arr_delay, na.rm = TRUE)) %&gt;% arrange(-delay_mean) %&gt;% head #&gt; # A tibble: 6 × 2 #&gt; carrier delay_mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 F9 21.9 #&gt; 2 FL 20.1 #&gt; 3 EV 15.8 #&gt; 4 YV 15.6 #&gt; 5 OO 11.9 #&gt; 6 MQ 10.8 Der Chef ist zufrieden. Sie können sich wieder wichtigeren Aufgaben zuwenden… 7.2 Befehlsübersicht Funktion Beschreibung data Lädt Daten aus einem Paket. dplyr::glimpse Zeigt einen Überblick über einen Datensatz dplyr::row_number Gibt die Zeilennummern zurück. 7.3 Verweise Eine ausführlichere Version einer “YACSDA”33 findet sich hier34 oder hier35. Es finden sich online viele ähnliche Datenanalysen zu dplyr, z.B. hier36 oder hier37. yet another case study on data analysis↩ https://sebastiansauer.github.io/Fallstudie_Flights/↩ https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html↩ http://stat545.com/block009_dplyr-intro.html↩ http://genomicsclass.github.io/book/pages/dplyr_tutorial.html↩ "],
["daten-visualisieren.html", "Kapitel 8 Daten visualisieren 8.1 Ein Bild sagt mehr als 1000 Worte 8.2 Die Anatomie eines Diagramms 8.3 Einstieg in ggplot2 - qplot 8.4 Häufige Arten von Diagrammen 8.5 Die Gefühlswelt von ggplot2 8.6 Aufgaben 8.7 Lösungen 8.8 Befehlsübersicht 8.9 Vertiefung: Geome bei ggplot2 8.10 Verweise", " Kapitel 8 Daten visualisieren Lernziele: An einem Beispiel erläutern können, warum/ wann ein Bild mehr sagt, als 1000 Worte. Häufige Arten von Diagrammen erstellen können. Diagramme bestimmten Zwecken zuordnen können. In diesem Kapitel werden folgende Pakete benötigt:: library(tidyverse) # Zum Plotten # library(car) # Umkodieren # library(knitr) # HTML-Tabellen Dieses Kapitel erläutert das Daten visualisieren anhand des R-Pakets ggplot2. 8.1 Ein Bild sagt mehr als 1000 Worte Ein Bild sagt bekanntlich mehr als 1000 Worte. Schauen wir uns zur Verdeutlichung das berühmte Beispiel von Anscombe38 an. Es geht hier um vier Datensätze mit zwei Variablen (Spalten; X und Y). Offenbar sind die Datensätze praktisch identisch: Alle X haben den gleichen Mittelwert und die gleiche Varianz; dasselbe gilt für die Y. Die Korrelation zwischen X und Y ist in allen vier Datensätzen gleich. Allerdings erzählt eine Visualisierung der vier Datensätze eine ganz andere Geschichte. Abbildung 8.1: Das Anscombe-Quartett Offenbar “passieren” in den vier Datensätzen gänzlich unterschiedliche Dinge. Dies haben die Statistiken nicht aufgedeckt; erst die Visualisierung erhellte uns… Kurz: Die Visualisierung ist ein unverzichtbares Werkzeug, um zu verstehen, was in einem Datensatz (und damit in der zugrunde liegenden “Natur”) passiert. Es gibt viele Möglichkeiten, Daten zu visualisieren (in R). Wir werden uns hier auf einen Weg bzw. ein Paket konzentrieren, der komfortabel, aber mächtig ist und gut zum Prinzip des Durchpfeifens passt: ggplot239. 8.2 Die Anatomie eines Diagramms ggplot2 unterscheidet folgende Bestandteile (“Anatomie”) eines Diagramms (vgl. Abb. 8.2): Daten Abbildende Aspekte (Achsen, Farben, …) Geome (statistische Bilder wie Punkte, Linien, Boxplots, …) Abbildung 8.2: Anatomie eines Diagramms Bei Daten muss ein Dataframe angegeben werden. Zu den abbildenden Aspekte (in ggplot2 als “aesthetics” bezeichnet) zählen vor allem die Achsen, aber auch Farben u.a. Was ist mit abbildend gemeint? Weist man einer Achse einen Variable zu, so wird jede Ausprägung der Variablen einer Ausprägung der Achse zugeordnet (welcher Wert genau entscheidet ggplot2 für uns, wenn wir es nicht explizieren). Mit Geom ist das eigentlich Art von “Bild” gemeint, wie Punkt, Linie oder Boxplot (vgl. Abschnitt 8.9). 8.3 Einstieg in ggplot2 - qplot Los geht’s! Laden wir zuerst den Datensatz nycflights::flights. data(flights, package = &quot;nycflights13&quot;) qplot(x = carrier, y = arr_delay, geom = &quot;boxplot&quot;, data = flights) Abbildung 8.3: Mittlere Verspätung nach Flugggesellschaft Schauen wir uns den Befehl qplot etwas näher an. Wie ist er aufgebaut? qplot: Erstelle schnell (q wie quick in qplot) mal einen Plot (engl. “plot”: Diagramm). x: Der X-Achse soll die Variable “carrier” zugeordnet werden. y: Der Y-Achse soll die Variable “arr_dely” zugeorndet werden. geom: (“geometriches Objekt”) Gemalt werden soll ein Boxplot, nicht etwa Punkte, Linien oder sonstiges. data: Als Datensatz bitte flights verwenden. Offenbar gibt es viele Extremwerte, was die Verspätung betrifft. Das erscheint mir nicht unplausibel (Schneesturm im Winter, Flugzeug verschwunden…). Vor dem Hintergrund der Extremwerte erscheinen die mittleren Verspätungen (Mediane) in den Boxplots als ähnlich. Vielleicht ist der Unterschied zwischen den Monaten ausgeprägter? qplot(x = factor(month), y = arr_delay, geom = &quot;boxplot&quot;, data = flights) Kaum ein Unterschied ersichtlich; das spricht gegen die Schneesturm-Idee als Grund für Verspätung. Aber schauen wir uns zuerst die Syntax von qplot näher an. “q” in qplot steht für “quick”. Tatsächlich hat qplot einen großen Bruder, ggplot40, der deutlich mehr Funktionen aufweist - und daher auch die umfangreichere (=komplexere) Syntax. Fangen wir mit qplot an. Diese Syntax des letzten Beispiels ist recht einfach, nämlich: qplot (x = X_Achse, y = Y_Achse, data = mein_dataframe, geom = &quot;ein_geom&quot;) Wir definieren mit x, welche Variable der X-Achse des Diagramms zugewiesen werden soll, z.B. month; analog mit Y-Achse. Mit data sagen wir, in welchem Dataframe die Spalten “wohnen” und als “geom” ist die Art des statistischen “geometrischen Objects” gemeint, also Punkte, Linien, Boxplots, Balken… 8.4 Häufige Arten von Diagrammen Unter den vielen Arten von Diagrammen und vielen Arten, diese zu klassifizieren greifen wir uns ein paar häufige Diagramme heraus und schauen uns diese der Reihe nach an. 8.4.1 Eine kontinuierliche Variable Schauen wir uns die Verteilung der Schuhgrößen von Studierenden an. wo_men &lt;- read.csv(&quot;data/wo_men.csv&quot;) qplot(x = shoe_size, data = wo_men) Weisen wir nur der X-Achse (aber nicht der Y-Achse) eine kontinuierliche Variable zu, so wählt ggplot2 automatisch als Geom automatisch ein Histogramm; wir müssen daher nicht explizieren, dass wir ein Histogramm als Geom wünschen (aber wir könnten es hinzufügen). Alternativ wäre ein Dichtediagramm hier von Interesse: # qplot(x = shoe_size, data = wo_men) wie oben qplot(x = shoe_size, data = wo_men, geom = &quot;density&quot;) Was man sich merken muss, ist, dass hier nur das Geom mit Anführungsstrichen zu benennen ist, die übrigen Parameter ohne. Vielleicht wäre es noch schön, beide Geome zu kombinieren in einem Diagramm. Das ist etwas komplizierter; wir müssen zum großen Bruder ggplot umsteigen, da qplot nicht diese Funktionen anbietet. ggplot(data = wo_men) + aes(x = shoe_size) + geom_histogram(aes(y = ..density..), alpha = .7) + geom_density(color = &quot;blue&quot;) Zuerst haben wir mit dem Parameter data den Dataframe benannt. aes definiert, welche Variablen welchen Achsen (oder auch z.B. Füllfarben) zugewiesen werden. Hier sagen wir, dass die Schuhgröße auf X-Achse stehen soll. Das +-Zeichen trennt die einzelnen Bestandteile des ggplot-Aufrufs voneinander. Als nächstes sagen wir, dass wir gerne ein Histogram hätten: geom_histogram. Dabei soll aber nicht wie gewöhnlich auf der X-Achse die Häufigkeit stehen, sondern die Dichte. ggplot berechnet selbständig die Dichte und nennt diese Variable ..density..; die vielen Punkte sollen wohl klar machen, dass es sich nicht um eine “normale” Variable aus dem eigenen Datenframe handelt, sondern um eine “interne” Variable von ggplot - die wir aber nichtsdestotrotz verwenden können. alpha bestimmt die “Durchsichtigkeit” eines Geoms; spielen Sie mal etwas damit herum. Schließlich malen wir noch ein blaues Dichtediagramm über das Histogramm. Wünsche sind ein Fass ohne Boden… Wäre es nicht schön, ein Diagramm für Männer und eines für Frauen zu haben, um die Verteilungen vergleichen zu können? qplot(x = shoe_size, data = wo_men, geom = &quot;density&quot;, color = sex) qplot(x = shoe_size, data = wo_men, geom = &quot;density&quot;, fill = sex, alpha = I(.7)) Hier sollten vielleicht noch die Extremwerte entfernt werden, um den Blick auf das Gros der Werte nicht zu verstellen: wo_men %&gt;% filter(shoe_size &lt;= 47) -&gt; wo_men2 qplot(x = shoe_size, data = wo_men2, geom = &quot;density&quot;, fill = sex, alpha = I(.7)) Besser. Man kann das Durchpfeifen auch bis zu qplot weiterführen: wo_men %&gt;% filter(shoe_size &lt;= 47) %&gt;% qplot(x = shoe_size, data = ., geom = &quot;density&quot;, fill = sex, alpha = I(.7)) Die Pfeife versucht im Standard, das Endprodukt des letzten Arbeitsschritts an den ersten Parameter des nächsten Befehls weiterzugeben. Ein kurzer Blick in die Hilfe von qplot zeigt, dass der erste Parameter nicht data ist, sondern x. Daher müssen wir explizit sagen, an welchen Parameter wir das Endprodukt des letzen Arbeitsschritts geben wollen. Netterweise müssen wir dafür nicht viel tippen: Mit einem schlichten Punkt . können wir sagen “nimm den Dataframe, so wie er vom letzten Arbeitsschritt ausgegeben wurde”. Mit fill = sex sagen wir qplot, dass er für Männer und Frauen jeweils ein Dichtediagramm erzeugen soll; jedem Dichtediagramm wird dabei eine Farbe zugewiesen (die uns ggplot2 im Standard voraussucht). Mit anderen Worten: Die Werte von sex werden der Füllfarbe der Histogramme zugeordnet. Anstelle der Füllfarbe hätten wir auch die Linienfarbe verwenden können; die Syntax wäre dann: color = sex. 8.4.2 Zwei kontinuierliche Variablen Ein Streudiagramm ist die klassische Art, zwei metrische Variablen darzustellen. Das ist mit qplot einfach: qplot(x = height, y = shoe_size, data = wo_men) Wir weisen wieder der X-Achse und der Y-Achse eine Variable zu; handelt es sich in beiden Fällen um Zahlen, so wählt ggplot2 automatisch ein Streudiagramm - d.h. Punkte als Geom (geom = &quot;point&quot;). Wir sollten aber noch die Extremwerte herausnehmen: wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% qplot(x = height, y = shoe_size, data = .) Der Trend ist deutlich erkennbar: Je größer die Person, desto länger die Füß´. Zeichnen wir noch eine Trendgerade ein. wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% qplot(x = height, y = shoe_size, data = .) + geom_smooth(method = &quot;lm&quot;) Synonym könnten wir auch schreiben: wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% ggplot() + aes(x = height, y = shoe_size) + geom_point() + geom_smooth(method = &quot;lm&quot;) Da ggplot als ersten Parameter die Daten erwartet, kann die Pfeife hier problemlos durchgereicht werden. Innerhalb eines ggplot-Aufrufs werden die einzelne Teile durch ein Pluszeichen + voneinander getrennt. Nachdem wir den Dataframe benannt haben, definieren wir die Zuweisung der Variablen zu den Achsen mit aes (“aes” wie “aesthetics”, also das “Sichtbare” eines Diagramms, die Achsen etc., werden definiert). Ein “Smooth-Geom” ist eine Linie, die sich schön an die Punkte anschmiegt, in diesem Falls als Gerade (lineares Modell, lm). Bei sehr großen Datensätze, sind Punkte unpraktisch, da sie sich überdecken (“overplotting”). Ein Abhilfe ist es, die Punkte nur “schwach” zu färben. Dazu stellt man die “Füllstärke” der Punkte über alpha ein: geom_point(alpha = 1/100). Um einen passablen Alpha-Wert zu finden, bedarf es häufig etwas Probierens. Zu beachten ist, dass es mitunter recht lange dauert, wenn ggplot viele (&gt;100.000) Punkte malen soll. Bei noch größeren Datenmengen bietet sich an, den Scatterplot als “Schachbrett” aufzufassen, und das Raster einzufärben, je nach Anzahl der Punkte pro Schachfeld; zwei Geome dafür sind geom_hex() und geom_bin2d(). data(flights, package = &quot;nycflights13&quot;) nrow(flights) # groß! #&gt; [1] 336776 ggplot(flights) + aes(x = distance, y = air_time) + geom_hex() Wenn man dies verdaut hat, wächst der Hunger nach einer Aufteilung in Gruppen. wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) -&gt; wo_men2 wo_men2 %&gt;% qplot(x = height, y = shoe_size, color = sex, data = .) Mit color = sex sagen wir, dass die Linienfarbe (der Punkte) entsprechend der Stufen von sex eingefärbt werden sollen. Die genaue Farbwahl übernimmt ggplot2 für uns. Alternativ kann man auch zwei “Teil-Bildchen” (“facets”) erstellen, eines für Frauen und eines für Männer: wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% qplot(x = height, y = shoe_size, facets = &quot;~sex&quot;, color = sex, data = .) Man beachte die Tilde ~, die vor die “Gruppierungsvariable” sex zu setzen ist. 8.4.2.1 Vertiefung zu Facetten Ein netter visueller Effekt wird erreicht, wenn in jeder Facette zwar alle Punkte gezeigt werden in einem leichten Grau. Aber farbig betont werden nur die Punkte, die zur jeweiligen Gruppe gehören. Der optische Eindruck erklärt es einfacher als Worte: wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% select(-sex) -&gt; wo_men4 wo_men4 %&gt;% ggplot(aes(x = height, y = shoe_size)) + geom_point(color = &quot;grey80&quot;) + facet_wrap(~sex) + geom_point(data = wo_men2, aes(color = sex)) Der “ggplot-Trick” ist, zuerst die Punkte ohne Gruppierungsinformation (hier: sex) zu plotten. Danach plotten wir die nach Gruppenzugehörigkeit gefärbten Punkte. 8.4.3 Eine diskrete Variable Bei diskreten Variablen, vor allem nominalen Variablen, geht es in der Regel darum, Häufigkeiten auszuzählen. Wie viele Männer und Frauen sind in dem Datensatz? qplot(x = sex, data = wo_men) Falls nur die X-Achse definiert ist und dort eine Faktorvariable oder eine Text-Variable steht, dann nimmt qplot automatisch ein Balkendiagramm als Geom. Entfernen wir vorher noch die fehlenden Werte: wo_men %&gt;% na.omit() %&gt;% qplot(x = sex, data = .) Wir könnten uns jetzt die Frage stellen, wie viele kleine und viele große Menschen es bei Frauen und bei den Männern gibt. Dazu müssen wir zuerst eine Variable wie “Größe gruppiert” erstellen mit zwei Werten: “klein” und “groß”. Nennen wir sie groesse_gruppe wo_men$groesse_gruppe &lt;- car::recode(wo_men$height, &quot;lo:175 = &#39;klein&#39;; else = &#39;gross&#39;&quot;) wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% na.omit -&gt; wo_men2 qplot(x = sex, fill = groesse_gruppe, data = wo_men2) In Worten sagt der recode-Befehl hier in etwa: “Kodiere wo_men$height um, und zwar vom kleinsten (lo) Wert bis 170 soll den Wert klein bekommen, ansonsten bekommt eine Größe den Wert gross”. Hier haben wir qplot gesagt, dass der die Balken entsprechend der Häufigkeit von groesse_gruppe füllen soll. Und bei den Frauen sind bei dieser Variablen die Werte klein häufig; bei den Männern hingegen die Werte gross. Schön wäre noch, wenn die Balken Prozentwerte angeben würden. Das geht mit qplot (so) nicht; wir schwenken auf ggplot um41. wo_men2 %&gt;% ggplot() + aes(x = sex, fill = groesse_gruppe) + geom_bar(position = &quot;fill&quot;) Schauen wir uns die Struktur des Befehls ggplot näher an. wo_men2: Hey R, nimm den Datensatz wo_men2 UND DANN… ggpplot() : Hey R, male ein Diagramm von Typ ggplot (mit dem Datensatz aus dem vorherigen Pfeifen-Schritt, d.h. aus der vorherigen Zeile, also wo_men2)! +: Das Pluszeichen grenzt die Teile eines ggplot-Befehls voneinander ab. aes: von “aethetics”, also welche Variablen des Datensatzes den sichtbaren Aspekten (v.a. Achsen, Farben) zugeordnet werden. x: Der X-Achse (Achtung, x wird klein geschrieben hier) wird die Variable sex zugeordnet. y: gibt es nicht??? Wenn in einem ggplot-Diagramm keine Y-Achse definiert wird, wird ggplot automatisch ein Histogramm bzw. ein Balkendiagramm erstellen. Bei diesen Arten von Diagrammen steht auf der Y-Achse keine eigene Variable, sondern meist die Häufigkeit des entsprechenden X-Werts (oder eine Funktion der Häufigkeit, wie relative Häufigkeit). fill Das Diagramm (die Balken) sollen so gefüllt werden, dass sich die Häufigkeit der Werte von groesse_gruppe darin widerspiegelt. geom_XYZ: Als “Geom” soll ein Balken (“bar”) gezeichnet werden. Ein Geom ist in ggplot2 das zu zeichnende Objekt, also ein Boxplot, ein Balken, Punkte, Linien etc. Entsprechend wird gewünschte Geom mit geom_bar, geom_boxplot, geom_point` etc. gewählt. position = fill: position_fill will sagen, dass die Balken alls eine Höhe von 100% (1) haben. Die Balken zeigen also nur die Anteile der Werte der fill-Variablen. Die einzige Änderung in den Parametern ist position = &quot;fill&quot;. Dieser Parameter weist ggplot an, die Positionierung der Balken auf die Darstellung von Anteilen auszulegen. Damit haben alle Balken die gleiche Höhe, nämlich 100% (1). Aber die “Füllung” der Balken schwankt je nach der Häufigkeit der Werte von groesse_gruppe pro Balken (d.h. pro Wert von sex). Wir sehen, dass die Anteile von großen bzw. kleinen Menschen bei den beiden Gruppen (Frauen vs. Männer) unterschiedlich hoch ist. Dies spricht für einen Zusammenhang der beiden Variablen; man sagt, die Variablen sind abhängig (im statistischen Sinne). Je unterschiedlicher die “Füllhöhe”, desto stärker sind die Variablen (X-Achse vs. Füllfarbe) voneinander abhängig (bzw. desto stärker der Zusammenhang). 8.4.4 Zwei diskrete Variablen Arbeitet man mit nominalen Variablen, so sind Kontingenztabellen Täglich Brot. Z.B.: Welche Produkte wurden wie häufig an welchem Standort verkauft? Wie ist die Verteilung von Alkoholkonsum und Körperform bei Menschen einer Single-Börse. Bleiben wir bei letztem Beispiel. data(profiles, package = &quot;okcupiddata&quot;) profiles %&gt;% count(drinks, body_type) %&gt;% ggplot + aes(x = drinks, y = body_type, fill = n) + geom_tile() + theme(axis.text.x = element_text(angle = 90)) Was haben wir gemacht? Also: Nehme den Datensatz “profiles” UND DANN Zähle die Kombinationen von “drinks” und “body_type” UND DANN Erstelle ein ggplot-Plot UND DANN Weise der X-Achse “drinks” zu, der Y-Achse “body_type” und der Füllfarbe “n” UND DANN Male Fliesen UND DANN Passe das Thema so an, dass der Winkel für Text der X-Achse auf 90 Grad steht. Was sofort ins Auge sticht, ist dass “soziales Trinken”, nennen wir es mal so, am häufigsten ist, unabhängig von der Körperform. Ansonsten scheinen die Zusammenhäng nicht sehr stark zu sein. 8.4.5 Zusammenfassungen zeigen Manchmal möchten wir nicht die Rohwerte einer Variablen darstellen, sondern z.B. die Mittelwerte pro Gruppe. Mittelwerte sind eine bestimmte Zusammenfassung einer Spalte; also fassen wir zuerst die Körpergröße zum Mittelwert zusammen - gruppiert nach Geschlecht. wo_men2 %&gt;% group_by(sex) %&gt;% summarise(Groesse_MW = mean(height)) -&gt; wo_men3 wo_men3 #&gt; # A tibble: 2 × 2 #&gt; sex Groesse_MW #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 man 183 #&gt; 2 woman 167 Diese Tabelle schieben wir jetzt in ggplot2; natürlich hätten wir das gleich in einem Rutsch durchpfeifen können. wo_men3 %&gt;% qplot(x = sex, y = Groesse_MW, data = .) Das Diagramm besticht nicht durch die Tiefe und Detaillierung. Wenn wir noch zusätzlich die Mittelwerte nach Groesse_Gruppe ausweisen, wird das noch überschaubar bleiben. wo_men2 %&gt;% group_by(sex, groesse_gruppe) %&gt;% summarise(Groesse_MW = mean(height)) %&gt;% qplot(x = sex, color = factor(groesse_gruppe), y = Groesse_MW, data = .) 8.5 Die Gefühlswelt von ggplot2 Geben Sie eine diskrete X-Achse an und *keine Y-AchseÜ, so greift qplot im Standard auf das Geom bar zurück (Balkendiagramm), falls Sie kein Geom angeben: qplot(x = smoker, data = tips) # identisch zu qplot(x = smoker, data = tips, geom = &quot;bar) Geben Sie eine kontinuierliche X-Achse an und keine Y-Achse, so greift qplot im Standard auf das Geom histogram zurück (Histogramm). qplot(x = smoker, data = tips) # identisch zu qplot(x = smoker, data = tips, geom = &quot;histogram&quot;) Geben Sie eine kontinuierliche X-Achse an und eine kontinuierliche Y-Achse an, so greift qplot im Standard auf das Geom point zurück (Streudiagramm). qplot(x = total_bill, y = tip, data = tips) # identisch zu qplot(x = total_bill, y= tip, data = tips, geom = &quot;point&quot;) Möchten Sie mehrere Geome für eine Variable darstellen, so muss die Variable diskret sein: #oh no: qplot(x = rating, y = affairs, geom = &quot;boxplot&quot;, data = Affairs) #oh yes: qplot(x = factor(rating), y = affairs, geom = &quot;boxplot&quot;, data = Affairs) #oh yes: qplot(x = gender, y = affairs, geom = &quot;boxplot&quot;, data = Affairs) 8.6 Aufgaben Erzählen Sie einer vertrauenswürdigen Person jeweils eine “Geschichte”, die das Zustandekommen der vier Plots von Anscombe (Abb. 8.1) erklärt! Abb. 8.3 stellt die mittlere Verspätung verschiedener Fluggesellschaften dar; als “Geom” wird ein Boxplot verwendet. Andere Geome wären auch möglich - aber wie sinnvoll wären sie? Erstellen Sie ein Diagramm, welches Histogramme der Verspätung verwendet anstelle von Boxplots! Damit das Diagramm nicht so groß wird, nehmen Sie zur Gruppierung nicht carrier sondern origin. Ist das Histogramm genauso erfolgreich wie der Boxplot, wenn es darum geht, viele Verteilungen vergleichend zu präsentieren? Warum? Erstellen Sie ein sehr grobes und ein sehr feines Histogramm für die Schuhgröße! Vertiefung: Erstellen Sie ein Diagramm, das sowohl eine Zusammenfassung (Mittelwert) der Körpergrößen nach Geschlecht darstellt als auch die einzelnen Werte darstellt! 8.7 Lösungen :-) : qplot(x = arr_delay, geom = &quot;histogram&quot;, data = flights, facets = &quot;~origin&quot;) Der Boxplot ist besser geeignet, um mehrere Verteilungen vergleichend zu präsentieren. Durch die gleiche Ausrichtung der Boxplots ist es dem Auge viel einfacher, Vergleiche anzustellen im Vergleich zu den Histogrammen. Einen optisch schönenen Effekt könnte man mit geom_jitter anstelle von geom_pointerreichen. Auch die Reihenfolge der beiden Geome könnte man umdrehen. Natürlich ist auch an Form, Größe und Farbe der Geome noch zu feilen. : qplot(x = shoe_size, data = wo_men, bins = 10) qplot(x = shoe_size, data = wo_men, bins = 50) : wo_men2 %&gt;% group_by(sex) %&gt;% summarise(height = mean(height)) -&gt; wo_men3 wo_men3 %&gt;% ggplot() + aes(x = sex, y = height) + geom_point(color = &quot;red&quot;, size = 8) + geom_point(data = wo_men2, color = &quot;grey80&quot;) Der “Trick” ist hier, erst die zusammengefassten Daten in ein Geom zu stecken (wo_men3). Dann werden die Rohdaten (wo_men2) ebenfalls in ein Geom gepackt. Allerdings muss die Achsen-Beschriftung bei beiden Geomen identisch sein, sonst gibt es eine Fehlermeldung. 8.8 Befehlsübersicht Paket::Funktion Beschreibung ggplot2::qplot Malt schnell mal einen Plot ggplot2::ggplot Malt einen Plot factor Wandelt einen Vektor in den Typ factor um 8.9 Vertiefung: Geome bei ggplot2 Einen guten Überblick über Geome bietet das Cheatsheet von ggplot242. Verschiedenen Taxonomien von statistischen “Bildchen” sind denkbar; eine einfache ist die folgende; es wird nur ein Teil der verfügbaren Geome dargestellt. Eine kontinuerliche Variable Zwei kontinuierliche Variablen Eine diskrete Variable (X-Achse) ggplot(tips) + aes(x = day) + geom_bar() Eine diskrete Variable auf der X-Achse und eine kontinuierliche Y-Achse 8.10 Verweise Einen Befehlsüberblick zu ggplot2 findet sich hier: http://ggplot2.tidyverse.org/reference/. Edward Tufte gilt als Grand Seigneur der Datenvisualisierung; er hat mehrere lesenswerte Bücher zu dem Thema geschrieben (Tufte 2001; Tufte 2006; Tufte 1990). William Cleveland, ein amerikanischer Statistiker ist bekannt für seine grundlegenden, und weithin akzeptierten Ansätze für Diagramme, die die wesentliche Aussage schnörkellos transportieren (Cleveland 1993). Die (graphische) Auswertung von Umfragedaten basiert häufig auf Likert-Skalen. Ob diese metrisches Niveau aufweisen, darf bezweifelt werden. Hier findet sich einige vertiefenden Überlegungen dazu und zur Frage, wie Likert-Daten ausgewertet werden könnten: https://bookdown.org/Rmadillo/likert/. Es finden sich viele Tutorials online zu ggplot2; ein deutschsprachiger Tutorial findet sich hier: http://md.psych.bio.uni-goettingen.de/mv/unit/ggplot2/ggplot2.html. Literaturverzeichnis "],
["fallstudie-zur-visualisierung.html", "Kapitel 9 Fallstudie zur Visualisierung 9.1 Daten einlesen 9.2 Daten umstellen 9.3 Diagramme für Anteile 9.4 Um 90° drehen 9.5 Text-Labels für die Items 9.6 Diagramm mit Häufigkeiten 9.7 Farbschema", " Kapitel 9 Fallstudie zur Visualisierung Lernziele: Diagramme für nominale Variablen erstellen können. Balkendiagramme mit Prozentpunkten auf der Y-Achse erstellen können. Balkendiagramme drehen können. Text-Labels an Balkendiagramme anfügen können. Farbschemata von Balkendiagrammen ändern können. Benötigte Pakete: library(tidyverse) library(corrr) library(GGally) Eine recht häufige Art von Daten in der Wirtschaft kommen von Umfragen in der Belegschaft. Diese Daten gilt es dann aufzubereiten und graphisch wiederzugeben. Das ist der Gegenstand dieser Fallstudie. 9.1 Daten einlesen Hier laden wir einen Datensatz von einer Online-Umfrage: data &lt;- read.csv(&quot;https://osf.io/meyhp/?action=download&quot;) Der DOI für diesen Datensatz ist 10.17605/OSF.IO/4KGZH. Der Datensatz besteht aus 10 Extraversions-Items (B5T nach Satow43) sowie einigen Verhaltenskorrelaten (zumindest angenommenen). Uns interessieren also hier nur die 10 Extraversions-Items, die zusammen Extraversion als Persönlichkeitseigenschaft messen (sollen). Wir werden die Antworte der Befragten darstelle, aber uns hier keine Gedanken über Messqualität u.a. machen. Die Umfrage kann hier44 eingesehen werden. Schauen wir uns die Daten mal an: glimpse(data) #&gt; Observations: 501 #&gt; Variables: 28 #&gt; $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ... #&gt; $ timestamp &lt;fctr&gt; 11.03.2015 19:17:48, 11.03.2015 19:18:05, ... #&gt; $ code &lt;fctr&gt; HSC, ERB, ADP, KHB, PTG, ABL, ber, hph, IH... #&gt; $ i01 &lt;int&gt; 3, 2, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 3... #&gt; $ i02r &lt;int&gt; 3, 2, 4, 3, 3, 2, 4, 3, 4, 4, 3, 4, 3, 3, 3... #&gt; $ i03 &lt;int&gt; 3, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 1... #&gt; $ i04 &lt;int&gt; 3, 2, 4, 4, 4, 4, 3, 3, 4, 4, 3, 3, 2, 4, 3... #&gt; $ i05 &lt;int&gt; 4, 3, 4, 3, 4, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3... #&gt; $ i06r &lt;int&gt; 4, 2, 1, 3, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3... #&gt; $ i07 &lt;int&gt; 3, 2, 3, 3, 4, 4, 2, 3, 3, 3, 2, 4, 2, 3, 3... #&gt; $ i08 &lt;int&gt; 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 4... #&gt; $ i09 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 2, 4, 4, 4... #&gt; $ i10 &lt;int&gt; 1, 1, 1, 2, 4, 3, 2, 1, 2, 3, 1, 3, 2, 3, 2... #&gt; $ n_facebook_friends &lt;int&gt; 250, 106, 215, 200, 100, 376, 180, 432, 200... #&gt; $ n_hangover &lt;int&gt; 1, 0, 0, 15, 0, 1, 1, 2, 5, 0, 1, 2, 20, 2,... #&gt; $ age &lt;int&gt; 24, 35, 25, 39, 29, 33, 24, 28, 29, 38, 25,... #&gt; $ sex &lt;fctr&gt; Frau, Frau, Frau, Frau, Frau, Mann, Frau, ... #&gt; $ extra_single_item &lt;int&gt; 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4... #&gt; $ time_conversation &lt;dbl&gt; 10, 15, 15, 5, 5, 20, 2, 15, 10, 10, 1, 5, ... #&gt; $ presentation &lt;fctr&gt; nein, nein, nein, nein, nein, ja, ja, ja, ... #&gt; $ n_party &lt;int&gt; 20, 5, 3, 25, 4, 4, 3, 6, 12, 5, 10, 5, 10,... #&gt; $ clients &lt;fctr&gt; , , , , , , , , , , , , , , , , , , , , , ... #&gt; $ extra_vignette &lt;fctr&gt; , , , , , , , , , , , , , , , , , , , , , ... #&gt; $ extra_description &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... #&gt; $ prop_na_per_row &lt;dbl&gt; 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0... #&gt; $ extra_mean &lt;dbl&gt; 2.9, 2.1, 2.6, 2.9, 3.2, 2.8, 2.8, 2.5, 3.2... #&gt; $ extra_median &lt;dbl&gt; 3.0, 2.0, 3.0, 3.0, 3.5, 3.0, 3.0, 2.5, 3.5... #&gt; $ client_freq &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... 9.2 Daten umstellen Wir haben ein Diagramm vor Augen (s.u.), bei dem auf der X-Achse die Items stehen (1,2,…,n) und auf der Y-Achse die Anzahl der Kreuze nach Kategorien. Viele Grafik-Funktionen sind nun so aufgebaut, dass auf der X-Achsen nur eine Variable steht. ggplot2, das wir hier verwenden, ist da keine Ausnahme. Wir müssen also die “breite” Tabelle (10 Spalten, pro Item eine) in eine “lange Spalte” umbauen: Eine Spalte heißt dann “Itemnummer” und die zweite “Wert des Items” oder so ähnlich. Also, los geht’s: Zuerst wählen wir aus der Fülle der Daten, die Spalten, die uns interessieren: Die 10 Extraversions-Items, in diesem Fall. data_items &lt;- select(data, i01:i10) Dann stellen wir die Daten von “breit” nach “lang” um, so dass die Items eine Variable bilden und damit für ggplot2 gut zu verarbeiten sind. data_long &lt;- gather(data_items, key = items, value = Antwort) data_long$Antwort &lt;- factor(data_long$Antwort) Den Befehl mit factor brauchen wir für zum Diagramm erstellen im Folgenden. Dieser Befehl macht aus den Zahlen bei der Variable Antwort eine nominale Variable (in R: factor) mit Text-Werten “1”, “2” und so weiter. Wozu brauchen wir das? Der Digrammbefehl unten kann nur mit nominalen Variablen Gruppierungen durchführen. Wir werden in dem Diagramm die Anzahl der Antworten darstellen - die Anzahl der Antworten nach Antwort-Gruppe (Gruppe mit Antwort “1” etc.). Keine Sorge, wenn sich das reichlich ungewöhnlich anhört. Sie müssen es an dieser Stelle nicht erfinden :-) Man gewöhnt sich daran einerseits; und andererseits ist es vielleicht auch so, dass diese Funktionen nicht perfekt sind, oder nicht aus unserer Sicht oder nur aus Sicht des Menschen, der die Funktion geschrieben hat. Jedenfalls brauchen wir hier eine factor Variable zur Gruppierung… Damit haben wir es schon! Jetzt wird gemalt. 9.3 Diagramme für Anteile Wir nutzen ggplot2, wie gesagt, und davon die Funktion qplot (q wie quick, nehme ich an.). ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) Was macht dieser ggplot Befehl? Schauen wir es uns in Einzelnen an: ggplot(data = ...): Wir sagen “Ich möchte gern die Funktion ggplot nutzen, um den Datensatz … zu plotten”. aes(...): Hier definieren wir die “aesthetics” des Diagramms, d.h. alles “Sichtbare”. Wir ordnen in diesem Fall der X-Achse die Variable items zu. Per Standardeinstellung geht ggplot davon aus, dass sie die Häufigkeiten der X-Werte auf der Y-Achse haben wollen, wenn Sie nichts über die Y-Achse sagen. Jetzt haben wir ein Koordinatensystem definiert (das noch leer ist). geom_bar(): “Hey R oder ggplot, jetzt male mal einen barplot in den ansonsten noch leeren plot”. aes(fill = Antwort): Genauer gesagt nutzen wir aes um einen sichtbaren Aspekte des Diagramms (wie die X-Achse) eine Variable des Datensatzes zuzuordnen. Jetzt sagen wir, dass die Füllung (im Balkendiagramm) durch die Werte von Antwort definiert sein sollen (also “1”, “2” etc.). position = &quot;fill&quot; sagt, dass die Gesamt-Höhe des Balken aufgeteilt werden soll mit den “Teil-Höhen” der Gruppen (Antwort-Kategorien 1 bis 4); wir hätten die Teil-Höhen auch nebeneinander stellen können. Vielleicht ist es schöner, die NAs erst zu entfernen. data_long &lt;- na.omit(data_long) Und dann noch mal plotten: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) 9.4 Um 90° drehen Dazu nehmen wir + coord_flip(), also “flippe das Koordinatensystem”. ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) + coord_flip() 9.5 Text-Labels für die Items Wir definieren die Texte (“Labels”) für die Items: item_labels &lt;- c(&quot;Ich bin das erste Item&quot;, &quot;Das zweite Item&quot;, &quot;Item 3 sdjfkladsjk&quot;, &quot;Ich bin ein krasser Couch-Potato UMKODIERT&quot;, &quot;i5 asf&quot;, &quot;i6 sdf&quot;, &quot;adfjks&quot;, &quot;sfjlkd&quot;, &quot;sdfkjl&quot;, &quot;sdfjkl&quot;) Jetzt hängen wir die Labels an die Items im Diagramm: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) + coord_flip() + scale_x_discrete(labels = item_labels) Man kann auch einen Zeilenumbruch in den Item-Labels erzwingen… wobei das führt uns schon recht weit, aber gut, zum Abschluss :-) item_labels &lt;- c(&quot;Ich bin das erste Item&quot;, &quot;Das zweite Item&quot;, &quot;Item 3 sdjfkladsjk&quot;, &quot;Ich bin ein krasser \\nCouch-Potato***mit Zeilenumbruch***&quot;, &quot;i5 asf&quot;, &quot;i6 sdf&quot;, &quot;adfjks&quot;, &quot;sfjlkd&quot;, &quot;sdfkjl&quot;, &quot;sdfjkl&quot;) Und wieder plotten: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) + coord_flip() + scale_x_discrete(labels = item_labels, name = &quot;Extraversionsitems&quot;) + scale_y_continuous(name = &quot;Anteile&quot;) 9.6 Diagramm mit Häufigkeiten Ach so, schön wäre noch die echten Zahlen an der Y-Achse, nicht Anteile. Dafür müssen wir unseren Diagrammtyp ändern, bzw. die Art der Anordnung ändern. Mit position = &quot;fill&quot; wird der Anteil (also mit einer Summe von 100%) dargestellt. Wir können auch einfach die Zahlen/Häufigkeiten anzeigen, in dem wir die Kategorien “aufeinander stapeln” ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;stack&quot;) + coord_flip() + scale_x_discrete(labels = item_labels) 9.7 Farbschema Ja, die Wünsche hören nicht auf… Also, noch ein anderes Farbschema: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;stack&quot;) + coord_flip() + scale_x_discrete(labels = item_labels) + scale_fill_brewer(palette = 17) https://www.zpid.de/pub/tests/PT_9006357_B5T_Forschungsbericht.pdf↩ https://docs.google.com/forms/d/e/1FAIpQLSfD4wQuhDV_edx1WBfN3Qos7XqoVbe41VpiKLRKtGLeuUD09Q/viewform↩ "],
["ii-modellieren.html", "II MODELLIEREN", " II MODELLIEREN "],
["mod1.html", "Kapitel 10 Grundlagen des Modellierens 10.1 Was ist ein Modell? Was ist Modellieren? 10.2 Ein Besipiel zum Modellieren in der Datenanalyse 10.3 Taxonomie der Ziele des Modellierens 10.4 Die vier Schritte des statistischen Modellierens 10.5 Einfache vs. komplexe Modelle: Unter- vs. Überanpassung 10.6 Bias-Varianz-Abwägung 10.7 Training- vs. Test-Stichprobe 10.8 Wann welches Modell? 10.9 Modellgüte 10.10 Auswahl von Prädiktoren 10.11 Aufgaben 10.12 Befehlsübersicht 10.13 Verweise", " Kapitel 10 Grundlagen des Modellierens Lernziele: Erläutern können, was man unter einem Modell versteht. Die Ziele des Modellieren aufzählen und erläutern können. Die Vor- und Nachteile von einfachen vs. komplexen Modellen vergleichen können. Wissen, was man unter “Bias-Varianz-Abwägung” versteht. Um die Notwendigkeit von Trainings- und Test-Stichproben wissen. Wissen, was man unter Modellgüte versteht. Um die Schwierigkeiten der Prädiktorenauswahl wissen. In diesem Kapitel benötigen wir diese Pakete: library(tidyverse) 10.1 Was ist ein Modell? Was ist Modellieren? In diesem Kapitel geht es um Modelle und Modellieren; aber was ist das eigentlich? Seit dem 16. Jahrhundert wird mit dem italienischen Begriff modelle ein verkleinertes Muster, Abbild oder Vorbild für ein Handwerksstück benannt (Gigerenzer 1980). Prototisch für ein Modell ist - wer hätt’s gedacht - ein Modellauto (s. Abb. 10.1). Abbildung 10.1: Modell eines VW-Käfers In die Wissenschaft kam der Begriff in der Zeit nach Kant, als man sich klar wurde, dass (physikalische) Theorien nicht die Wirklichkeit als solche zeigen, sondern ein Modell davon. Modellieren ist eine grundlegenden Tätigkeit, derer sich Menschen fortlaufend bedienen, um die Welt zu verstehen. Denn das Leben ist schwer… oder sagen wir: komplex. Um einen Ausschnitt der Wirklichkeit zu verstehen, erscheint es daher sinnvoll, sich einige als wesentlich erachteten Aspekte “herauszugreifen” bzw. auszusuchen und sich nur noch deren Zusammenspiel näher anzuschauen. Modelle sind häufig vereinfachend: es wird nur ein Ausschnitt der Wirklichkeit berücksichtigt. Manche Aspekte der Wirklichkeit sind wirklicher als andere: Interessiert man sich für den Zusammenhang von Temperatur und Grundwasserspiegel, so sind diese Dinge direkt beobachtbar. Interessiert man sich hingegen für Lebensqualität und Zufriedenheit, so muss man diese Untersuchungsgegenstände erst konstruieren, da Lebensqualität nicht direkt beobachtbar ist. Sprechen wir daher von Wirklichkeit lieber vorsichtiger vom Gegenstandsbereich, also den konstruierten Auszug der Wirklichkeit für den sich die forschende Person interessiert. Bestenfalls (er)findet man eine Annäherung an die Wirklichkeit, schlechterenfalls eine verzerrte, gar grob falsche Darstellung (vgl. Abb. 10.2). Da keine Wiedergabe der Wirklichkeit perfekt ist, sind streng genommen alle Modelle “falsch” in diesem Sinne. Abbildung 10.2: Modellieren Damit verstehen wir Modellieren als eine typische Aktivität von Menschen (Gigerenzer 1980), genauer eines Menschen mit einem bestimmten Ziel. Wir können gar nicht anders, als uns ein Modell unserer Umwelt zu machen. Vielfältige Medien kommen dazu in Frage: Bilder, Geschichten, Logik, Gleichungen. Wir werden uns hier auf eine bestimmte Art formalisierter Modelle, numerische Modelle, konzentrieren, weil es dort am einfachsten ist, die Informationen auf präzise Art und Weise herauszuziehen. Allgemein gesprochen ist hier unter Modellieren der Vorgang gemeint, ein Stück Wirklichkeit (“Empirie”) in eine mathematische Struktur zu übersetzen. Wirklichkeit kann dabei als empirisches System bezeichnet werden, welches aus einer oder mehr Mengen von Objekten besteht, die zu einander in bestimmten Beziehungen stehen. Ein Bespiel wäre eine Reihe von Personen, die in bestimmten Größe-Relationen zueinander stehen oder eine Reihe von Menschen, bei denen die Füße tendenziell größer werden, je größer die Körpergröße ist. Mit mathematische Struktur ist ein formalisiertes Pendant zum empirischen System gemeint, daher spricht man von einem numerischen System. Auch hier gibt es eine Reihe von (mathematischen) Objekten wie Zahlen oder Vektoren. Diese mathematischen Objekten stehen ebenfalls in gewissen Relationen zueinander. Der springende Punkt ist: die Beziehungen zwischen den mathematischen Objekten sollen die Beziehungen zwischen den empirischen Objekten widerspiegeln. Was heißt das? Ist Anna (Person A) größer als Berta (Berson B), so soll die Zahl von Anna größer sein als die Zahl von Berta. Oder: Anna hat ein große Körpergröße (KG) und eine große Schuhgröße (SG); Berta ist kleiner, daher soll gelten \\(KG_A &gt; KG_B \\wedge SG_A &gt; SG_B\\). Sprich: Die Relation im empirischen System findet sich im numerischen System wieder. Abbildung 10.3: Formaleres Modell des Modollierens In gewisser Weise gibt es keine Modelle. Es gibt nur “Modelle von” etwas; dieser Satz soll zeigen, dass zwar ein empirisches System für sich alleine stehen kann, aber ein Modell nicht. Ein Modell verweist immer auf ein empirisches System. Abb. 10.3 stellt diese formalere Sichtweise des Modellierens dar; das empirische System E wird dem numerische System Z zugeordnet. Dabei besteht E aus einer Menge von Objekten O sowie einer Menge von Relationen R_E (Relationen zwischen den Elementen von O). Analog besteht Z aus einer Menge von numerischen Objekten Z sowie einer Menge von Relationen R_Z (Relationen zwischen den Elementen von Z)45. 10.2 Ein Besipiel zum Modellieren in der Datenanalyse Schauen wir uns ein Beispiel aus der Datenanalyse an; laden Sie dazu zuerst den Datensatz wo_men. Abbildung 10.4: Ein Beispiel für Modellieren Im ersten Plot von Abb. 10.4 sehen wir - schon übersetzt in eine Datenvisualisierung - den Gegenstandsbereich. Dort sind einige Objekte zusammen mit ihren Relationen abgebildet (Körpergröße und Schuhgröße). Der rechte Plot spezifiziert nun diesen Einfluss: Es wird ein linearer Zusammenhang (eine Gerade) zwischen Körpergröße und Schuhgröße unterstellt. Im nächsten Plot (Abb. 10.5) sehen wir ein Schema dazu, ein sog. Pfadmodell. Noch ist das Modell recht unspezifisch; es wird nur postuliert, dass Körpergröße auf Schuhgröße einen linearen Einfluss habe. Linear heißt hier, dass der Einfluss von Körpergröße auf Schuhgröße immer gleich groß ist, also unabhängig vom Wert der Körpergröße. Abbildung 10.5: Ein Beispiel für ein Pfadmodell Ein etwas aufwändigeres Modell könnte so aussehen (Abb. 10.6: Abbildung 10.6: Ein etwas aufwändigeres Modell Allgemeiner formuliert, haben wir einen oder mehrere Eingabegrößen bzw. Prädiktoren, von denen wir annehmen, dass sie einen Einfluss haben auf genau eine Zielgröße (Ausgabegröße) bzw. Kriterium. Einfluss ist hier nicht kausal gemeint, auch wenn es das Wort so vermuten lässt. Stattdessen ist nur ein statistischer Einfluss gemeint; letztlich nichts anderes als ein Zusammenhang. In diesem Sinne könnte man postulieren, dass die Größe des Autos, das man fährt einen “Einfluss” auf das Vermögen des Fahrers habe. Empirisch ist es gut möglich, dass man Belege für dieses Modell findet. Jedoch wird dieser Einfluss nicht kausal sein (man informiere mich, wenn es anders sein sollte). Modelle, wie wir sie betrachten werden, postulieren eine quantifizierbaren Zusammenhang zwischen diesen beiden Arten von Größen - Prädiktoren und Kriterien. Wir gehen dabei nicht davon aus, dass unsere Modelle perfekt sind, sondern dass Fehler passieren. Damit lassen sich unsere Modelle in drei Aspekte gliedern. Abbildung 10.7: Modelle mit schwarzer Kiste Die Einflussgrößen werden in einer “schwarzen Kiste”, die wir hier noch nicht näher benennen, irgendwie verwurstet, will sagen, verrechnet, so dass ein geschätzter Wert für das Kriterium, eine Vorhersage “hinten bei rauskommt”46. Mathematischer ausgedrückt: \\[Y = f(X) + \\epsilon\\] Hier stehen \\(Y\\) für das Kriterium, \\(X\\) für den oder die Prädiktoren, \\(f\\) für die “schwarze Kiste” und \\(\\epsilon\\) für den Fehler, den wir bei unserer Vorhersage begehen. Durch den Fehlerterm in der Gleichung ist das Modell nicht deterministisch, sondern beinhaltet erstens einen funktionalen Term (\\(Y=f(x)\\)) und zweitens einen stochastischen Term (\\(\\epsilon\\)). Die schwarze Kiste könnte man auch als eine “datengenerierende Maschine” bezeichnen. Übrigens: Auf das Skalenniveau der Eingabe- bzw. Ausgabegrößen (qualitativ vs. quantitativ) kommt es hier nicht grundsätzlich an; es gibt Modelle für verschiedene Skalenniveaus bzw. Modelle, die recht anspruchslos sind hinsichtlich des Skalenniveaus (sowohl für Eingabe- als auch Ausgabegrößen). Was die Ausgabegröße (das Kriterium) betrifft, so “fühlen” qualitative Variablen von quantitativen Variablen anders an. Ein Beispiel zur Verdeutlichung: “Gehört Herr Bussi-Ness zur Gruppe der Verweigerer oder der Wichtigmacher?” (qualitatives Kriterium); “Wie hoch ist der Wichtigmacher-Score von Herrn Bussi-Ness?” (quantitatives Kriterium). Ein Modell mit qualitativem Kriterium bezeichnet man auch als Klassifikation; ein Modell mit quantitativem Kriterium bezeichnet man auch als Regression. Bei letzterem Begriff ist zu beachten, dass er doppelt verwendet wird. Neben der gerade genannten Bedeutung steht er auch für ein häufig verwendetes Modell - eigentlich das prototypische Modell - für quantitative Kriterien. 10.3 Taxonomie der Ziele des Modellierens Modelle kann man auf vielerlei Arten gliedern; für unsere Zwecke ist folgende Taxonomie der Ziele von Modellieren nützlich. Geleitetes Modellieren Prädiktives Modellieren Explikaties Modellieren Ungeleitetes Modellieren Dimensionsreduzierendes Modellieren Fallreduzierendes Modellieren Betrachten wir diese vier Ziele des Modellierens genauer. Geleitetes Modellieren ist jede Art des Modellierens, wo die Variablen in Prädiktoren und Kriterien unterteilt werden, z.B. Abb. 10.5. Man könnte diese Modelle einfach dastellen als “X führt zu Y”. Prädiktives Modellieren könnte man kurz als Vorhersagen bezeichnen. Hier ist das Ziel, eine Black Box geschickt zu wählen, so dass der Vohersagefehler möglichst klein ist. Man zielt also darauf ab, möglichst exakte Vorhersagen zu treffen. Sicherlich wird der Vorhersagefehler kaum jemals Null sein; aber je präziser, desto besser. Das Innenleben der “schwarzen Kiste” interessiert uns hier nicht. Explikatives Modellieren oder kurz Erklären bedeutet, verstehen zu wollen, wie oder warum sich ein Kriteriumswert so verändert, wie er es tut. Auf welche Art werden die Prädiktoren verrechnet, so dass eine bestimmter Kriteriumswert resultiert? Welche Prädikatoren sind dabei (besonders) wichtig? Ist die Art der Verrechnung abhängig von den Werten der Prädiktoren? Hierbei interessiert uns vor allem die Beschaffenheit der schwarzen Kiste; die Güte der Vorhersage ist zweitrangig. Vorhersagen und Erklären haben gemein, dass Eingabegrößen genutzt werden, um Aussagen über einen Ausgabegröße zu treffen. Hat man einen Datensatz, so kann man prüfen, wie gut das Modell funktioniert, also wie genau man die Ausgabewerte vorhergesagt hat. Das ist also eine Art “Lernen mit Anleitung” oder angeleitetes Lernen oder geleitetes Modellieren (engl. supervised learning). Abbildung 10.7 gibt diesen Fall wieder. Beim ungeleiteten Modellieren entfällt die Unterteilung zwischen Prädiktor und Kriterium. Ungeleitetes Modelieren (Reduzieren) meint, dass man die Fülle des Datenmaterials verringert, in dem man ähnliche Dinge zusammenfasst (vgl. Abb. 10.8). Abbildung 10.8: Die zwei Arten des ungeleiteten Modellierens Fässt man Fälle zusammen, so spricht man von Fallreduzierendem Modellieren. Zum Beispiel könnte man spektakulärerweise “Britta”, “Carla” und “Dina” zu “Frau” und “Joachim”, “Alois” und “Casper” zu “Mann” zusammen fassen. Analog spricht man von Dimensionsreduzierendes Modellieren wenn Variablen zusammengefasst werden. Hat man z.B. einen Fragebogen zur Mitarbeiterzufriedenheit mit den Items “Mein Chef ist fair”, “Mein Chef ist kompetent”, “Meinem Chef ist meine Karriere wichtig”, so könnte man - wenn die Daten dies unterstützen - die Items zu einer Variable “Zufriedenheit mit Chef” zusammenfassen. Wenn also das Ziel des Modellieren lautet, die Daten zu reduzieren, also z.B. Kunden nach Persönlichkeit zu gruppieren, so ist die Lage anders als beim geleiteten Modellieren: Es gibt keine Zielgröße. Wir wissen nicht, was die “wahre Kundengruppe” von Herrn Casper Bussi-Ness ist. Wir sagen eher, “OK, die drei Typen sind sich irgendwie ähnlich, sie werden wohl zum selben Typen von Kunden gehören”. Wir tappen (noch mehr) in Dunkeln, was die “Wahrheit” ist. Unser Modell muss ohne Hinweise darauf, was richtig ist auskommen. Man spricht daher in diesem Fall von Lernen ohne Anleitung oder ungeleitetes Modellieren (engl. unsupervised learning). 10.4 Die vier Schritte des statistischen Modellierens Modellieren ist in der Datenanalyse bzw. in der Statistik eine zentrale Tätigkeit. Modelliert man in der Statistik, so führt man die zwei folgenden Schritte aus: Man wählt eines der vier Ziele des Modellierens (z.B. ein prädiktives Modell). Man wählt ein Modell aus (genauer: eine Modellfamilie), z.B. postuliert man, dass die Körpergröße einen linearen Einfluss auf die Schuhgröße habe. Man bestimmt die Details des Modells anhand der Daten: Wie groß ist die Steigung der Geraden und wo ist der Achsenabschnitt? Man sagt auch, dass man die Modellparameter anhand der Daten schätzt (“Modellinstantiierung”). Dann prüft man, wie gut das Modell zu den Daten passt; wie gut lässt sich die Schuhgröße anhand der Körpergröße vorhersagen bzw. wie groß ist der Vorhersagefehler? 10.5 Einfache vs. komplexe Modelle: Unter- vs. Überanpassung Je komplexer ein Modell, desto besser passt sie meistens auf den Gegenstandsbereich. Eine grobe, Holzschnitt artige Theorie ist doch schlechter als eine, die feine Nuancen berücksichtigt, oder nicht? Einiges spricht dafür; aber auch einiges dagegen. Schauen wir uns ein Problem mit komplexen Modellen an. Abbildung 10.9: Welches Modell passt am besten zu diesen Daten? Der 1. Plot (links) von Abb. 10.9 zeigt den Datensatz ohne Modell; der 2. Plot legt ein lineares Modell (rote Gerade) in die Daten. Der 3. Plot zeigt ein Modell, welches die Daten exakt erklärt - die (blaue) Linie geht durch alle Punkte. Der 4. Plot zeigt ein Modell (grüne Linie), welches die Punkte gut beschreibt, aber nicht exakt trifft. Welchem Modell würden Sie (am meisten) vertrauen? Das “blaue” Modell beschreibt die Daten sehr gut, aber hat das Modell überhaupt eine “Idee” vom Gegenstandsbereich, eine “Ahnung”, wie Y und X zusammenhängen, bzw. wie X einen Einfluss auf Y ausübt? Offenbar nicht. Das Modell ist “übergenau” oder zu komplex. Man spricht von Überanpassung (engl. overfitting). Das Modell scheint zufälliges, bedeutungsloses Rauschen zu ernst zu nehmen. Das Resultat ist eine zu wackelige Linie - ein schlechtes Modell, da wir wenig Anleitung haben, auf welche Y-Werte wir tippen müssten, wenn wir neue, unbekannte X-Werte bekämen. Was das “blaue Modell” zu detailverliebt ist, ist das “rote Modell” zu simpel. Die Gerade beschreibt die Y-Werte nur sehr schlecht. Man hätte gleich den Mittelwert von Y als Schätzwert für jedes einzelne \\(Y_i\\) hernehmen können. Dieses lineare Modell ist unterangepasst, könnte man sagen (engl. underfittting). Auch dieses Modell wird uns wenig helfen können, wenn es darum geht, zukünftige Y-Werte vorherzusagen (gegeben jeweils einen bestimmten X-Wert). Ah! Das grüne Modell scheint das Wesentliche, die “Essenz” der “Punktebewegung” zu erfassen. Nicht die Details, die kleinen Abweichungen, aber die “große Linie” scheint gut getroffen. Dieses Modell erscheint geeignet, zukünftige Werte gut zu beschreiben. Das grüne Modell ist damit ein Kompromiss aus Einfachheit und Komplexität und würde besonders passen, wenn es darum gehen sollte, zyklische Veränderungen zu erklären47. Je komplexer ein Modell ist, desto besser beschreibt es einen bekannten Datensatz (Trainings-Stichprobe). Allerdings ist das Modell, welches den Trainings-Datensatz am besten beschreibt, nicht zwangsläufig das Modell, welches neue, unbekannte Daten am besten beschreibt. Oft im Gegenteil! Je komplexer das Modell, desto kleiner der Fehler im Trainings-Datensatz. Allerdings: Die Fehler-Kurve im Test-Datensatz ist U-förmig: Mit steigender Komplexität wird der Fehler einige Zeit lang kleiner; ab einer gewissen Komplexität steigt der Fehler im Test-Datensatz wieder! 10.6 Bias-Varianz-Abwägung Einfache Modelle bilden (oft) verfehlen oft wesentliche Aspekte des Gegenstandsbereich; die Wirklichkeit ist häufig zu komplex für einfache Modelle. Die resultierende Verzerrung in den vorhergesagten Werten nennt man auch Bias. Mit anderen Worten: ist ein Modell zu einfach, passt es zu wenig zu den Daten (engl. underfitting). Auf der anderen Seite ist das Modell aber robust in dem Sinne, dass sich die vorhergesagten Werte kaum ändern, falls sich der Trainings-Datensatz etwas ändert. Ist das Modell aber zu reichhaltig (“komplex”), bildet es alle Details des Trainings-Datensatzes ab, wird es auch zufällige Variation des Datensatzes vorhersagen; Variation, die nicht relevant ist, der nichts Eigentliches abbildet. Das Modell ist “überangepasst” (engl. overfitting); geringfügige Änderungen im Datensatz können das Modell stark verändern. Das Model ist nicht robust. Auf der positiven Seite werden die Nuancen der Daten gut abgebildet; der Bias ist gering bzw. tendenziell geringer als bei einfachen Modellen. Einfache Modelle: Viel Bias, wenig Varianz. Komplexe Modelle: Wenig Bias, viel Varianz. Dieser Sachverhalt ist in folgendem Diagramm dargestellt (vgl. Abb. ??; basierend auf (Kuhn and Johnson 2013)). Der linke Plot zeigt ein komplexes Modell48; das Modell (blaue Linie) erscheint “zittrig”; kleine Änderungen in den Daten können große Auswirkungen auf das Modell (Verlauf der blauen Linie) haben. Darüber hinaus sind einige Details des Modells unplausibel: es gibt viele kleine “Hügel”, die nicht augenscheinlich plausibel sind. Der Plot auf der rechten Seiten hingegen ist sehr einfach und robust. Änderungen in den Daten werden vergleichsweise wenig Einfluss auf das Modell (die beiden roten Linien) haben. 10.7 Training- vs. Test-Stichprobe Wie wir gerade gesehen haben, kann man immer ein Modell finden, welches die vorhandenen Daten sehr gut beschreibt. Das gleicht der Tatsache, dass man im Nachhinein (also bei vorhandenen Daten) leicht eine Erklärung findet. Ob diese Erklärung sich in der Zukunft, bei unbekannten Daten bewahrheitet, steht auf einem ganz anderen Blatt. Daher sollte man immer sein Modell an einer Stichprobe entwickeln (“trainieren” oder “üben”) und an einer zweiten Stichprobe testen. Die erste Stichprobe nennt man auch training sample (Trainings-Stichprobe) und die zweite test sample (Test-Stichprobe). Entscheidend ist, dass das Test-Sample beim Entwickeln des Modells unbekannt war bzw. nicht verwendet wurde. Die Güte des Modells sollte nur anhand eines - bislang nicht verwendeten - Test-Samples überprüft werden. Das Test-Sample darf bis zur Modellüberprüfung nicht analysiert werden. Die Modellgüte ist im Trainings-Sample meist deutlich besser als im Test-Sample (vgl. die Fallstudie dazu: 12.9). train &lt;- wo_men %&gt;% sample_frac(.8, replace = FALSE) # Stichprobe von 80%, ohne Zurücklegen test &lt;- wo_men %&gt;% anti_join(train) # Alle Zeilen von &quot;wo_men&quot;, die nicht in &quot;train&quot; vorkommen Damit haben wir ein Trainings-Sample (train), in dem wir ein oder besser mehrere Modelle entwickeln können. So schön wie dieses Vorgehen auch ist, es ist nicht perfekt. Ein Nachteil ist, dass unsere Modellgüte wohl anders wäre, hätten wir andere Fälle im Test-Sample erwischt. Würden wir also ein neues Trainings-Sample und ein neues Test-Sample aus diesen Datensatz ziehen, so hätten wir wohl andere Ergebnisse. Was wenn diese Ergebnisse nun deutlich von den ersten abweichen? Dann würde unser Vertrauen in die die Modellgüte sinken. Wir bräuchten also noch ein Verfahren, welches Variabilität in der Modellgüte widerspiegelt. 10.8 Wann welches Modell? Tja, mit dieser Frage lässt sich ein Gutteil des Kopfzerbrechens in diesem Metier erfassen. Die einfache Antwort lautet: Es gibt kein “bestes Modell”, aber es mag für einen bestimmten Gegenstandsbereich, in einem bestimmten (historisch-kulturellen) Kontext, für ein bestimmtes Ziel und mit einer bestimmten Stichprobe ein best mögliches Modell geben. Dazu einige Eckpfeiler: Unter sonst gleichen Umständen sind einfachere Modelle den komplexeren vorzuziehen. Gott sei Dank. Je nach Ziel der Modellierung ist ein erklärendes Modell oder ein Modell mit reinem Vorhersage-Charakter vorzuziehen. Man sollte stets mehrere Modelle vergleichen, um abzuschätzen, welches Modell in der aktuellen Situation geeigneter ist. 10.9 Modellgüte Wie “gut” ist mein Modell? Modelle bewerten bzw. vergleichend bewerten ist einer der wichtigsten Aufgaben beim Modellieren. Die Frage der Modellgüte hat viele feine technisch-statistische Verästelungen, aber einige wesentlichen Aspekte kann man einfach zusammenfassen. Kriterium der theoretischen Plausibilität: Ein statistisches Modell sollte theoretisch plausibel sein. Anstelle “alles mit allem” durchzuprobieren, sollte man sich auf Modelle konzentrieren, die theoretisch plausibel sind. Die Modellwahl ist theoretisch zu begründen. Kriterium der guten Vorhersage: Die Vorhersagen eines Modells sollen präzise und überraschend sein. Dass ein Modell die Wirklichkeit präzise vorhersagen soll, liegt auf der Hand. Hier verdient nur der Term vorhersagen Beachtung. Es ist einfach, im Nachhinein Fakten (Daten) zu erklären. Jede Nachbesprechung eines Bundesliga-Spiels liefert reichlich Gelegenheit, posthoc Erklärungen zu hören. Schwieriger sind Vorhersagen49. Die Modellgüte ist also idealerweise an in der Zukunft liegende Ereignisse bzw. deren Vorhersage zu messen. Zur Not kann man auch schon in der Vergangenheit angefallene Daten hernehmen. Dann müssen diese Daten aber für das Modell neu sein. Was ist mit überraschend gemeint? Eine Vorhersage, dass die Temperatur morgen in Nürnberg zwischen -30 und +40°C liegen wird, ist sicherlich sehr treffend, aber nicht unbedingt präzise und nicht wirklich überraschend. Die Vorhersage, dass der nächste Chef der Maurer-Innung (wenn es diese geben sollte) ein Mann sein wird, und nicht eine Frau, kann zwar präzise sein, ist aber nicht überraschend. Wir werden also in dem Maße unseren Hut vor dem Modell ziehen, wenn die Vorhersagen sowohl präzise als auch überraschen sind. Dazu später mehr Details. Kriterium der Situationsangemessenheit: Die Güte des Modells ist auf die konkrete Situation abzustellen. Ein Klassifikationsmodell muss anders beurteilt werden als ein Regressionsmodell. Reduktionsmodelle müssen wiederum anders beurteilt werden. In den entsprechenden Kapiteln werden diese Unterschiede präzisiert. 10.10 Auswahl von Prädiktoren Wie oben diskutiert, stellen wir ein (geleitetes) Modell gerne als ein Pfaddiagramm des Typs \\(X \\rightarrow Y\\) dar (wobei X ein Vektor sein kann). Nehmen wir an das Kriterium \\(Y\\) als gesetzt an; bleibt die Frage: Welche Prädiktoren (\\(X\\)) wählen wir, um das Kriterium möglichst gut vorherzusagen? Eine einfache Frage. Keine leichte Antwort. Es gibt zumindest drei Möglichkeiten, die Prädiktoren zu bestimmen: theoriegeleitet, datengetrieben oder auf gut Glück. theoriegeleitet: Eine starke Theorie macht präzise Aussagen, welche Faktoren eine Rolle spielen und welche nicht. Auf dieser Basis wählen wir die Prädiktoren. Diese Situation ist wünschenswert; nicht nur, weil Sie Ihnen das Leben leicht macht, sondern weil es nicht die Gefahr gibt, die Daten zu “overfitten”, “Rauschen als Muster” zu bewerten - kurz: zu optimistisch bei der Interpretation von Statistiken zu sein. datengetrieben: Kurz gesagt werden die Prädiktoren ausgewählt, welche das Kriterium am besten vorhersagen. Das ist einerseits stimmig, andererseits birgt es die Gefahr, dass Zufälligkeiten in den Daten für echte Strukturen, die sich auch in zukünftigen Stichproben finden würden, missverstanden werden. auf gut Glück: tja, kann man keine Theorie zu Rate ziehen und sind die Daten wenig aussagekräftig oder man nicht willens ist, sie nicht genug zu quälen analysieren, so neigen Menschen dazu, zuerst sich selbst und dann andere von der Plausibilität der Entscheidung zu überzeugen. Keine sehr gute Strategie. In späteren Kapiteln betrachten wir Wege, um Prädiktoren für bestimmte Modelle auszuwählen. 10.11 Aufgaben Erfolg beim Online-Dating Lesen Sie diesen50 Artikel (Sauer and Wolff 2016). Zeichnen Sie ein Pfaddiagramm zum Modell!51. Ziele des Modellierens Welche drei Ziele des Modellierens kann man unterscheiden?52 Bias-Varianz-Abwägung Betrachten Sie Abb. 10.10. Welches der beiden Modelle (visualiert im jeweiligen Plot) ist wahrscheinlich… mehr bzw. weniger robust gegenüber Änderungen im Datensatz? mehr oder weniger präzise? Abbildung 10.10: Bias-Varianz-Abwägung. Links: Wenig Bias, viel Varianz. Rechts: Viel Bias, wenig Varianz. Richtig oder falsch?53 Richtig oder Falsch!? Die Aussage “Pro Kilo Schoki steigt der Hüftumfang um einen Zentimeter” kann als Beispiel für ein deterministisches Modell herhalten. Gruppiert man Kunden nach ähnlichen Kaufprofilen, so ist man insofern an “Reduzieren” der Datenmenge interessiert. Grundsätzlich gilt: Je komplexer ein Modell, desto besser. Mit “Bias” ist gemeint, dass ein Modell “zittrig” oder “wackelig” ist - sich also bei geringer Änderung der Stichprobendaten massiv in den Vorhersagen ändert. In der Gleichung \\(Y=f(x)+\\epsilon\\) steht \\(\\epsilon\\) für den Teil der Kriteriums, der nicht durch das Modell erklärt wird. 10.12 Befehlsübersicht Funktion Beschreibung dplyr::sample_frac Zielt eine Stichprobe von x% aus einem Dataframe dplyr::anti_join Behält alle Zeilen von df1, die *nicht in df2 vorkommen 10.13 Verweise Einige Ansatzpunkte zu moderner Statistik (“Data Science”) finden sich bei Peng und Matsui (2015). Chester Ismay erläutert einige Grundlagen von R und RStudio, die für Modellierung hilfreich sind: https://bookdown.org/chesterismay/rbasics/. Literaturverzeichnis "],
["der-p-wert.html", "Kapitel 11 Der p-Wert 11.1 Der p-Wert sagt nicht das, was viele denken 11.2 Der p-Wert ist eine Funktion der Stichprobengröße 11.3 Mythen zum p-Wert 11.4 Zur Philosophie des p-Werts 11.5 Fazit", " Kapitel 11 Der p-Wert Lernziele: Den p-Wert erläutern können. Den p-Wert kritisieren können. Abbildung 11.1: Der größte Statistiker des 20. Jahrhunderts (p &lt; .05) 11.1 Der p-Wert sagt nicht das, was viele denken Der p-Wert, entwickelt von Sir Ronald Fisher (Abb. 11.1), ist die heilige Kuh der Forschung. Das ist nicht normativ, sondern deskriptiv gemeint. Der p-Wert entscheidet (häufig) darüber, was publiziert wird, und damit, was als Wissenschaft sichtbar ist - und damit, was Wissenschaft ist (wiederum deskriptiv, nicht normativ gemeint). Kurz: Dem p-Wert wird viel Bedeutung zugemessen (vgl. Abb. 11.2). Abbildung 11.2: Der p-Wert wird oft als wichtig erachtet Was sagt uns der p-Wert? Eine gute intuitive Definition ist: Der p-Wert sagt, wie gut die Daten zur Nullhypothese passen. Je größer p, desto besser passen die Daten zur Nullhypothese. Allerdings hat der p-Wert seine Probleme. Vor allem: Er wird missverstanden. Jetzt kann man sagen, dass es dem p-Wert (dem armen) nicht anzulasten, dass andere/ einige ihm missverstehen. Auf der anderen Seite finde ich, dass sich Technologien dem Nutzer anpassen sollten (soweit als möglich) und nicht umgekehrt. Die (genaue) Definition des p-Werts ist aber auch so kompliziert, man kann sie leicht missverstehen: Der p-Wert gibt die Wahrscheinlichkeit P unserer Daten D an (und noch extremerer), unter der Annahme, dass die getestete Hypothese H wahr ist (und wenn wir den Versuch unendlich oft wiederholen würden, unter identischen Bedingungen und ansonsten zufällig). p = P(D|H) Viele Menschen - inkl. Professoren und Statistik-Dozenten - haben Probleme mit dieser Definition (Gigerenzer 2004). Das ist nicht deren Schuld: Die Definition ist kompliziert. Vielleicht denken viele, der p-Wert sage das, was tatsächlich interessant ist: die Wahrscheinlichkeit der (getesteten) Hypothese H, gegeben der Tatsache, dass bestimmte Daten D vorliegen. Leider ist das nicht die Definition des p-Werts. Also: \\[ P(D|H) \\ne P(H|D) \\] 11.1.1 Von Moslems und Terroristen Formeln haben die merkwürdige Angewohnheit vor dem inneren Auge zu verschwimmen; Bilder sind für viele Menschen klarer, scheint’s. Übersetzen wir die obige Formel in folgenden Satz: Wahrscheinlichkeit, Moslem zu sein, wenn man Terrorist ist UNGLEICH zur Wahrscheinlichkeit, Terrorist zu sein, wenn man Moslem ist. Oder kürzer: \\[ P(M|T) \\ne P(T|M) \\] Abbildung 11.3: Moslem und Terrorist zu sein, ist nicht das gleiche. Das Bild (Abb. 11.3) zeigt den Anteil der Moslems an den Terroristen (sehr hoch). Und es zeigt den Anteil der Terroristen von allen Moslems (sehr gering). Dabei können wir uns Anteil mit Wahrscheinlichkeit übersetzen. Kurz: Die beiden Anteile (Wahrscheinlichkeiten) sind nicht gleich. Man denkt leicht, der p-Wert sei die Wahrscheinlichkeit, Terrorist zu sein, wenn man Moslem ist. Das ist falsch. Der p-Wert ist die Wahrscheinlichkeit, Moslem zu sein, wenn man Terrorist ist. Ein großer Unterschied54. 11.2 Der p-Wert ist eine Funktion der Stichprobengröße Der p-Wert ist für weitere Dinge kritisiert worden (Wagenmakers 2007, Briggs (2016)); z.B. dass die “5%-Hürde” einen zu schwachen Test für die getestete Hypothese bedeutet. Letzterer Kritikpunkt ist aber nicht dem p-Wert anzulasten, denn dieses Kriterium ist beliebig, könnte konservativer gesetzt werden und jegliche mechanisierte Entscheidungsmethode kann ausgenutzt werden. Ähnliches kann man zum Thema “P-Hacking” argumentieren (Head et al. 2015, Wicherts et al. (2016)); andere statistische Verfahren können auch gehackt werden. Ein wichtiger Anklagepunkt lautet, dass der p-Wert nicht nur eine Funktion der Effektgröße ist, sondern auch der Stichprobengröße. Sprich: Bei großen Stichproben wird jede Hypothese signifikant. Damit verliert der p-Wert an Nützlichkeit (vgl. Abb. 11.4. Die Details der Simulation, die hinter Abb. 11.4 sind etwas umfangreicher und hier nicht so wichtig, daher nicht angegeben55. Abbildung 11.4: Zwei Haupteinflüsse auf den p-Wert Die Verteitigung argumentiert hier, dass das “kein Bug, sondern ein Feature” sei: Wenn man z.B. die Hypothese prüfe, dass der Gewichtsunteschied zwischen Männern und Frauen 0,000000000kg sei und man findet 0,000000123kg Unterschied, ist die getestete Hypothese falsch. Punkt. Der p-Wert gibt demnach das korrekte Ergebnis. Meiner Ansicht nach ist die Antwort zwar richtig, geht aber an den Anforderungen der Praxis vorbei. 11.2.1 Vertiefung: Praktisches Beispiel zum Stichprobeneinfluss auf den p-Wert Betrachten wir ein praktisches Beispiel des Einfluss der Stichprobengröße auf den p-Wert. Simulieren wir ein paar Variablen und testen wir, ob sich deren Mittelwerte statistisch signifikant unterscheiden (t-Test). Dabei wählen wir die Werte so, dass sie tatsächlich leicht unterschiedlich sind, dass also die H0 (in unseren Daten) wirklich falsch ist. Mit steigender Stichprobengröße sollte der Anteil an statistisch signifikanten Tests steigen. Schauen wir, ob dem so ist (vgl. Abb. 11.5). Abbildung 11.5: Der Anteil an statistisch signifikanten p-Werten bei simulierten Daten. Die X-Achse zeigt die Stichprobengröße (ns), die Y-Achse den Anteil der statistisch signifikanten p-Werte (ps) Das Diagramm zeigt: Mit steigendem Stichprobenumfang werden die Tests immer signifikanter. Zugespitzt formuliert: Große Stichprobe: Test wird signifikant. Kleine Stichprobe: Test wird nicht signifikant. “Groß” bzw. “klein” heißt hier “groß/klein genug”. 11.3 Mythen zum p-Wert Falsche Lehrmeinungen sterben erst aus, wenn die beteiligten Professoren in Rente gehen, heißt es. Jedenfalls halten sich eine Reihe von Mythen hartnäckig; sie sind alle falsch. Wenn der p-Wert kleiner als 5% ist, dann ist meine Hypothese (H1) sicher richtig. Richtig ist: “Wenn der p-Wert kleines ist als 5% (oder allgemeiner: kleiner als \\(\\alpha\\), dann sind die Daten (oder noch extereme) unwahrscheinlich, vorausgesetzt die H0 gilt”. Wenn der p-Wert kleiner als 5% ist, dann habe ich die Ursache eines Phänomens gefunden. Richtig ist: Keine Statistik kann für sich genommen eine Ursache erkennen. Bestenfalls kann man sagen: hat man alle konkurrierenden Ursachen ausgeschlossen und sprechen die Daten für die Ursache und sind die Daten eine plausible Erklärung, so erscheint es der beste Schluss, anzunehmen, dass man eine Ursache gefunden hat - im Rahmen des Geltungsbereichs einer Studie. Wenn der p-Wert kleiner als 5% ist, dann kann ich meine Studie veröffentlichen. Richtig. Leider entscheidet zu oft (nur) der p-Wert über das Wohl und Wehe einer Studie. Wichtiger wäre zu prüfen, wie “gut” das Modell ist - wie präzise sind die Vorhersagen? Wie theoretisch befriedigend ist das Modell? 11.4 Zur Philosophie des p-Werts Der p-Wert basiert auf der Idee, dass man ein Experiment unendlich oft wiederholen könnte; und das unter zufälligen aber ansonsten komplett gleichen Bedingungen. Ob es im Universum irgendetwas gibt, das unendlich ist, ist umstritten (Rucker, n.d.). Jedenfalls ist die Vorstellung, das Experiment unendlich oft zu wiederholen, unrealistisch. Inwieweit Zufälligkeit und Vergleichbarkeit hergestellt werden kann, kann auch kritisiert werden (Briggs 2016). 11.5 Fazit Meine Meinung ist, dass der p-Wert ein problematisch ist (und ein Dinosaurier) und nicht oder weniger benutzt werden sollte (das ist eine normative Aussage). Da der p-Wert aber immer noch der Platzhirsch auf vielen Forschungsauen ist, führt kein Weg um ihn herum. Er muss genau verstanden werden: Was er sagt und - wichtiger noch - was er nicht sagt. Literaturverzeichnis "],
["iii-geleitetes-modellieren.html", "III GELEITETES MODELLIEREN", " III GELEITETES MODELLIEREN "],
["klassische-lineare-numerische-regression.html", "Kapitel 12 Klassische lineare (numerische) Regression 12.1 Einfache Regression 12.2 Überprüfung der Annahmen der linearen Regression 12.3 Regression mit kategorialen Prädiktoren 12.4 Multiple Regression 12.5 Inferenz in der linearen Regression 12.6 Modellgüte bei Regressionsmodellen 12.7 Vertiefungen zum Regressionmodell 12.8 Übung: Teaching Rating 12.9 Fallstudie zu Overfitting 12.10 Literatur", " Kapitel 12 Klassische lineare (numerische) Regression Lernziele: Wissen, was man unter Regression versteht. Die Annahmen der Regression überprüfen können. Regression mit kategorialen Prädiktoren durchführen können. Die Regression inferenzstatisisch absichern können. Die Modellgüte bei der Regression bestimmen können. Vertiefende Aspekte beherrschen, wie Modellwahl und Interaktionen. Benötigte Pakete: library(caret) # Modellieren library(tidyverse) # Datenjudo, Visualisierung,... library(gridExtra) # Mehrere Plots kombinieren 12.1 Einfache Regression Wir werden weiter den Datensatz tips analysieren (Bryant and Smith 1995). Sofern noch nicht geschehen, können Sie in hier als csv-Datei herunterladen: tips &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/tips.csv&quot;) Zur Unterstützung der Analyse wird (wieder) das Paket mosaic verwendet; außerdem laden wir ggplot2 für qplot: library(mosaic) library(ggplot2) Wie hängen Trinkgeldhöhe tip und Rechnungshöhe total_bill zusammen? Kann die Höhe des Trinkgeldes als lineare Funktion der Rechnungshöhe linear modelliert werden? \\[tip_i=\\beta_0+\\beta_1\\cdot total\\_bill_i+\\epsilon_i\\] Zunächst eine visuelle Analyse mi Hilfe eines Scatterplots. qplot(y = tip, x = total_bill, data = tips) Es scheint einen positiven Zusammenhang zu geben. Modellieren wir die abhängige Variable tip (inhaltliche Entscheidung!) als lineare Funktion der unabhängigen Variable total_bill: LinMod.1 &lt;- lm(tip ~ total_bill, data=tips) summary(LinMod.1) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.198 -0.565 -0.097 0.486 3.743 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.92027 0.15973 5.76 2.5e-08 *** #&gt; total_bill 0.10502 0.00736 14.26 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 242 degrees of freedom #&gt; Multiple R-squared: 0.457, Adjusted R-squared: 0.454 #&gt; F-statistic: 203 on 1 and 242 DF, p-value: &lt;2e-16 Der Achsenabschnitt (intercept) wird mit 0.92 geschätzt, die Steigung in Richtung total_bill mit 0.11: steigt total_bill um einen Dollar, steigt im Durchschnitt tip um 0.11. Die (Punkt-)Prognose für tip lautet also tip = 0.92 + 0.11 * total_bill Die Koeffzienten werden dabei so geschätzt, dass \\(\\sum \\epsilon_i^2\\) minimiert wird. Dies wird auch als Kleinste Quadrate (Ordinary Least Squares, OLS) Kriterium bezeichnet. Eine robuste Regression ist z. B. mit der Funktion rlm() aus dem Paket MASS möglich. In mosaic kann ein solches Modell einfach als neue Funktion definiert werden: LinMod.1Fun &lt;- makeFun(LinMod.1) Die (Punkt-)Prognose für die Trinkgeldhöhe, bspw. für eine Rechnung von 30$ kann dann berechnet werden LinMod.1Fun(total_bill=30) #&gt; 1 #&gt; 4.07 also 4.07$. In mosaic kann die Modellgerade über plotModel(LinMod.1) betrachtet werden. Das Bestimmtheitsmaß R² ist mit 0.46 “ok”: 46-% der Variation des Trinkgeldes wird im Modell erklärt. 12.2 Überprüfung der Annahmen der linearen Regression Aber wie sieht es mit den Annahmen aus? Die Linearität des Zusammenhangs haben wir zu Beginn mit Hilfe des Scatterplots “überprüft”. Zur Überprüfung der Normalverteilung der Residuen zeichnen wir ein Histogramm. Die Residuen können über den Befehl resid() aus einem Linearen Modell extrahiert werden. Hier scheint es zu passen: resid_df &lt;- data.frame(Residuen = resid(LinMod.1)) qplot(x = Residuen, data = resid_df) Konstante Varianz: Dies kann z. B. mit einem Scatterplot der Residuen auf der y-Achse und den angepassten Werten auf der x-Achse überprüft werden. Die angepassten (geschätzten) Werte werden über den Befehl fitted()[^3] extrahiert. Diese Annahme scheint verletzt zu sein (siehe unten): je größer die Prognose des Trinkgeldes, desto größer wirkt die Streuung der Residuen. Dieses Phänomen ließ sich schon aus dem ursprünglichen Scatterplot qplot(x = tip, y = total_bill, data=tips) erahnen. Das ist auch inhaltlich plausibel: je höher die Rechnung, desto höher die Varianz beim Trinkgeld. Die Verletzung dieser Annahme beeinflusst nicht die Schätzung der Steigung, sondern die Schätzung des Standardfehlers, also des p-Wertes des Hypothesentests, d. h., \\(H_0:\\beta_1=0\\). resid_df$fitted &lt;- fitted(LinMod.1) qplot(x = Residuen, y = fitted, data = resid_df) Extreme Ausreißer: Wie am Plot der Linearen Regression plotModel(LinMod.1) erkennbar, gibt es vereinzelt Ausreißer nach oben, allerdings ohne einen extremen Hebel. Hängt die Rechnungshöhe von der Anzahl der Personen ab? Bestimmt, aber wie? xyplot(total_bill ~ size, data=tips) Da bei diskreten metrischen Variablen (hier size) Punkte übereinander liegen können, sollte man “jittern” (“schütteln”), d. h., eine (kleine) Zufallszahl addieren: qplot(x = total_bill, y = size, data = tips, geom = &quot;jitter&quot;) Um wie viel Dollar steigt im Durchschnitt das Trinkgeld, wenn eine Person mehr am Tisch sitzt? Für wie aussagekräftig halten Sie Ihr Ergebnis aus 1.? 12.3 Regression mit kategorialen Prädiktoren Der Wochentag day ist eine kategoriale Variable. Wie sieht eine Regression des Trinkgeldes darauf aus? Zunächst grafisch: qplot(x = tip,y = day, data=tips) Und als Lineares Modell: LinMod.2 &lt;- lm(tip ~ day, data=tips) summary(LinMod.2) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ day, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.245 -0.993 -0.235 0.538 7.007 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.7347 0.3161 8.65 7.5e-16 *** #&gt; daySat 0.2584 0.3489 0.74 0.46 #&gt; daySun 0.5204 0.3534 1.47 0.14 #&gt; dayThur 0.0367 0.3613 0.10 0.92 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.38 on 240 degrees of freedom #&gt; Multiple R-squared: 0.0205, Adjusted R-squared: 0.00823 #&gt; F-statistic: 1.67 on 3 and 240 DF, p-value: 0.174 Die im Modell angegebenen Schätzwerte sind die Änderung der Trinkgeldprognose, wenn z. B. der Tag ein Samstag (daySat) im Vergleich zu einer Referenzkategorie. Dies ist in R das erste Element des Vektors der Faktorlevel. Welcher dies ist ist über den Befehl levels() zu erfahren levels(tips$day) #&gt; [1] &quot;Fri&quot; &quot;Sat&quot; &quot;Sun&quot; &quot;Thur&quot; hier also Fri (aufgrund der standardmäßig aufsteigenden alphanumerischen Sortierung). Dies kann über relevel() geändert werden. Soll z. B. die Referenz der Donnerstag, Thur sein: tips$day &lt;- relevel(tips$day, ref = &quot;Thur&quot;) levels(tips$day) #&gt; [1] &quot;Thur&quot; &quot;Fri&quot; &quot;Sat&quot; &quot;Sun&quot; Das Modell ändert sich entsprechend: LinMod.3 &lt;- lm(tip ~ day, data=tips) summary(LinMod.3) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ day, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.245 -0.993 -0.235 0.538 7.007 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.7715 0.1750 15.84 &lt;2e-16 *** #&gt; dayFri -0.0367 0.3613 -0.10 0.919 #&gt; daySat 0.2217 0.2290 0.97 0.334 #&gt; daySun 0.4837 0.2358 2.05 0.041 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.38 on 240 degrees of freedom #&gt; Multiple R-squared: 0.0205, Adjusted R-squared: 0.00823 #&gt; F-statistic: 1.67 on 3 and 240 DF, p-value: 0.174 sowie als Plot: plotModel(LinMod.3) Eine Alternative zu relevel() zur Bestimmung der Referenzkategorie ist es, innerhalb von factor() die Option levels= direkt in der gewünschten Sortierung zu setzen. day &lt;- factor(tips$day, levels=c(&quot;Thur&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)) Die (Punkt-)Prognose für die Trinkgeldhöhe, bspw. an einen Freitag kann dann berechnet werden LinMod.3Fun &lt;- makeFun(LinMod.3) LinMod.3Fun(day=&quot;Fri&quot;) #&gt; 1 #&gt; 2.73 Wie verändert sich die Rechnungshöhe im Durchschnitt, wenn die Essenszeit Dinner statt Lunch ist? Wie viel % der Variation der Rechnungshöhe können Sie durch die Essenszeit modellieren? 12.4 Multiple Regression Aber wie wirken sich die Einflussgrößen zusammen auf das Trinkgeld aus? LinMod.4 &lt;- lm(tip ~ total_bill + size + sex + smoker + day + time, data=tips) summary(LinMod.4) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill + size + sex + smoker + day + time, #&gt; data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.848 -0.573 -0.103 0.476 4.108 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.6416 0.4976 1.29 0.199 #&gt; total_bill 0.0945 0.0096 9.84 &lt;2e-16 *** #&gt; size 0.1760 0.0895 1.97 0.051 . #&gt; sexMale -0.0324 0.1416 -0.23 0.819 #&gt; smokerYes -0.0864 0.1466 -0.59 0.556 #&gt; dayFri 0.1623 0.3934 0.41 0.680 #&gt; daySat 0.0408 0.4706 0.09 0.931 #&gt; daySun 0.1368 0.4717 0.29 0.772 #&gt; timeLunch 0.0681 0.4446 0.15 0.878 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 235 degrees of freedom #&gt; Multiple R-squared: 0.47, Adjusted R-squared: 0.452 #&gt; F-statistic: 26.1 on 8 and 235 DF, p-value: &lt;2e-16 Interessant sind die negativen Vorzeichen vor den Schätzwerten für sexMale und smokerYes – anscheinend geben Männer und Raucher weniger Trinkgeld, wenn alle anderen Faktoren konstant bleiben. Bei einer rein univariaten Betrachtung wäre etwas anderes herausgekommen. summary(lm(tip ~ sex, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ sex, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.090 -1.090 -0.090 0.667 6.910 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.833 0.148 19.14 &lt;2e-16 *** #&gt; sexMale 0.256 0.185 1.39 0.17 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.38 on 242 degrees of freedom #&gt; Multiple R-squared: 0.0079, Adjusted R-squared: 0.0038 #&gt; F-statistic: 1.93 on 1 and 242 DF, p-value: 0.166 summary(lm(tip ~ smoker, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ smoker, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.009 -0.994 -0.100 0.558 6.991 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.9919 0.1128 26.52 &lt;2e-16 *** #&gt; smokerYes 0.0169 0.1828 0.09 0.93 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.39 on 242 degrees of freedom #&gt; Multiple R-squared: 3.51e-05, Adjusted R-squared: -0.0041 #&gt; F-statistic: 0.00851 on 1 and 242 DF, p-value: 0.927 Diese Umkehrung des modellierten Effektes liegt daran, dass es auch einen positiven Zusammenhang zur Rechnungshöhe gibt: summary(lm(total_bill ~ sex, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = total_bill ~ sex, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.99 -6.02 -1.94 3.99 30.07 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 18.057 0.946 19.08 &lt;2e-16 *** #&gt; sexMale 2.687 1.180 2.28 0.024 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.83 on 242 degrees of freedom #&gt; Multiple R-squared: 0.021, Adjusted R-squared: 0.0169 #&gt; F-statistic: 5.19 on 1 and 242 DF, p-value: 0.0236 summary(lm(total_bill ~ smoker, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = total_bill ~ smoker, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -17.69 -6.46 -1.89 4.58 30.05 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.188 0.723 26.53 &lt;2e-16 *** #&gt; smokerYes 1.568 1.172 1.34 0.18 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.89 on 242 degrees of freedom #&gt; Multiple R-squared: 0.00735, Adjusted R-squared: 0.00325 #&gt; F-statistic: 1.79 on 1 and 242 DF, p-value: 0.182 Im vollem Modell LinMod.4 sind alle unabhängigen Variablen berücksichtigt, die Koeffizienten beziehen sich dann immer auf: gegeben, die anderen Variablen bleiben konstant, d. h. ceteris paribus. Vergleichen wir mal zwei Modelle: LinMod.5a &lt;- lm(tip ~ sex, data=tips) coef(LinMod.5a) # Koeffizienten extrahieren #&gt; (Intercept) sexMale #&gt; 2.833 0.256 LinMod.5b &lt;- lm(tip ~ sex + total_bill, data=tips) coef(LinMod.5b) # Koeffizienten extrahieren #&gt; (Intercept) sexMale total_bill #&gt; 0.9333 -0.0266 0.1052 Ohne die Berücksichtigung der Kovariable/Störvariable Rechnungshöhe geben Male ein um im Durchschnitt 0.26 höheres Trinkgeld, bei Kontrolle, d. h. gleicher Rechnungshöhe ein um 0.03 niedrigeres Trinkgeld als die Referenzklasse Female (levels(tips$sex)[1]). 12.5 Inferenz in der linearen Regression Kehren wir noch einmal zur multivariaten Regression (LinMod.4) zurück. summary(LinMod.4) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill + size + sex + smoker + day + time, #&gt; data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.848 -0.573 -0.103 0.476 4.108 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.6416 0.4976 1.29 0.199 #&gt; total_bill 0.0945 0.0096 9.84 &lt;2e-16 *** #&gt; size 0.1760 0.0895 1.97 0.051 . #&gt; sexMale -0.0324 0.1416 -0.23 0.819 #&gt; smokerYes -0.0864 0.1466 -0.59 0.556 #&gt; dayFri 0.1623 0.3934 0.41 0.680 #&gt; daySat 0.0408 0.4706 0.09 0.931 #&gt; daySun 0.1368 0.4717 0.29 0.772 #&gt; timeLunch 0.0681 0.4446 0.15 0.878 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 235 degrees of freedom #&gt; Multiple R-squared: 0.47, Adjusted R-squared: 0.452 #&gt; F-statistic: 26.1 on 8 and 235 DF, p-value: &lt;2e-16 In der 4. Spalte der, mit Zeilennamen versehenen Tabelle Coefficients stehen die p-Werte der Nullhypothese, die unabhängige Variable hat, gegeben alle anderen Variablen im Modell, keinen linearen Einfluss auf die abhängige Variable: \\(H_0: \\beta_i=0\\). Zur Bestimmung des p-Wertes wird der Schätzer (Estimate) durch den Standardfehler (Std. Error) dividiert. Der resultierende t-Wert (t value) wird dann, zusammen mit der Anzahl an Freiheitsgraden zur Berechnung des p-Wertes (Pr(&gt;|t|)) verwendet. Ein einfacher t-Test! Zur schnelleren Übersicht finden sich dahinter “Sternchen” und “Punkte”, die die entsprechenden Signifikanzniveaus symbolisieren: *** bedeutet eine Irrtumswahrscheinlichkeit, Wahrscheinlichkeit für Fehler 1. Art, von unter 0.001, d. h. unter 0,1%. ** entsprechend 1%, * 5% und . 10%. Zum Signifikanzniveau von 10% sind hier also zwei Faktoren und der Achsenabschnitt ((Intercept)) signifikant – nicht notwendigerweise relevant: Rechnungshöhe total_bill sowie Anzahl Personen size. Beides wirkt sich linear positiv auf die Trinkgeldhöhe aus: Mit jedem Dollar Rechnungshöhe steigt im Mittelwert die Trinkgeldhöhe um 0.09 Dollar, mit jeder Person um 0.18 Dollar – gegeben alle anderen Faktoren bleiben konstant. Das Bestimmtheitsmaß R² (Multiple R-squared:) liegt bei 0.47, also 47% der Variation des Trinkgeldes wird im Modell erklärt. Außerdem wird getestet, ob alle Koeffizienten der unabhängigen Variablen gleich Null sind: \\[H_0: \\beta_1=\\beta_2=\\cdots=\\beta_k=0\\] Das Ergebnis des zugrundeliegenden F-Tests (vgl. Varianzanalyse) wird in der letzten Zeile angegeben (F-Statistic). Hier wird \\(H_0\\) also verworfen. 12.6 Modellgüte bei Regressionsmodellen In einem Regressionsmodell lautet die grundlegenden Überlegung zur Modellgüte so: Wie groß ist der Unterschied zwischen Vorhersage und Wirklichkeit? Die Größe des Unterschieds (Differenz, “Delta”) zwischen vorhergesagten (geschätzten) Wert und Wirklichkeit, bezeichnet man als Fehler, Residuum oder Vohersagefehler, häufig mit \\(\\epsilon\\) (griechisch e wie “error”) abgekürzt. Graphisch kann man das gut veranschaulichen: Betrachten Sie die beiden Plots. Die rote Linie gibt die vorhergesagten (geschätzten) Werte wieder; die Punkte die beobachteten (“echten”) Werte. Je länger die blauen Linien, desto größer die Vorhersagefehler. Je kürzer die typische “Abweichungslinie”, desto besser die Vohersage. Sagt mein Modell voraus, dass Ihre Schuhgröße 49 ist, aber in Wahrheit liegt sie bei 39, so werden Sie dieses Modell als schlecht beurteilen. Leider ist es nicht immer einfach zu sagen, wie groß der Fehler sein muss, damit das Modell als “schlecht” gilt. Man kann argumentieren, dass es keine wissenschaftliche Frage sei, wie viel “viel” oder “genug” ist (Briggs 2016). Das ist zwar plausibel, hilft aber nicht, wenn ich eine Entscheidung treffen muss. Stellen Sie sich vor: Ich zwinge Sie mit der Pistole auf der Brust, meine Schuhgröße zu schätzen. Eine einfache Lösung ist, das beste Modell unter mehreren Kandidaten zu wählen. Ein anderer Ansatz ist, die Vorhersage in Bezug zu einem Kriterium zu setzen. Dieses “andere Kriterium” könnte sein “einfach raten”. Oder, etwas intelligenter, Sie schätzen meine Schuhgröße auf einen Wert, der eine gewisse Plausibiliät hat, also z.B. die durchschnittliche Schuhgröße des deutschen Mannes. Auf dieser Basis kann man dann quantifizieren, ob und wieviel besser man als dieses Referenzkriterium ist. 12.6.1 Mittlere Quadratfehler Eine der häufigsten Gütekennzahlen ist der mittlere quadrierte Fehler (engl. “mean squared error”, MSE), wobei Fehler wieder als Differenz zwischen Vorhersage (pred) und beobachtete Wirklichkeit (obs, y) definiert ist. Dieser berechnet für jede Beobachtung den Fehler, quadriert diesen Fehler und bilden dann den Mittelwert dieser “Quadratfehler”, also einen mittleren Quadratfehler. Die englische Abkürzung MSE ist auch im Deutschen gebräuchlich. \\[ MSE = \\frac{1}{n} \\sum{(pred - obs)^2} \\] Konzeptionell ist dieses Maß an die Varianz angelehnt. Zieht man aus diesem Maß die Wurzel, so erhält man den sog. root mean square error (RMSE), welchen man sich als die Standardabweichung der Vorhesagefehler vorstellen kann. In Pseudo-R-Syntax: RMSE &lt;- sqrt(mean((df$pred - df$obs)^2)) Der RMSE hat die selben Einheiten wie die zu schätzende Variable, also z.B. Schuhgrößen-Nummern. Übrigens: Der RMSE hat eine Reihe von wünschenswerten statistischen Eigenschaften, über die wir uns hier ausschweigen 12.6.2 R-Quadrat (\\(R^2\\)) \\(R^2\\), auch Bestimmtheitsmaß oder Determinationskoeffizient genannt, gibt die Vorhersagegüte im Verhältnis zu einem “Nullmodell” an. Das Nullmodell hier würde sagen, wenn es sprechen könnte: “Keine Ahnung, was ich schätzen soll, mich interessieren auch keine Prädiktoren, ich schätzen einfach immer den Mittelwert der Grundgesamtheit!”. Damit gibt \\(R^2\\) an, wie gut unsere Vorhersagen im Verhältnis zu den Vorhersagen des Nullmodells sind. Ein \\(R^2\\) von 25% (0.25) hieße, dass unser Vorhersagefehler 25% kleiner ist als der der Nullmodells. Ein \\(R^2\\) von 100% (1) heißt also, dass wir den kompletten Fehler reduziert haben (Null Fehler übrig) - eine perfekte Vorhersage. Etwas formaler, kann man \\(R^2\\) so definieren: \\[ R^2 = 1 - (\\frac{Nullmodellfehler - Vorhersagefehler}{Nullmodellfehler})\\] Präziser, in R-Syntax: R2 &lt;- 1 - sum((df$pred - df$obs)^2) / sum((mean(df$obs) - df$obs)^2) Praktischerweise gibt es einige R-Pakete, die diese Berechnung für uns besorgen: library(caret) postResample(obs = obs, pred = pred) Hier stehtobs für beobachtete Werte und pred für die vorhergesagten Werte. Dieser Befehl gibt sowohl RMSE als auch \\(R^2\\) wieder. 12.6.3 Likelihood and Friends Der Likelihood \\(L\\) beantwortet folgende Frage: Angenommen, ein Modell M ist wahr. Wie wahrscheinlich ist es dann, die Daten D zu beobachten? Zum Beispiel: Eine faire Münze wird 10 Mal geworfen (Modell M: faire Münze). Wie wahrscheinlich ist es, 10 Mal Zahl zu werfen? Die Wahrscheinlichkeit hierfür liegt bei ca. 0.1%. Der Likelihood wäre also hier ~0.1%. Bei komplexen Modellen kann der Likelihood sehr klein werden. Damit haben Computer Probleme, weil z.B. nur eine begrenzte Anzahl von Dezimalen berücksichtigt werden. Werden zuviele Dezimalstellen gerundet, kann es das Ergebnis verfälschen. Daher wird der Likelihood häufig logarithmiert; man spricht dann vom log Likelihood. Der Logarithmus von einer positiven, sehr kleine Zahl ist eine negative Zahl mit großen Absolutwert. Man verwendet meist den natürlichen Logarithmus, wobei das eigentlich keine Rolle spielt. Manchmal dreht man noch das Vorzeichen um, damit der Log Likelihood wieder positiv ist. Gütekriterien wie AIC, BIC, CAIC oder die Devianz (engl. deviance) sind vom Likelihood abgeleitet. Meist wird noch berücksichtigt, wie komplex das Modell ist; komplexe Modelle tun sich leichter als einfachere Modelle, die Daten zu erklären. Aber sie könnten die Daten auch “überanpassen”. Um die mögliche Scheingenauigkeit komplexerer Modelle auszugleichen, wird der Likelihood vom AIC etc. mit einem Strafwert belegt, der proportional zur Komplexität des Modells ist (Zumel, Mount, and Porzak 2014). Man sollte in der Regel die Korrelation (r) nicht als Gütekriterium verwenden. Der Grund ist, dass die Korrelation sich nicht verändert, wenn man die Variablen skaliert. Die Korrelation zieht allein auf das Muster der Zusammenhänge - nicht die Größe der Abstände - ab. In der Regel ist die Größe der Abstände zwischen beobachteten und vorhergesagten Werten das, was uns interessiert. 12.7 Vertiefungen zum Regressionmodell 12.7.1 Modellwahl Das Modell mit allen Variablen des Datensatzes, d. h., mit 6 unabhängigen (LinMod.4) erklärt 47.01% der Variation, das Modell nur mit der Rechnungshöhe als erklärende Variable (LinMod.1) schon 45.66%, der Erklärungszuwachs liegt also gerade einmal bei 1.35 Prozentpunkten. In der Statistik ist die Wahl des richtigen Modells eine der größten Herausforderungen, auch deshalb, weil das wahre Modell in der Regel nicht bekannt ist und es schwer ist, die richtige Balance zwischen Einfachheit und Komplexität zu finden. Aufgrund des Zufalls kann es immer passieren, dass das Modell sich zu sehr an die zufälligen Daten anpasst (Stichwort: Overfitting). Es gibt unzählige Modellwahlmethoden, und leider garantiert keine, dass immer das beste Modell gefunden wird. Eine Möglichkeit ist die sogenannte Schrittweise-Rückwärtsselektion auf Basis des Akaike-Informationskriteriums (AIC)56. Diese ist nicht nur recht weit verbreitet - und liefert unter bestimmten Annahmen das “richtige” Modell - sondern in R durch den Befehl step() einfach umsetzbar: step(LinMod.4) #&gt; Start: AIC=20.5 #&gt; tip ~ total_bill + size + sex + smoker + day + time #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - day 3 0.6 247 15.1 #&gt; - time 1 0.0 247 18.5 #&gt; - sex 1 0.1 247 18.6 #&gt; - smoker 1 0.4 247 18.9 #&gt; &lt;none&gt; 247 20.5 #&gt; - size 1 4.1 251 22.5 #&gt; - total_bill 1 101.6 348 102.7 #&gt; #&gt; Step: AIC=15.1 #&gt; tip ~ total_bill + size + sex + smoker + time #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - time 1 0.0 247 13.1 #&gt; - sex 1 0.0 247 13.2 #&gt; - smoker 1 0.4 248 13.5 #&gt; &lt;none&gt; 247 15.1 #&gt; - size 1 4.3 251 17.4 #&gt; - total_bill 1 101.7 349 97.2 #&gt; #&gt; Step: AIC=13.1 #&gt; tip ~ total_bill + size + sex + smoker #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - sex 1 0.0 247 11.2 #&gt; - smoker 1 0.4 248 11.5 #&gt; &lt;none&gt; 247 13.1 #&gt; - size 1 4.3 251 15.4 #&gt; - total_bill 1 103.3 350 96.3 #&gt; #&gt; Step: AIC=11.2 #&gt; tip ~ total_bill + size + smoker #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - smoker 1 0.4 248 9.5 #&gt; &lt;none&gt; 247 11.2 #&gt; - size 1 4.3 252 13.4 #&gt; - total_bill 1 104.3 351 95.0 #&gt; #&gt; Step: AIC=9.53 #&gt; tip ~ total_bill + size #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 248 9.5 #&gt; - size 1 5.2 253 12.6 #&gt; - total_bill 1 106.3 354 94.7 #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill + size, data = tips) #&gt; #&gt; Coefficients: #&gt; (Intercept) total_bill size #&gt; 0.6689 0.0927 0.1926 In den letzten Zeilen der Ausgabe steht das beste Modell, das diese Methode (schrittweise, rückwärts) mit diesem Kriterium (AIC) bei diesen Daten findet (Punktprognose, d. h. ohne Residuum): tip = 0.66894 + 0.09271 * total_bill + 0.19260 * size Der Ausgabe können Sie auch entnehmen, welche Variablen in welcher Reihenfolge entfernt wurden: Zunächst day, dann time, danach sex und schließlich smoker. Hier sind also dieselben Variablen noch im Modell, die auch in LinMod.4 signifikant zum Niveau 10% waren, eine Auswahl der dort signifikanten Variablen hätte also dasselbe Modell ergeben. Das ist häufig so, aber nicht immer! 12.7.2 Interaktionen Wir haben gesehen, dass es einen Zusammenhang zwischen der Trinkgeldhöhe und der Rechnungshöhe gibt. Vielleicht unterscheidet sich der Zusammenhang je nachdem, ob geraucht wurde, d. h., vielleicht gibt es eine Interaktion (Wechselwirkung). Die kann in lm einfach durch ein * zwischen den unabhängigen Variablen modelliert werden: LinMod.6 &lt;- lm(tip ~ smoker*total_bill, data = tips) summary(LinMod.6) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ smoker * total_bill, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.679 -0.524 -0.120 0.475 4.900 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.36007 0.20206 1.78 0.07601 . #&gt; smokerYes 1.20420 0.31226 3.86 0.00015 *** #&gt; total_bill 0.13716 0.00968 14.17 &lt; 2e-16 *** #&gt; smokerYes:total_bill -0.06757 0.01419 -4.76 3.3e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.979 on 240 degrees of freedom #&gt; Multiple R-squared: 0.506, Adjusted R-squared: 0.5 #&gt; F-statistic: 81.9 on 3 and 240 DF, p-value: &lt;2e-16 Der Schätzwert für die Interaktion steht bei :. Hier also: Wenn geraucht wurde, ist die Steigung im Durchschnitt um 6,8 Cent geringer. Aber wenn geraucht wurde, ist die Rechnung im Achsenabschnitt erstmal um 1,20$ höher (Effekt, ceteris paribus). Wer will, kann ausrechnen, ab welcher Rechnungshöhe Rauchertische im Mittelwert lukrativer sind… Das gleiche Bild (höhere Achsenabschnitt, geringere Steigung) ergibt sich übrigens bei getrennten Regressionen: lm(tip~total_bill, data=tips, subset = smoker==&quot;Yes&quot;) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill, data = tips, subset = smoker == #&gt; &quot;Yes&quot;) #&gt; #&gt; Coefficients: #&gt; (Intercept) total_bill #&gt; 1.5643 0.0696 lm(tip~total_bill, data=tips, subset = smoker==&quot;No&quot;) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill, data = tips, subset = smoker == #&gt; &quot;No&quot;) #&gt; #&gt; Coefficients: #&gt; (Intercept) total_bill #&gt; 0.360 0.137 12.7.3 Weitere Modellierungsmöglichkeiten Über das Formelinterface y~x können auch direkt z. B. Polynome modelliert werden. Hier eine quadratische Funktion: summary(lm(tip~I(total_bill^2)+total_bill, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ I(total_bill^2) + total_bill, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.200 -0.559 -0.098 0.484 3.776 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.91e-01 3.47e-01 2.57 0.01078 * #&gt; I(total_bill^2) -5.71e-05 6.02e-04 -0.09 0.92457 #&gt; total_bill 1.08e-01 3.08e-02 3.51 0.00054 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 241 degrees of freedom #&gt; Multiple R-squared: 0.457, Adjusted R-squared: 0.452 #&gt; F-statistic: 101 on 2 and 241 DF, p-value: &lt;2e-16 D. h., die geschätzte Funktion ist eine “umgedrehte Parabel” (negatives Vorzeichen bei I(total_bill^2)), bzw. die Funktion ist konkav, die Steigung nimmt ab. Allerdings ist der Effekt nicht signifikant. Hinweis: Um zu “rechnen” und nicht beispielsweise Interaktion zu modellieren, geben Sie die Variablen in der Formel in der Funktion I() (As Is) ein. 12.7.4 Prognoseintervalle Insgesamt haben wir viel “Unsicherheit” u. a. aufgrund von Variabilität in den Beobachtungen und in den Schätzungen. Wie wirken sich diese auf die Prognose aus? Dazu können wir über die Funktion predict.lm Prognoseintervalle berechnen – hier für das einfache Modell LinMod.1: newdat &lt;- data.frame(total_bill = seq(0, 75)) preddat &lt;- predict(LinMod.1, newdata = newdat, interval = &quot;prediction&quot;) head(preddat) #&gt; fit lwr upr #&gt; 1 0.92 -1.117 2.96 #&gt; 2 1.03 -1.010 3.06 #&gt; 3 1.13 -0.903 3.16 #&gt; 4 1.24 -0.797 3.27 #&gt; 5 1.34 -0.690 3.37 #&gt; 6 1.45 -0.583 3.47 tail(preddat) #&gt; fit lwr upr #&gt; 71 8.27 6.13 10.4 #&gt; 72 8.38 6.23 10.5 #&gt; 73 8.48 6.33 10.6 #&gt; 74 8.59 6.43 10.7 #&gt; 75 8.69 6.53 10.9 #&gt; 76 8.80 6.63 11.0 matplot(newdat$total_bill, preddat, lty = c(1,2,2), type=&quot;l&quot; ) points(x=tips$total_bill, y=tips$tip) Sie sehen, dass 95% Prognoseintervall ist recht breit: über den gewählten Rechnungsbereich von \\(0-75\\)$ im Mittelwert bei 4.11$. favstats((preddat[,3]-preddat[,2])) #&gt; min Q1 median Q3 max mean sd n missing #&gt; 4.03 4.04 4.07 4.17 4.34 4.12 0.0904 76 0 Zu den Rändern hin wird es breiter. Am schmalsten ist es übrigens beim Mittelwert der unabhängigen Beobachtungen, hier also bei 19.79$. 12.8 Übung: Teaching Rating Dieser Datensatz analysiert u. a. den Zusammenhang zwischen Schönheit und Evaluierungsergebnis von Dozenten (Hamermesh and Parker 2005). Sie können ihn, sofern noch nicht geschehen, von https://goo.gl/6Y3KoK als csv herunterladen. Versuchen Sie, das Evaluierungsergebnis als abhängige Variable anhand geeigneter Variablen des Datensatzes zu erklären. Wie groß ist der Einfluss der Schönheit? Sind die Modellannahmen erfüllt und wie beurteilen Sie die Modellgüte? 12.9 Fallstudie zu Overfitting Vergleichen wir im ersten Schritt eine Regression, die die Modellgüte anhand der Trainingsstichprobe schätzt mit einer Regression, bei der die Modellgüte in einer Test-Stichprobe überprüft wird. Zuerst führen wir dafür eine simple Regression aus und lassen uns \\(R^2\\) ausgeben. df &lt;- read_csv(&quot;https://sebastiansauer.github.io/data/wo_men.csv&quot;) lm1 &lt;- lm(shoe_size ~ height, data = df) summary(lm1)$r.squared #&gt; [1] 0.306 Im zweiten Schritt teilen wir die Stichprobe in eine Trainings- und eine Test-Stichprobe auf. Wir “trainineren” das Modell anhand der Daten aus der Trainings-Stichprobe: train &lt;- df %&gt;% sample_frac(.8, replace = FALSE) # Stichprobe von 80%, ohne Zurücklegen test &lt;- df %&gt;% anti_join(train) # Alle Zeilen von &quot;df&quot;, die nicht in &quot;train&quot; vorkommen lm2 &lt;- lm(shoe_size ~ height, data = train) Dann testen wir (die Modellgüte) anhand der Test-Stichprobe. Also los, lm2, mach Deine Vorhersage: lm2_predict &lt;- predict(lm2, newdata = test) Diese Syntax sagt: Speichere unter dem Namen “lm2_predict” das Ergebnis folgender Berechnung: Mache eine Vorhersage (“to predict”) anhand des Modells “lm2”, wobei frische Daten (“data = test”) verwendet werden sollen. Als Ergebnis bekommen wir einen Vektor, der für jede Beobachtung des Test-Samples den geschätzten (vorhergesagten) Trinkgeld-Wert speichert. caret::postResample(pred = lm2_predict, obs = test$shoe_size) #&gt; RMSE Rsquared #&gt; 10.540 0.634 Die Funktion postResample aus dem Paket caret liefert uns zentrale Gütekennzahlen unser Modell. Wir sehen, dass die Modellgüte im Test-Sample deutlich schlechter ist als im Trainings-Sample. Ein typischer Fall, der uns warnt, nicht vorschnell optimistisch zu sein! 12.10 Literatur David M. Diez, Christopher D. Barr, Mine Çetinkaya-Rundel (2014): Introductory Statistics with Randomization and Simulation, https://www.openintro.org/stat/textbook.php?stat_book=isrs, Kapitel 5, 6.1-6.3 Nicholas J. Horton, Randall Pruim, Daniel T. Kaplan (2015): Project MOSAIC Little Books A Student’s Guide to R, https://github.com/ProjectMOSAIC/LittleBooks/raw/master/StudentGuide/MOSAIC-StudentGuide.pdf, Kapitel 5.4, 10.2 Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013): An Introduction to Statistical Learning – with Applications in R, http://www-bcf.usc.edu/~gareth/ISL/, Kapitel 3 Maike Luhmann (2015): R für Einsteiger, Kapitel 16, 17.1-17.3 Andreas Quatember (2010): Statistik ohne Angst vor Formeln, Kapitel 3.11 Daniel Wollschläger (2014): Grundlagen der Datenanalyse mit R, Kapitel 6 Diese Übung basiert teilweise auf Übungen zum Buch OpenIntro von Andrew Bray und Mine Çetinkaya-Rundel unter der Lizenz Creative Commons Attribution-ShareAlike 3.0 Unported. Literaturverzeichnis "],
["klassifizierende-regression.html", "Kapitel 13 Klassifizierende Regression 13.1 Vorbereitung 13.2 Problemstellung 13.3 Die Idee der logistischen Regression 13.4 Welche Unterschiede zur linearen Regression gibt es in der Ausgabe? 13.5 Interpretation der Koeffizienten 13.6 Kategoriale Variablen 13.7 Multiple logistische Regression 13.8 Modell- bzw. Klassifikationsgüte 13.9 Vertiefung 13.10 Übung: Rot- oder Weißwein? 13.11 Literatur", " Kapitel 13 Klassifizierende Regression Lernziele: Die Idee der logistischen Regression verstehen. Die Koeffizienten der logistischen Regression interpretieren können. Vertiefungen wie Modellgüte kennen. 13.1 Vorbereitung Hier werden wir den Datensatz Aktienkauf der Universität Zürich (Universität Zürich, Methodenberatung) analysieren. Es handelt es sich hierbei um eine Befragung einer Bank im Zusammenhang mit den Fakten, die mit der Wahrscheinlichkeit, dass jemand Aktien erwirbt, zusammenhängen. Es wurden 700 Personen befragt. Folgende Daten wurden erhoben: Aktienkauf (0 = nein, 1 = ja), Jahreseinkommen (in Tausend CHF), Risikobereitschaft (Skala von 0 bis 25) und Interesse an der aktuellen Marktlage (Skala von 0 bis 45). Den Datensatz können Sie in so als csv-Datei herunterladen: Aktien &lt;- read.csv2(&quot;https://raw.githubusercontent.com/luebby/Datenanalyse-mit-R/master/Daten/Aktienkauf.csv&quot;) Zur Unterstützung der Analyse wird (wieder) mosaic und ggplot2 verwendet. library(mosaic) library(ggplot2) 13.2 Problemstellung Können wir anhand der Risikobereitschaft abschätzen, ob die Wahrscheinlichkeit für einen Aktienkauf steigt? Schauen wir uns zunächst ein Streudiagramm an: xyplot(Aktienkauf ~ Risikobereitschaft, data = Aktien) Der Zusammenhang scheint nicht sehr ausgeprägt zu sein. Lassen Sie uns dennoch ein lineare Regression durchführen und das Ergebnis auswerten und graphisch darstellen. lm1 &lt;- lm(Aktienkauf ~ Risikobereitschaft, data = Aktien) summary(lm1) #&gt; #&gt; Call: #&gt; lm(formula = Aktienkauf ~ Risikobereitschaft, data = Aktien) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.684 -0.243 -0.204 0.348 0.814 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.18246 0.02001 9.12 &lt; 2e-16 *** #&gt; Risikobereitschaft 0.05083 0.00762 6.67 5.2e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.427 on 698 degrees of freedom #&gt; Multiple R-squared: 0.0599, Adjusted R-squared: 0.0586 #&gt; F-statistic: 44.5 on 1 and 698 DF, p-value: 5.25e-11 plotModel(lm1) Der Schätzer für die Steigung für Risikobereitschaft ist signifikant. Das Bestimmtheitsmaß \\(R^2\\) ist allerdings sehr niedrig, aber wir haben bisher ja auch nur eine unabhängige Variable für die Erklärung der abhängigen Variable herangezogen. Doch was bedeutet es, dass die Wahrscheinlichkeit ab einer Risikobereitsschaft von ca. 16 über 1 liegt? Wahrscheinlichkeiten müssen zwischen 0 und 1 liegen. Daher brauchen wir eine Funktion, die das Ergebnis einer linearen Regression in einen Bereich von 0 bis 1 bringt, die sogenannte Linkfunktion. Eine häufig dafür verwendete Funktion ist die logistische Funktion: \\[p(y=1)=\\frac{e^\\eta}{1+e^\\eta}=\\frac{1}{1+e^{-\\eta}}\\] \\(\\eta\\), das sogenannte Logit, ist darin die Linearkombination der Einflussgrößen: \\[\\eta=\\beta_0+\\beta_1\\cdot x_1+\\dots\\] Exemplarisch können wir die logistische Funktion für einen Bereich von \\(\\eta=-10\\) bis \\(+10\\) darstellen (vgl. 13.1). Der Graph der logistischen Funktion ähnelt einem langgestreckten S (“Ogive” genannt). Abbildung 13.1: Die logistische Regression beschreibt eine ‘s-förmige’ Kurve 13.3 Die Idee der logistischen Regression Die logistische Regression ist eine Anwendung des allgemeinen linearen Modells (general linear model, GLM). Die Modellgleichung lautet: \\[p(y_i=1)=L\\bigl(\\beta_0+\\beta_1\\cdot x_{i1}+\\dots+\\beta_K\\cdot x_{ik}\\bigr)+\\epsilon_i\\] \\(L\\) ist die Linkfunktion, in unserer Anwendung die logistische Funktion. \\(x_{ik}\\) sind die beobachten Werte der unabhängigen Variablen \\(X_k\\). \\(k\\) sind die unabhängigen Variablen \\(1\\) bis \\(K\\). Die Funktion glm führt die logistische Regression durch. Wir schauen uns im Anschluss zunächst den Plot an. glm1 &lt;- glm(Aktienkauf ~ Risikobereitschaft, family = binomial(&quot;logit&quot;), data = Aktien) plotModel(glm1) Es werden ein Streudiagramm der beobachten Werte sowie die Regressionslinie ausgegeben. Wir können so z. B. ablesen, dass ab einer Risikobereitschaft von etwa 7 die Wahrscheinlichkeit für einen Aktienkauf nach unserem Modell bei mehr als 50 % liegt. Die Zusammenfassung des Modells zeigt folgendes: summary(glm1) #&gt; #&gt; Call: #&gt; glm(formula = Aktienkauf ~ Risikobereitschaft, family = binomial(&quot;logit&quot;), #&gt; data = Aktien) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.653 -0.738 -0.677 0.825 1.823 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.4689 0.1184 -12.4 &lt; 2e-16 *** #&gt; Risikobereitschaft 0.2573 0.0468 5.5 3.8e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 804.36 on 699 degrees of freedom #&gt; Residual deviance: 765.86 on 698 degrees of freedom #&gt; AIC: 769.9 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Der Achsenabschnitt (intercept) des logits \\(\\eta\\) wird mit -1.47 geschätzt, die Steigung in Richtung Risikobereitschaft mit 0.26. Die (Punkt-)Prognose für die Wahrscheinlickeit eines Aktienkaufs \\(p(y=1)\\) benötigt anders als in der linearen Regression noch die Linkfunktion und ergibt sich somit zu: \\[p(\\texttt{Aktienkauf}=1)=\\frac{1}{1+e^{-(-1.47 + 0.26 \\cdot \\texttt{Risikobereitschaft})}}\\] Die p-Werte der Koeffizienten können in der Spalte Pr(&gt;|z|) abgelesen werden. Hier wird ein Wald-Test durchgeführt, nicht wie bei der linearen Regression ein t-Test, ebenfalls mit der \\(H_0:\\beta_i=0\\). Die Teststastistik (z value) wird wie in der linearen Regression durch Divisions des Schätzers (Estimate) durch den Standardfehler (Std. Error) ermittelt. Im Wald-Test ist die Teststatistik allerdings \\(\\chi^2\\)-verteilt mit einem Freiheitsgrad. 13.4 Welche Unterschiede zur linearen Regression gibt es in der Ausgabe? Es gibt kein \\(R^2\\) im Sinne einer erklärten Streuung der \\(y\\)-Werte, da die beobachteten \\(y\\)-Werte nur \\(0\\) oder \\(1\\) annehmen können. Das Gütemaß bei der logistischen Regression ist das Akaike Information Criterion (AIC). Hier gilt allerdings: je kleiner, desto besser. (Anmerkung: es kann ein Pseudo-\\(R^2\\) berechnet werden – kommt später.) Es gibt keine F-Statistik (oder ANOVA) mit der Frage, ob das Modell als Ganzes signifikant ist. (Anmerkung: es kann aber ein vergleichbarer Test durchgeführt werden – kommt später.) 13.5 Interpretation der Koeffizienten 13.5.1 y-Achsenabschnitt (Intercept) \\(\\beta_0\\) Für \\(\\beta_0&gt;0\\) gilt, dass selbst wenn alle anderen unabhängigen Variablen \\(0\\) sind, es eine Wahrscheinlichkeit von mehr als 50% gibt, dass das modellierte Ereignis eintritt. Für \\(\\beta_0&lt;0\\) gilt entsprechend das Umgekehrte. 13.5.2 Steigung \\(\\beta_i\\) mit \\(i=1,2,...,K\\) Für \\(\\beta_i&gt;0\\) gilt, dass mit zunehmenden \\(x_i\\) die Wahrscheinlichkeit für das modellierte Ereignis steigt. Bei \\(\\beta_i&lt;0\\) nimmt die Wahrscheinlichkeit entsprechend ab. Eine Abschätzung der Änderung der Wahrscheinlichkeit (relatives Risiko, relative risk \\(RR\\)) kann über das Chancenverhältnis (Odds Ratio \\(OR\\)) gemacht werden.57 Es ergibt sich vereinfacht \\(e^{\\beta_i}\\). Die Wahrscheinlichkeit ändert sich näherungsweise um diesen Faktor, wenn sich \\(x_i\\) um eine Einheit erhöht. Hinweis: \\(RR\\approx OR\\) gilt nur, wenn der Anteil des modellierten Ereignisses in den beobachteten Daten sehr klein (\\(&lt;5\\%\\)) oder sehr groß ist (\\(&gt;95\\%\\)). Übung: Berechnen Sie das relative Risiko für unser Beispielmodell, wenn sich die Risikobereitschaft um 1 erhöht (Funktion exp()). Vergleichen Sie das Ergebnis mit der Punktprognose für Risikobereitschaft\\(=7\\) im Vergleich zu Risikobereitschaft\\(=8\\). Zur Erinnerung: Sie können makeFun(model) verwenden. # aus Koeffizient abgeschätzt exp(coef(glm1)[2]) #&gt; Risikobereitschaft #&gt; 1.29 In Worten: “Mit jedem Punkt mehr Risikobereitschaft steigen die Chancen (das OR) für Aktienkauf um 1.293”. # mit dem vollständigen Modell berechnet fun1 &lt;- makeFun(glm1) fun1(Risikobereitschaft = 1) #&gt; 1 #&gt; 0.229 fun1(Risikobereitschaft = 8) #&gt; 1 #&gt; 0.643 # als Faktor ausgeben fun1(Risikobereitschaft = 8)/fun1(Risikobereitschaft = 1) #&gt; 1 #&gt; 2.8 Bei einer Risikobereitschaft von 1 beträgt die Wahrscheinlichkeit für \\(y=1\\), d.h. für das Ereignis “Aktienkauf”, 0.23. Bei einer Risikobereitschaft von 8 liegt diese Wahrscheinlichkeit bei 0.64. Sie sehen also, die ungefähr abgeschätzte Änderung der Wahrscheinlichkeit weicht hier doch deutlich von der genau berechneten Änderung ab. Der Anteil der Datensätze mit Risikobereitschaft\\(=1\\) liegt allerdings auch bei 0.26. 13.6 Kategoriale Variablen Wie schon in der linearen Regression können auch in der logistschen Regression kategoriale Variablen als unabhängige Variablen genutzt werden. Als Beispiel nehmen wir den Datensatz tips und versuchen abzuschätzen, ob sich die Wahrscheinlichkeit dafür, dass ein Raucher bezahlt hat (smoker == yes), in Abhängigkeit vom Wochentag ändert. Sofern noch nicht geschehen, können Sie so als csv-Datei herunterladen: tips &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/tips.csv&quot;) Zunächst ein Plot: xyplot(jitter(as.numeric(smoker)) ~ day, data = tips) Hinweis: Um zu sehen, ob es an manchen Tagen mehr Raucher gibt, sollten Sie zumindest eine Variable “verrauschen” (“jittern”). Da die Variable smoker eine nominale Variable ist und die Funktion jitter() nur mit numerischen Variablen arbeitet, muss sie mit as.numeric() in eine numerische Variable umgewandelt werden. Die relativen Häufigkeiten zeigt folgende Tabelle: (tab_smoke &lt;- tally(smoker ~ day, data = tips, format = &quot;proportion&quot;)) #&gt; day #&gt; smoker Fri Sat Sun Thur #&gt; No 0.211 0.517 0.750 0.726 #&gt; Yes 0.789 0.483 0.250 0.274 Hinweis: Durch die Klammerung wird das Objekt tab_smoke direkt ausgegeben. Probieren wir die logistische Regression aus: glmtips &lt;- glm(smoker ~ day, family = binomial(&quot;logit&quot;),data = tips) summary(glmtips) #&gt; #&gt; Call: #&gt; glm(formula = smoker ~ day, family = binomial(&quot;logit&quot;), data = tips) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.765 -0.801 -0.758 1.207 1.665 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.322 0.563 2.35 0.01883 * #&gt; daySat -1.391 0.602 -2.31 0.02093 * #&gt; daySun -2.420 0.622 -3.89 1e-04 *** #&gt; dayThur -2.295 0.631 -3.64 0.00027 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 324.34 on 243 degrees of freedom #&gt; Residual deviance: 298.37 on 240 degrees of freedom #&gt; AIC: 306.4 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Auch hier können wir die Koeffizienten in Relation zur Referenzkategorie (hier: Freitag) interpretieren. Die Wahrscheinlichkeit ist an einem Samstag niedriger, der Wert für daySat ist negativ. Eine Abschätzung erhalten wir wieder mit \\(e^{\\beta_i}\\): exp(coef(glmtips)[2]) #&gt; daySat #&gt; 0.249 Daher ist das Chancenverhältnis (Odds Ratio), dass am Samstag ein Raucher am Tisch sitzt, näherungsweise um den Faktor 0.25 niedriger als am Freitag58: \\[{OR=\\frac{\\frac{P(Raucher|Samstag)}{1-P(Raucher|Samstag)}} {\\frac{P(Raucher|Freitag)}{1-P(Raucher|Freitag)}} =\\frac{\\frac{0.483}{0.517}} {\\frac{0.79}{0.21}} \\approx 0.249}\\] Die Wahrscheinlichkeit für einen Raucher am Samstag können wir uns wieder komfortabel so ausgeben lassen: fun2 &lt;- makeFun(glmtips) fun2(day = &quot;Sat&quot;) #&gt; 1 #&gt; 0.483 13.7 Multiple logistische Regression Wir kehren wieder zurück zu dem Datensatz Aktienkauf. Können wir unser Model glm1 mit nur einer erklärenden Variable verbessern, indem weitere unabhängige Variablen hinzugefügt werden? glm2 &lt;- glm(Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse, family = binomial(&quot;logit&quot;), data = Aktien) plotModel(glm2) summary(glm2) #&gt; #&gt; Call: #&gt; glm(formula = Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse, #&gt; family = binomial(&quot;logit&quot;), data = Aktien) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.130 -0.715 -0.539 0.518 3.214 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.66791 0.27903 -5.98 2.3e-09 *** #&gt; Risikobereitschaft 0.34781 0.08822 3.94 8.1e-05 *** #&gt; Einkommen -0.02157 0.00564 -3.83 0.00013 *** #&gt; Interesse 0.08520 0.01775 4.80 1.6e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 804.36 on 699 degrees of freedom #&gt; Residual deviance: 679.01 on 696 degrees of freedom #&gt; AIC: 687 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Alle Schätzer sind signifkant zum 0.1 %-Niveau (*** in der Ausgabe). Zunehmende Risikobereitschaft (der Einfluss ist im Vergleich zum einfachen Modell stärker geworden) und zunehmendes Interesse erhöhen die Wahrscheinlichkeit für einen Aktienkauf. Steigendes Einkommen hingegen senkt die Wahrscheinlichkeit. Ist das Modell besser als das einfache? Ja, da der AIC-Wert von 769.86 auf 687.01 gesunken ist. Die Graphik zeigt die Verläufe in Abhängigkeit von den verschiedenen Variablen und den Kombinationen der Variablen. 13.8 Modell- bzw. Klassifikationsgüte Logistische Regressionsmodelle werden häufig zur Klassifikation verwendet, z. B. ob der Kredit für einen Neukunden ein “guter” Kredit ist oder nicht. Daher sind die Klassifikationseigenschaften bei logistischen Modellen wichtige Kriterien. Hierzu werden die aus dem Modell ermittelten Wahrscheinlichkeiten ab einem Schwellenwert (cutpoint), häufig \\(0.5\\), einer geschätzten \\(1\\) zugeordnet, unterhalb des Schwellenwertes einer \\(0\\). Diese aus dem Modell ermittelten Häufigkeiten werden dann in einer sogenannten Konfusionsmatrix (confusion matrix) mit den beobachteten Häufigkeiten verglichen. Daher sind wichtige Kriterien eines Modells, wie gut diese Zuordnung erfolgt. Dazu werden die Sensitivität (True Positive Rate, TPR), also der Anteil der mit \\(1\\) geschätzten an allen mit \\(1\\) beobachten Werten, und die Spezifität (True Negative Rate) berechnet. Ziel ist es, dass beide Werte möglichst hoch sind. Sie können die Konfusionsmatrix “zu Fuß” berechnen, in dem Sie eine neue Variable einfügen, die ab dem cutpoint \\(1\\) und sonst \\(0\\) ist und mit dem Befehl tally() ausgeben. Alternativ können Sie das Paket SDMTools verwenden mit der Funktion confusion.matrix(). Ein Parameter ist cutpoint, der standardmäßig auf \\(0.5\\) steht. # Konfusionsmatrix &quot;zu Fuß&quot; berechnen # cutpoint = 0.5 setzen # neue Variable predicted anlegen mit 1, wenn modellierte Wahrscheinlichkeit &gt; 1 ist cutpoint = 0.5 Aktien$predicted &lt;- ((glm1$fitted.values) &gt; cutpoint)*1 # Kreuztabelle berechnen (cm &lt;- tally(~predicted+Aktienkauf, data = Aktien)) #&gt; Aktienkauf #&gt; predicted 0 1 #&gt; 0 509 163 #&gt; 1 8 20 # Sensitivität (TPR) cm[2,2]/sum(cm[,2]) #&gt; [1] 0.109 # Spezifität (TNR) cm[1,1]/sum(cm[,1]) #&gt; [1] 0.985 # mit Hilfe des Pakets SDMTools # ggf. install.packages(&quot;SDMTools&quot;) library(SDMTools) # optional noch Parameter cutpoint = 0.5 angeben (cm &lt;- confusion.matrix(Aktien$Aktienkauf, glm1$fitted.values)) #&gt; obs #&gt; pred 0 1 #&gt; 0 509 163 #&gt; 1 8 20 #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;confusion.matrix&quot; sensitivity(cm) #&gt; [1] 0.109 specificity(cm) #&gt; [1] 0.985 Wenn die Anteile der \\(1\\) in den beobachteten Daten sehr gering sind (z. B. bei einem medizinischem Test auf eine seltene Krankheit, Klicks auf einen Werbebanner oder Kreditausfall), kommt eine Schwäche der logistischen Regression zum Tragen: Das Modell wird so optimiert, dass die Wahrscheinlichkeiten \\(p(y=1)\\) alle unter \\(0.5\\) liegen. Das würde zu einer Sensitität von \\(0\\) und einer Spezifiät von \\(1\\) führen. Daher kann es sinnvoll sein, den Cutpoint zu varieren. Daraus ergibt sich ein verallgemeinertes Gütemaß, die ROC-Kurve (Return Operating Characteristic) und den daraus abgeleiteten AUC-Wert (Area Under Curve). Hierzu wird der Cutpoint zwischen 0 und 1 variiert und die Sensitivität gegen \\(1-\\)Spezifität (welche Werte sind als \\(1\\) modelliert worden unter den beobachten \\(0\\), False Positive Rate, FPR). Um diese Werte auszugeben, benötigen Sie das Paket ROCR und die Funktion performance(). # ggf. install.packages(&quot;ROCR&quot;) library(ROCR) # Ein für die Auswertung notwendiges prediction Objekt anlegen pred &lt;- prediction(glm1$fitted.values, Aktien$Aktienkauf) # ROC Kurve perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf) abline(0,1, col = &quot;grey&quot;) # Area under curve (ROC-Wert) performance(pred,&quot;auc&quot;)@y.values #&gt; [[1]] #&gt; [1] 0.636 AUC liegt zwischen \\(0.5\\), wenn das Modell gar nichts erklärt (im Plot die graue Linie) und \\(1\\). Hier ist der Wert also recht gering. Akzeptable Werte liegen bei \\(0.7\\) und größer, gute Werte sind es ab \\(0.8\\).59 13.9 Vertiefung 13.9.1 Modellschätzung Das Modell wird nicht wie bei der lineare Regression über die Methode der kleinsten Quadrate (OLS) geschätzt, sondern über die Maximum Likelihood Methode. Die Koeffizienten werden so gewählt, dass die beobachteten Daten am wahrscheinlichsten (Maximum Likelihood) werden. Das ist ein iteratives Verfahren (OLS erfolgt rein analytisch), daher wird in der letzten Zeile der Ausgabe auch die Anzahl der Iterationen (Fisher Scoring Iterations) ausgegeben. Die Devianz des Modells (Residual deviance) ist \\(-2\\) mal die logarithmierte Likelihood. Die Nulldevianz (Null deviance) ist die Devianz eines Nullmodells, d. h., alle \\(\\beta\\) außer der Konstanten sind 0. 13.9.2 Likelihood Quotienten Test Der Likelihood Quotienten Test (Likelihood Ratio Test, LR-Test) vergleicht die Likelihood \\(L_0\\) des Nullmodels mit der Likelihood \\(L_{\\beta}\\) des geschätzten Modells. Die Prüfgröße des LR-Tests ergibt sich aus: \\[{T=-2\\cdot ln\\left( \\frac{L_0}{L_{\\beta}}\\right)}\\] \\(T\\) ist näherungsweise \\(\\chi ^2\\)-verteilt mit \\(k\\) Freiheitsgraden. In R können Sie den Test mit lrtest() aufrufen. Sie benötigen dazu das Paket lmtest. library(lmtest) lrtest(glm2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse #&gt; Model 2: Aktienkauf ~ 1 #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 4 -340 #&gt; 2 1 -402 -3 125 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Das Modell glm2 ist als Ganzes signifikant, der p-Wert ist sehr klein. Den Likelihood Quotienten Test können Sie auch verwenden, um zwei Modelle miteinander zu vergleichen, z. B., wenn Sie eine weitere Variable hinzugenommen haben und wissen wollen, ob die Verbesserung auch signifikant war. lrtest(glm1, glm2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Aktienkauf ~ Risikobereitschaft #&gt; Model 2: Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 2 -383 #&gt; 2 4 -340 2 86.9 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ja, die Modelle glm1 (mit einer erklärenden Variable) und glm2 unterscheiden sich signifikant voneinander. 13.9.3 Pseudo-\\(R^2\\) Verschiedene Statistiker haben versucht, aus der Likelihood eine Größe abzuleiten, die dem \\(R^2\\) der linearen Regression entspricht. Exemplarisch sei hier McFaddens \\(R^2\\) gezeigt: \\[{R^2=1-\\frac{ln(L_{\\beta})}{ln(L_0)}}\\] Wie bei bei dem \\(R^2\\) der linearen Regression liegt der Wertebereich zwischen 0 und 1. Ab einem Wert von 0,4 kann die Modellanpassung als gut eingestuft werden. Wo liegen \\(R^2\\) der beiden Modelle glm1 und glm2? Sie können es direkt berechnen oder das Paket BaylorEdPsych verwenden. # direkte Berechnung 1 - glm1$deviance/glm1$null.deviance #&gt; [1] 0.0479 1 - glm2$deviance/glm2$null.deviance #&gt; [1] 0.156 # ggf. install.packages(&quot;BaylorEdPsych&quot;) library(BaylorEdPsych) PseudoR2(glm1) #&gt; McFadden Adj.McFadden Cox.Snell Nagelkerke #&gt; 0.0479 0.0404 0.0535 0.0783 #&gt; McKelvey.Zavoina Effron Count Adj.Count #&gt; 0.0826 0.0584 0.7557 0.0656 #&gt; AIC Corrected.AIC #&gt; 769.8624 769.8796 PseudoR2(glm2) #&gt; McFadden Adj.McFadden Cox.Snell Nagelkerke #&gt; 0.1558 0.1434 0.1640 0.2400 #&gt; McKelvey.Zavoina Effron Count Adj.Count #&gt; 0.2828 0.1845 0.7614 0.0874 #&gt; AIC Corrected.AIC #&gt; 687.0068 687.0644 Insgesamt ist die Modellanpassung, auch mit allen Variablen, als schlecht zu bezeichnen. Hinweis: Die Funktion PseudoR2(model) zeigt verschiedene Pseudo-\\(R^2\\) Statistiken, die jeweils unter bestimmten Bedingungen vorteilhaft einzusetzen sind. Für weitere Erläuterungen sei auf die Literatur verwiesen. 13.10 Übung: Rot- oder Weißwein? Der Datensatz untersucht den Zusammenhang zwischen der Qualität und physiochemischen Eigenschaften von portugisieschen Rot- und Weißweinen (Cortez et al. 2009). Sie können in unter https://goo.gl/Dkd7nK herunterladen. Die Originaldaten finden Sie im UCI Machine Learning Repository60. Versuchen Sie anhand geeigneter Variablen, Rot- und Weißweine (richtig) zu klassifizieren61. Zusatzaufgabe: Die Originaldaten bestehen aus einem Datensatz für Weißweine und einem für Rotweine. Laden Sie diese, beachten Sie die Fehlermeldung und beheben die damit verbundenen Fehler und fassen beide Datensätze zu einem gemeinsamen Datensatz zusammen, in dem eine zusätzliche Variable color aufgenommen wird (Rot = 0, Weiß = 1). 13.11 Literatur David M. Diez, Christopher D. Barr, Mine Çetinkaya-Rundel (2014): Introductory Statistics with Randomization and Simulation, https://www.openintro.org/stat/textbook.php?stat_book=isrs, Kapitel 6.4 Nicholas J. Horton, Randall Pruim, Daniel T. Kaplan (2015): Project MOSAIC Little Books A Student’s Guide to R, https://github.com/ProjectMOSAIC/LittleBooks/raw/master/StudentGuide/MOSAIC-StudentGuide.pdf, Kapitel 8 Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013): An Introduction to Statistical Learning – with Applications in R, http://www-bcf.usc.edu/~gareth/ISL/, Kapitel 4.1-4.3 Maike Luhmann (2015): R für Einsteiger, Kapitel 17.5 Daniel Wollschläger (2014): Grundlagen der Datenanalyse mit R, Kapitel 8.1 Literaturverzeichnis "],
["fallstudien-zum-geleiteten-modellieren.html", "Kapitel 14 Fallstudien zum geleiteten Modellieren 14.1 Überleben auf der Titanic 14.2 Außereheliche Affären 14.3 Befehlsübersicht", " Kapitel 14 Fallstudien zum geleiteten Modellieren 14.1 Überleben auf der Titanic In dieser YACSDA (Yet-another-case-study-on-data-analysis) geht es um die beispielhafte Analyse nominaler Daten anhand des “klassischen” Falls zum Untergang der Titanic. Eine Frage, die sich hier aufdrängt, lautet: Kann (konnte) man sich vom Tod freikaufen, etwas polemisch formuliert. Oder neutraler: Hängt die Überlebensquote von der Klasse, in der derPassagiers reist, ab? 14.1.1 Daten und Pakete laden library(&quot;titanic&quot;) data(titanic_train) Man beachte, dass ein Paket nur einmalig zu installieren ist (wie jede Software). Dann aber muss das Paket bei jedem Starten von R wieder von neuem gestartet werden. Außerdem ist es wichtig zu wissen, dass das Laden eines Pakets nicht automatisch die Datensätze aus dem Paket lädt. Man muss das oder die gewünschten Pakete selber (mit data(...)) laden. Und: Der Name eines Pakets (z.B. titanic) muss nicht identisch sein mit dem oder den Datensätzen des Pakets (z.B. titanic_train). library(tidyverse) 14.1.2 Erster Blick Werfen wir einen ersten Blick in die Daten: # install.packages(&quot;dplyr&quot;, dependencies = TRUE) # ggf. vorher installieren glimpse(titanic_train) #&gt; Observations: 891 #&gt; Variables: 12 #&gt; $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,... #&gt; $ Survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,... #&gt; $ Pclass &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3,... #&gt; $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bra... #&gt; $ Sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;mal... #&gt; $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, ... #&gt; $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4,... #&gt; $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1,... #&gt; $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;1138... #&gt; $ Fare &lt;dbl&gt; 7.25, 71.28, 7.92, 53.10, 8.05, 8.46, 51.86, 21.07... #&gt; $ Cabin &lt;chr&gt; &quot;&quot;, &quot;C85&quot;, &quot;&quot;, &quot;C123&quot;, &quot;&quot;, &quot;&quot;, &quot;E46&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, ... #&gt; $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, ... 14.1.3 Welche Variablen sind interessant? Von 12 Variablen des Datensatzes interessieren uns offenbar Pclass und Survived; Hilfe zum Datensatz kann man übrigens mit help(titanic_train) bekommen. Diese beiden Variablen sind kategorial (nicht-metrisch), wobei sie in der Tabelle mit Zahlen kodiert sind. Natürlich ändert die Art der Codierung (hier als Zahl) nichts am eigentlichen Skalenniveau. Genauso könnte man “Mann” mit 1 und “Frau” mit 2 kodieren; ein Mittelwert bliebe genauso (wenig) aussagekräftig. Zu beachten ist hier nur, dass sich manche R-Befehle verunsichern lassen, wenn nominale Variablen mit Zahlen kodiert sind. Daher ist es oft besser, nominale Variablen mit Text-Werten zu benennen (wie “survived” vs. “drowned” etc.). Wir kommen später auf diesen Punkt zurück. 14.1.4 Univariate Häufigkeiten Bevor wir uns in kompliziertere Fragestellungen stürzen, halten wir fest: Wir untersuchen zwei nominale Variablen. Sprich: wir werden Häufigkeiten auszählen. Häufigkeiten (und relative Häufigkeiten, also Anteile oder Quoten) sind das, was uns hier beschäftigt. Zählen wir zuerst die univariaten Häufigkeiten aus: Wie viele Passagiere gab es pro Klasse? Wie viele Passagiere gab es pro Wert von Survived (also die überlebten bzw. nicht überlebten)? c1 &lt;- dplyr::count(titanic_train, Pclass) c1 #&gt; # A tibble: 3 × 2 #&gt; Pclass n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 216 #&gt; 2 2 184 #&gt; 3 3 491 Achtung - Namenskollision! Sowohl im Paket mosaic als auch im Paket dplyr gibt es einen Befehl count. Für select gilt das gleiche. Das arme R weiß nicht, welchen von beiden wir meinen und entscheidet sich im Zweifel für den falschen. Da hilft, zu sagen, aus welchem Paket wir den Befehl beziehen wollen. Das macht der Operator ::. Aha. Zur besseren Anschaulichkeit können wir das auch plotten (ein Diagramm dazu malen). # install.packages(&quot;ggplot2&quot;, dependencies = TRUE) library(ggplot2) qplot(x = Pclass, y = n, data = c1) Der Befehl qplot zeichnet automatisch Punkte, wenn auf beiden Achsen “Zahlen-Variablen” stehen (also Variablen, die keinen “Text”, sondern nur Zahlen beinhalten. In R sind das Variablen vom Typ int (integer), also Ganze Zahlen oder vom Typ num (numeric), also reelle Zahlen). c2 &lt;- dplyr::count(titanic_train, Survived) c2 #&gt; # A tibble: 2 × 2 #&gt; Survived n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 549 #&gt; 2 1 342 Man beachte, dass der Befehl count stehts eine Tabelle (data.frame bzw. tibble) verlangt und zurückliefert. 14.1.5 Bivariate Häufigkeiten OK, gut. Jetzt wissen wir die Häufigkeiten pro Wert von Survived (dasselbe gilt für Pclass). Eigentlich interessiert uns aber die Frage, ob sich die relativen Häufigkeiten der Stufen von Pclass innerhalb der Stufen von Survived unterscheiden. Einfacher gesagt: Ist der Anteil der Überlebenden in der 1. Klasse größer als in der 3. Klasse? Zählen wir zuerst die Häufigkeiten für alle Kombinationen von Survived und Pclass: c3 &lt;- dplyr::count(titanic_train, Survived, Pclass) c3 #&gt; Source: local data frame [6 x 3] #&gt; Groups: Survived [?] #&gt; #&gt; Survived Pclass n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 1 80 #&gt; 2 0 2 97 #&gt; 3 0 3 372 #&gt; 4 1 1 136 #&gt; 5 1 2 87 #&gt; 6 1 3 119 Da Pclass 3 Stufen hat (1., 2. und 3. Klasse) und innerhalb jeder dieser 3 Klassen es die Gruppe der Überlebenden und der Nicht-Überlebenden gibt, haben wir insgesamt 3*2=6 Gruppen. Es ist hilfreich, sich diese Häufigkeiten wiederum zu plotten; wir nehmen den gleichen Befehl wie oben. qplot(x = Pclass, y = n, data = c3) Hm, nicht so hilfreich. Schöner wäre, wenn wir (farblich) erkennen könnten, welcher Punkt für “Überlebt” und welcher Punkt für “Nicht-Überlebt” steht. Mit qplot geht das recht einfach: Wir sagen der Funktion qplot, dass die Farbe (color) der Punkte den Stufen von Survived zugeordnet werden sollen: qplot(x = Pclass, y = n, color = Survived, data = c3) Viel besser. Was noch stört, ist, dass Survived als metrische Variable verstanden wird. Das Farbschema lässt Nuancen, feine Farbschattierungen, zu. Für nominale Variablen macht das keinen Sinn; es gibt da keine Zwischentöne. Tot ist tot, lebendig ist lebendig. Wir sollten daher der Funktion sagen, dass es sich um nominale Variablen handelt: qplot(x = factor(Pclass), y = n, color = factor(Survived), data = c3) Viel besser. Jetzt noch ein bisschen Schnickschnack: qplot(x = factor(Pclass), y = n, color = factor(Survived), data = c3) + labs(x = &quot;Klasse&quot;, title = &quot;Überleben auf der Titanic&quot;, colour = &quot;Überlebt?&quot;) 14.1.6 Signifikanztest Manche Leute mögen Signifikanztests. Ich persönlich stehe ihnen kritisch gegenüber, da ein p-Wert eine Funktion der Stichprobengröße ist und außerdem zumeist missverstanden wird (er gibt nicht die Wahrscheinlichkeit der getesteten Hypothese an, was die Frage aufwirft, warum er mich dann interessieren sollte). Aber seisdrum, berechnen wir mal einen p-Wert. Es gibt mehrere statistische Tests, die sich hier potenziell anböten (was die Frage nach der Objektivität von statistischen Tests in ein ungünstiges Licht rückt). Nehmen wir den \\(\\chi^2\\)-Test. chisq.test(titanic_train$Survived, titanic_train$Pclass) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: titanic_train$Survived and titanic_train$Pclass #&gt; X-squared = 100, df = 2, p-value &lt;2e-16 Der p-Wert ist kleiner als 5%, daher entscheiden wir uns, entsprechend der üblichen Gepflogenheit, gegen die H0 und für die H1: “Es gibt einen Zusammenhang von Überlebensrate und Passagierklasse”. 14.1.7 Effektstärke Abgesehen von der Signifikanz, und interessanter, ist die Frage, wie sehr die Variablen zusammenhängen. Für Häufigkeitsanalysen mit 2*2-Feldern bietet sich das “Odds Ratio” (OR), das Chancenverhältnis an. Das Chancen-Verhältnis beantwortet die Frage: “Um welchen Faktor ist die Überlebenschance in der einen Klasse größer als in der anderen Klasse?”. Eine interessante Frage, als schauen wir es uns an. Das OR ist nur definiert für 2*2-Häufigkeitstabellen, daher müssen wir die Anzahl der Passagierklassen von 3 auf 2 verringern. Nehmen wir nur 1. und 3. Klasse, um den vermuteten Effekt deutlich herauszuschälen: t2 &lt;- filter(titanic_train, Pclass != 2) # &quot;!=&quot; heißt &quot;nicht&quot; Alternativ (synonym) könnten wir auch schreiben: t2 &lt;- filter(titanic_train, Pclass == 1 | Pclass == 3) # &quot;|&quot; heißt &quot;oder&quot; Und dann zählen wir wieder die Häufigkeiten aus pro Gruppe: c4 &lt;- dplyr::count(t2, Pclass) c4 #&gt; # A tibble: 2 × 2 #&gt; Pclass n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 216 #&gt; 2 3 491 Schauen wir nochmal den p-Wert an, da wir jetzt ja mit einer veränderten Datentabelle operieren: chisq.test(t2$Survived, t2$Pclass) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: t2$Survived and t2$Pclass #&gt; X-squared = 100, df = 1, p-value &lt;2e-16 Ein \\(\\chi^2\\)-Wert von ~96 bei einem n von 707. Dann berechnen wir die Effektstärke (OR) mit dem Paket compute.es (muss ebenfalls installiert sein). library(compute.es) chies(chi.sq = 96, n = 707) #&gt; Mean Differences ES: #&gt; #&gt; d [ 95 %CI] = 0.79 [ 0.63 , 0.95 ] #&gt; var(d) = 0.01 #&gt; p-value(d) = 0 #&gt; U3(d) = 78.6 % #&gt; CLES(d) = 71.2 % #&gt; Cliff&#39;s Delta = 0.42 #&gt; #&gt; g [ 95 %CI] = 0.79 [ 0.63 , 0.95 ] #&gt; var(g) = 0.01 #&gt; p-value(g) = 0 #&gt; U3(g) = 78.6 % #&gt; CLES(g) = 71.2 % #&gt; #&gt; Correlation ES: #&gt; #&gt; r [ 95 %CI] = 0.37 [ 0.3 , 0.43 ] #&gt; var(r) = 0 #&gt; p-value(r) = 0 #&gt; #&gt; z [ 95 %CI] = 0.39 [ 0.31 , 0.46 ] #&gt; var(z) = 0 #&gt; p-value(z) = 0 #&gt; #&gt; Odds Ratio ES: #&gt; #&gt; OR [ 95 %CI] = 4.21 [ 3.15 , 5.61 ] #&gt; p-value(OR) = 0 #&gt; #&gt; Log OR [ 95 %CI] = 1.44 [ 1.15 , 1.73 ] #&gt; var(lOR) = 0.02 #&gt; p-value(Log OR) = 0 #&gt; #&gt; Other: #&gt; #&gt; NNT = 3.57 #&gt; Total N = 707 Die Chance zu überleben ist also in der 1. Klasse mehr als 4 mal so hoch wie in der 3. Klasse. Es scheint: Money buys you live… 14.1.8 Logististische Regression Berechnen wir noch das Odds Ratio mit Hilfe der logistischen Regression. Zum Einstieg: Ignorieren Sie die folgende Syntax und schauen Sie sich das Diagramm an. Hier sehen wir die (geschätzten) Überlebens-Wahrscheinlichkeiten für Passagiere der 1. Klasse vs. Passagiere der 3. Klasse. titanic2 &lt;- titanic_train %&gt;% filter(Pclass %in% c(1,3)) %&gt;% mutate(Pclass = factor(Pclass)) glm1 &lt;- glm(data = titanic2, formula = Survived ~ Pclass, family = &quot;binomial&quot;) exp(coef(glm1)) #&gt; (Intercept) Pclass3 #&gt; 1.700 0.188 titanic2$pred_prob &lt;- predict(glm1, type = &quot;response&quot;) Wir sehen, dass die Überlebens-Wahrscheinlichkeit in der 1. Klasse höher ist als in der 3. Klasse. Optisch grob geschätzt, ~60% in der 1. Klasse und ~25% in der 3. Klasse. Schauen wir uns die logistische Regression an: Zuerst haben wir den Datensatz auf die Zeilen beschränkt, in denen Personen aus der 1. und 3. Klasse vermerkt sind (zwecks Vergleichbarkeit zu oben). Dann haben wir mit glm und family = &quot;binomial&quot; eine logistische Regression angefordert. Man beachte, dass der Befehl sehr ähnlich zur normalen Regression (lm(...)) ist. Da die Koeffizienten in der Logit-Form zurückgegeben werden, haben wir sie mit der Exponential-Funktion in die “normale” Odds-Form gebracht (delogarithmiert, boa). Wir sehen, dass die Überlebens-Chance (Odds) 1.7 zu 1 betrug - bei der ersten Stufe von Pclass (1)62; von 27 Menschen überlebten in dieser Gruppe also 17 (17/27 = .63 Überlebens-Wahrscheinlichkeit); s. Intercept; der Achsenabschnitt gibt den Odds an, wenn die Prädiktor-Variable(n) den Wert “Null” hat/ haben, bzw. die erste Ausprägung, hier 1. Im Vergleich dazu wird die Überlebens-Chance deutlich schlechter, wenn man die nächste Gruppe von Pclass (3) betrachtet. Die Odds verändern sich um den Faktor ~0.2. Da der Faktor kleiner als 1 ist, ist das kein gutes Zeichen. Die Überlebens-Chance sinkt; etwas genauer auf: 1.7 * 0.2 ≈ 0.34. Das heißt, die Überlebens-Chance ist in der 3. Klasse nur noch ca. 1 zu 3 (Überlebens-Wahrscheinlichkeit: ~25%). Komfortabler können wir uns die Überlebens-Wahrscheinlichkeiten mit der Funktion predict ausgeben lassen. predict(glm1, newdata = data.frame(Pclass = factor(&quot;1&quot;)), type = &quot;response&quot;) #&gt; 1 #&gt; 0.63 predict(glm1, newdata = data.frame(Pclass = factor(&quot;3&quot;)), type = &quot;response&quot;) #&gt; 1 #&gt; 0.242 Alternativ kann man die Häufigkeiten auch noch “per Hand” bestimmen: titanic_train %&gt;% filter(Pclass %in% c(1,3)) %&gt;% dplyr::select(Survived, Pclass) %&gt;% group_by(Pclass, Survived) %&gt;% summarise(n = n() ) %&gt;% mutate(Anteil = n / sum(n)) #&gt; Source: local data frame [4 x 4] #&gt; Groups: Pclass [2] #&gt; #&gt; Pclass Survived n Anteil #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 0 80 0.370 #&gt; 2 1 1 136 0.630 #&gt; 3 3 0 372 0.758 #&gt; 4 3 1 119 0.242 Übersetzen wir dies Syntax auf Deutsch: Nehme den Datensatz “titanic_train” UND DANN Filtere nur die 1. und die 3. Klasse heraus UND DANN wähle nur die Spalten “Survived” und “Pclass” UND DANN gruppiere nach “Pclass” und “Survived” UND DANN zähle die Häufigkeiten für jede dieser Gruppen aus UND DANN berechne den Anteil an Überlebenden bzw. Nicht-Überlebenden für jede der beiden Passagierklassen. FERTIG. 14.1.9 Effektstärken visualieren Zum Abschluss schauen wir uns die Stärke des Zusammenhangs noch einmal graphisch an. Wir berechnen dafür die relativen Häufigkeiten pro Gruppe (im Datensatz ohne 2. Klasse, der Einfachheit halber). c5 &lt;- dplyr::count(t2, Pclass, Survived) c5$prop &lt;- c5$n / 707 c5 #&gt; Source: local data frame [4 x 4] #&gt; Groups: Pclass [?] #&gt; #&gt; Pclass Survived n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 0 80 0.113 #&gt; 2 1 1 136 0.192 #&gt; 3 3 0 372 0.526 #&gt; 4 3 1 119 0.168 Genauer gesagt haben die Häufigkeiten pro Gruppe in Bezug auf die Gesamtzahl aller Passagiere berechnet; die vier Anteile addieren sich also zu 1 auf. Das visualisieren wir wieder qplot(x = factor(Pclass), y = prop, fill = factor(Survived), data = c5, geom = &quot;col&quot;) Das geom = &quot;col&quot; heißt, dass als “geometrisches Objekt” dieses Mal keine Punkte, sondern Säulen (columns) verwendet werden sollen. qplot(x = factor(Pclass), y = prop, fill = factor(Survived), data = c5, geom = &quot;col&quot;) Ganz nett, aber die Häufigkeitsunterscheide von Survived zwischen den beiden Werten von Pclass stechen noch nicht so ins Auge. Wir sollten es anders darstellen. Hier kommt der Punkt, wo wir von qplot auf seinen großen Bruder, ggplot wechseln sollten. qplot ist in Wirklichkeit nur eine vereinfachte Form von ggplot; die Einfachheit wird mit geringeren Möglichkeiten bezahlt. Satteln wir zum Schluss dieser Fallstudie also um: ggplot(data = c5) + aes(x = factor(Pclass), y = n, fill = factor(Survived)) + geom_col(position = &quot;fill&quot;) + labs(x = &quot;Passagierklasse&quot;, fill = &quot;Überlebt?&quot;, caption = &quot;Nur Passagiere, keine Besatzung&quot;) Jeden sehen wir die Häufigkeiten des Überlebens bedingt auf die Passagierklasse besser. Wir sehen auf den ersten Blick, dass sich die Überlebensraten deutlich unterscheiden: Im linken Balken überleben die meisten; im rechten Balken ertrinken die meisten. Diese letzte Analyse zeigt deutlich die Kraft von (Daten-)Visualisierungen auf. Der zu untersuchende Effekt tritt hier am stärken zu Tage; außerdem ist die Analyse relativ einfach. Eine alternative Darstellung ist diese: c5 %&gt;% ggplot + aes(x = factor(Pclass), y = factor(Survived), fill = n) + geom_tile() Hier werden die vier “Fliesen” gleich groß dargestellt; die Fallzahl wird durch die Füllfarbe besorgt. 14.1.10 Fazit In der Datenanalyse (mit R) kommt man mit wenigen Befehlen schon sehr weit; dplyr und ggplot2 zählen (zu Recht) zu den am häufigsten verwendeten Paketen. Beide sind flexibel, konsistent und spielen gerne miteinander. Die besten Einblicke haben wir aus deskriptiver bzw. explorativer Analyse (Diagramme) gewonnen. Signifikanztests oder komplizierte Modelle waren nicht zentral. In vielen Studien/Projekten der Datenanalyse gilt ähnliches: Daten umformen und verstehen bzw. “veranschaulichen” sind zentrale Punkte, die häufig viel Zeit und Wissen fordern. Bei der Analyse von nominalskalierten sind Häufigkeitsauswertungen ideal. 14.2 Außereheliche Affären Wovon ist die Häufigkeit von Affären (Seitensprüngen) in Ehen abhängig? Diese Frage soll anhand des Datensates Affair untersucht werden. Quelle: http://statsmodels.sourceforge.net/0.5.0/datasets/generated/fair.html Der Datensatz findet sich (in ähnlicher Form) auch im R-Paket COUNT (https://cran.r-project.org/web/packages/COUNT/index.html). Laden wir als erstes den Datensatz in R. Wählen Sie zuerst das Verzeichnis als Arbeitsverzeichnis, in dem die Daten liegen. Dann laden Sie z.B. mit dem R-Commander (s. Skript) oder “per Hand” z.B. bei mir so: Affair &lt;- read.csv(&quot;data/Affairs.csv&quot;) Schauen wir mal, ob es funktioniert hat (“Datenmatrix betrachten”): head(Affair) #&gt; X affairs gender age yearsmarried children religiousness education #&gt; 1 1 0 male 37 10.00 no 3 18 #&gt; 2 2 0 female 27 4.00 no 4 14 #&gt; 3 3 0 female 32 15.00 yes 1 12 #&gt; 4 4 0 male 57 15.00 yes 5 18 #&gt; 5 5 0 male 22 0.75 no 2 17 #&gt; 6 6 0 female 32 1.50 no 2 17 #&gt; occupation rating #&gt; 1 7 4 #&gt; 2 6 4 #&gt; 3 1 4 #&gt; 4 6 5 #&gt; 5 6 3 #&gt; 6 5 5 Ok scheint zu passen. Was jetzt? 14.2.1 Zentrale Statistiken Geben Sie zentrale deskriptive Statistiken an für Affärenhäufigkeit und Ehezufriedenheit! # nicht robust: mean(Affair$affairs, na.rm = T) #&gt; [1] 1.46 sd(Affair$affairs, na.rm = T) #&gt; [1] 3.3 # robust: median(Affair$affair, na.rm = T) #&gt; [1] 0 IQR(Affair$affair, na.rm = T) #&gt; [1] 0 Es scheint, die meisten Leute haben keine Affären: table(Affair$affairs) #&gt; #&gt; 0 1 2 3 7 12 #&gt; 451 34 17 19 42 38 Man kann sich viele Statistiken mit dem Befehl describe aus psych ausgeben lassen, das ist etwas praktischer: library(psych) describe(Affair$affairs) #&gt; vars n mean sd median trimmed mad min max range skew kurtosis se #&gt; X1 1 601 1.46 3.3 0 0.55 0 0 12 12 2.34 4.19 0.13 describe(Affair$rating) #&gt; vars n mean sd median trimmed mad min max range skew kurtosis se #&gt; X1 1 601 3.93 1.1 4 4.07 1.48 1 5 4 -0.83 -0.22 0.04 Dazu muss das Paket psych natürlich vorher installiert sein. Beachten Sie, dass man ein Paket nur einmal installieren muss, aber jedes Mal, wenn Sie R starten, auch starten muss (mit library). install.packages(&quot;psych&quot;) 14.2.2 Visualisieren Visualisieren Sie zentrale Variablen! Sicherlich sind Diagramme auch hilfreich. Dies geht wiederum mit dem R-Commander oder z.B. mit folgenden Befehlen: library(ggplot2) qplot(x = affairs, data = Affair) qplot(x = rating, data = Affair) Die meisten Menschen (dieser Stichprobe) scheinen mit Ihrer Beziehung sehr zufrieden zu sein. 14.2.3 Wer ist zufriedener mit der Partnerschaft: Personen mit Kindern oder ohne? Nehmen wir dazu mal ein paar dplyr-Befehle: library(dplyr) Affair %&gt;% group_by(children) %&gt;% summarise(rating_children = mean(rating, na.rm = T)) #&gt; # A tibble: 2 × 2 #&gt; children rating_children #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 no 4.27 #&gt; 2 yes 3.80 Ah! Kinder sind also ein Risikofaktor für eine Partnerschaft! Gut, dass wir das geklärt haben. 14.2.4 Wie viele fehlende Werte gibt es? Was machen wir am besten damit? Diesen Befehl könnten wir für jede Spalte auführen: sum(is.na(Affair$affairs)) #&gt; [1] 0 Oder lieber alle auf einmal: Affair %&gt;% summarise_each(funs(sum(is.na(.)))) #&gt; X affairs gender age yearsmarried children religiousness education #&gt; 1 0 0 0 0 0 0 0 0 #&gt; occupation rating #&gt; 1 0 0 Übrigens gibt es ein gutes Cheat Sheet für dplyr. Ah, gut, keine fehlenden Werte. Das macht uns das Leben leichter. 14.2.5 Wer ist glücklicher: Männer oder Frauen? Affair %&gt;% group_by(gender) %&gt;% summarise(rating_gender = mean(rating)) #&gt; # A tibble: 2 × 2 #&gt; gender rating_gender #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 female 3.94 #&gt; 2 male 3.92 Praktisch kein Unterschied. Heißt das auch, es gibt keinen Unterschied in der Häufigkeit der Affären? Affair %&gt;% group_by(gender) %&gt;% summarise(affairs_gender = mean(affairs)) #&gt; # A tibble: 2 × 2 #&gt; gender affairs_gender #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 female 1.42 #&gt; 2 male 1.50 Scheint auch kein Unterschied zu sein… Und zum Abschluss noch mal etwas genauer: Teilen wir mal nach Geschlecht und nach Kinderstatus auf, also in 4 Gruppen. Theoretisch dürfte es hier auch keine Unterschiede/Zusammenhänge geben. Zumindest fällt mir kein sinnvoller Grund ein; zumal die vorherige eindimensionale Analyse keine Unterschiede zu Tage gefördert hat. Affair %&gt;% group_by(gender, children) %&gt;% summarise(affairs_mean = mean(affairs), rating_mean = mean(rating)) #&gt; Source: local data frame [4 x 4] #&gt; Groups: gender [?] #&gt; #&gt; gender children affairs_mean rating_mean #&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female no 0.838 4.40 #&gt; 2 female yes 1.685 3.73 #&gt; 3 male no 1.014 4.10 #&gt; 4 male yes 1.659 3.86 Affair %&gt;% group_by(children, gender) %&gt;% summarise(affairs_mean = mean(affairs), rating_mean = mean(rating)) #&gt; Source: local data frame [4 x 4] #&gt; Groups: children [?] #&gt; #&gt; children gender affairs_mean rating_mean #&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 no female 0.838 4.40 #&gt; 2 no male 1.014 4.10 #&gt; 3 yes female 1.685 3.73 #&gt; 4 yes male 1.659 3.86 14.2.6 Effektstärken Berichten Sie eine relevante Effektstärke! Hm, auch keine gewaltigen Unterschiede. Höchstens für die Zufriedenheit mit der Partnerschaft bei kinderlosen Personen scheinen sich Männer und Frauen etwas zu unterscheiden. Hier stellt sich die Frage nach der Größe des Effekts, z.B. anhand Cohen’s d. Dafür müssen wir noch die SD pro Gruppe wissen: Affair %&gt;% group_by(children, gender) %&gt;% summarise(rating_mean = mean(rating), rating_sd = sd(rating)) #&gt; Source: local data frame [4 x 4] #&gt; Groups: children [?] #&gt; #&gt; children gender rating_mean rating_sd #&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 no female 4.40 0.914 #&gt; 2 no male 4.10 1.064 #&gt; 3 yes female 3.73 1.183 #&gt; 4 yes male 3.86 1.046 d &lt;- (4.4 - 4.1)/(1) Die Effektstärke beträgt etwa 0.3. 14.2.7 Korrelationen Berechnen und visualisieren Sie zentrale Korrelationen! Affair %&gt;% select_if(is.numeric) %&gt;% cor -&gt; cor_tab cor_tab #&gt; X affairs age yearsmarried religiousness #&gt; X 1.0000 0.57692 0.0362 0.1078 -0.1164 #&gt; affairs 0.5769 1.00000 0.0952 0.1868 -0.1445 #&gt; age 0.0362 0.09524 1.0000 0.7775 0.1938 #&gt; yearsmarried 0.1078 0.18684 0.7775 1.0000 0.2183 #&gt; religiousness -0.1164 -0.14450 0.1938 0.2183 1.0000 #&gt; education -0.0537 -0.00244 0.1346 0.0400 -0.0426 #&gt; occupation -0.0691 0.04961 0.1664 0.0446 -0.0397 #&gt; rating -0.1951 -0.27951 -0.1990 -0.2431 0.0243 #&gt; education occupation rating #&gt; X -0.05371 -0.0691 -0.1951 #&gt; affairs -0.00244 0.0496 -0.2795 #&gt; age 0.13460 0.1664 -0.1990 #&gt; yearsmarried 0.04000 0.0446 -0.2431 #&gt; religiousness -0.04257 -0.0397 0.0243 #&gt; education 1.00000 0.5336 0.1093 #&gt; occupation 0.53361 1.0000 0.0174 #&gt; rating 0.10930 0.0174 1.0000 library(corrplot) corrplot(cor_tab) 14.2.8 Ehejahre und Affären Wie groß ist der Einfluss (das Einflussgewicht) der Ehejahre bzw. Ehezufriedenheit auf die Anzahl der Affären? Dazu sagen wir R: “Hey R, rechne mal ein lineares Modell”, also eine normale (lineare) Regression. Dazu können wir entweder das entsprechende Menü im R-Commander auswählen, oder folgende R-Befehle ausführen: lm1 &lt;- lm(affairs ~ yearsmarried, data = Affair) summary(lm1) # Ergebnisse der Regression zeigen #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ yearsmarried, data = Affair) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.211 -1.658 -0.994 -0.597 11.366 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.5512 0.2351 2.34 0.019 * #&gt; yearsmarried 0.1106 0.0238 4.65 4e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.24 on 599 degrees of freedom #&gt; Multiple R-squared: 0.0349, Adjusted R-squared: 0.0333 #&gt; F-statistic: 21.7 on 1 and 599 DF, p-value: 4e-06 lm2 &lt;- lm(affairs ~ rating, data = Affair) summary(lm2) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ rating, data = Affair) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.906 -1.399 -0.563 -0.563 11.437 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.742 0.479 9.90 &lt;2e-16 *** #&gt; rating -0.836 0.117 -7.12 3e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.17 on 599 degrees of freedom #&gt; Multiple R-squared: 0.0781, Adjusted R-squared: 0.0766 #&gt; F-statistic: 50.8 on 1 and 599 DF, p-value: 3e-12 Also: yearsmarried und rating sind beide statistisch signifikante Prädiktoren für die Häufigkeit von Affären. Das adjustierte \\(R^2\\) ist allerdings in beiden Fällen nicht so groß. 14.2.9 Ehezufriedenheit als Prädiktor Um wie viel erhöht sich die erklärte Varianz (R-Quadrat) von Affärenhäufigkeit wenn man den Prädiktor Ehezufriedenheit zum Prädiktor Ehejahre hinzufügt? (Wie) verändern sich die Einflussgewichte (b)? lm3 &lt;- lm(affairs ~ rating + yearsmarried, data = Affair) lm4 &lt;- lm(affairs ~ yearsmarried + rating, data = Affair) summary(lm3) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ rating + yearsmarried, data = Affair) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.147 -1.650 -0.837 -0.162 11.894 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.7691 0.5671 6.65 6.8e-11 *** #&gt; rating -0.7439 0.1200 -6.20 1.1e-09 *** #&gt; yearsmarried 0.0748 0.0238 3.15 0.0017 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.15 on 598 degrees of freedom #&gt; Multiple R-squared: 0.0931, Adjusted R-squared: 0.0901 #&gt; F-statistic: 30.7 on 2 and 598 DF, p-value: 2.01e-13 summary(lm4) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ yearsmarried + rating, data = Affair) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.147 -1.650 -0.837 -0.162 11.894 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.7691 0.5671 6.65 6.8e-11 *** #&gt; yearsmarried 0.0748 0.0238 3.15 0.0017 ** #&gt; rating -0.7439 0.1200 -6.20 1.1e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.15 on 598 degrees of freedom #&gt; Multiple R-squared: 0.0931, Adjusted R-squared: 0.0901 #&gt; F-statistic: 30.7 on 2 and 598 DF, p-value: 2.01e-13 Ok. Macht eigentlich die Reihenfolge der Prädiktoren in der Regression einen Unterschied? Der Vergleich von Modell 3 vs. Modell 4 beantwortet diese Frage. Wir sehen, dass beim 1. Regressionsmodell das R^2 0.03 war; beim 2. Modell 0.08 und beim 3. Modell liegt R^2 bei 0.09. Die Differenz zwischen Modell 1 und 3 liegt bei (gerundet) 0.06; wenig. 14.2.10 Weitere Prädiktoren der Affärenhäufigkeit Welche Prädiktoren würden Sie noch in die Regressionsanalyse aufnehmen? Hm, diese Frage klingt nicht so, als ob der Dozent die Antwort selber wüsste… Naja, welche Variablen gibt es denn alles: #&gt; [1] &quot;X&quot; &quot;affairs&quot; &quot;gender&quot; &quot;age&quot; #&gt; [5] &quot;yearsmarried&quot; &quot;children&quot; &quot;religiousness&quot; &quot;education&quot; #&gt; [9] &quot;occupation&quot; &quot;rating&quot; Z.B. wäre doch interessant, ob Ehen mit Kinder mehr oder weniger Seitensprüngen aufweisen. Und ob die “Kinderfrage” die anderen Zusammenhänge/Einflussgewichte in der Regression verändert. Probieren wir es auch. Wir können wiederum im R-Comamnder ein Regressionsmodell anfordern oder es mit der Syntax probieren: lm5 &lt;- lm(affairs~ rating + yearsmarried + children, data = Affair) summary(lm5) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ rating + yearsmarried + children, data = Affair) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.354 -1.732 -0.893 -0.172 12.016 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.8524 0.5881 6.55 1.2e-10 *** #&gt; rating -0.7486 0.1204 -6.22 9.6e-10 *** #&gt; yearsmarried 0.0833 0.0285 2.92 0.0036 ** #&gt; childrenyes -0.1881 0.3482 -0.54 0.5893 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.15 on 597 degrees of freedom #&gt; Multiple R-squared: 0.0936, Adjusted R-squared: 0.089 #&gt; F-statistic: 20.5 on 3 and 597 DF, p-value: 1.11e-12 r2_lm5 &lt;- summary(lm5)$r.squared Das Regressionsgewicht von childrenyes ist negativ. Das bedeutet, dass Ehen mit Kindern weniger Affären verbuchen (aber geringe Zufriedenheit, wie wir oben gesehen haben! Hrks!). Allerdings ist der p-Wert nich signifikant, was wir als Zeichen der Unbedeutsamkeit dieses Prädiktors verstehen können. \\(R^2\\) lungert immer noch bei mickrigen 0.094 herum. Wir haben bisher kaum verstanden, wie es zu Affären kommt. Oder unsere Daten bergen diese Informationen einfach nicht. Wir könnten auch einfach mal Prädiktoren, die wir haben, ins Feld schicken. Mal sehen, was dann passiert: lm6 &lt;- lm(affairs ~ ., data = Affair) summary(lm6) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ ., data = Affair) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.162 -1.644 -0.484 1.016 9.509 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.612898 1.008088 0.61 0.54343 #&gt; X 0.010085 0.000634 15.92 &lt; 2e-16 *** #&gt; gendermale -0.222695 0.252198 -0.88 0.37759 #&gt; age -0.029519 0.018987 -1.55 0.12054 #&gt; yearsmarried 0.120077 0.034656 3.46 0.00057 *** #&gt; childrenyes -0.357956 0.293529 -1.22 0.22314 #&gt; religiousness -0.272637 0.094432 -2.89 0.00403 ** #&gt; education 0.001544 0.053711 0.03 0.97708 #&gt; occupation 0.182340 0.074579 2.44 0.01478 * #&gt; rating -0.456198 0.101757 -4.48 8.8e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.59 on 591 degrees of freedom #&gt; Multiple R-squared: 0.392, Adjusted R-squared: 0.383 #&gt; F-statistic: 42.4 on 9 and 591 DF, p-value: &lt;2e-16 r2_lm6 &lt;- round(summary(lm6)$r.squared, 2) Der “.” im Befehl affairs ~ . oben soll sagen: nimm “alle Variablen, die noch in der Datenmatrix übrig sind”. Insgesamt bleibt die erklärte Varian in sehr bescheidenem Rahmen: 0.39. Das zeigt uns, dass es immer noch nur schlecht verstanden ist – im Rahmen dieser Analyse – welche Faktoren die Affärenhäufigkeit erklärt. 14.2.11 Unterschied zwischen den Geschlechtern Unterscheiden sich die Geschlechter statistisch signifikant? Wie groß ist der Unterschied? Sollte hierlieber das d-Maß oder Rohwerte als Effektmaß angegeben werden? Hier bietet sich ein t-Test für unabhängige Gruppen an. Die Frage lässt auf eine ungerichtete Hypothese schließen (\\(\\alpha\\) sei .05). Mit dem entsprechenden Menüpunkt im R-Commander oder mit folgender Syntax lässt sich diese Analyse angehen: t1 &lt;- t.test(affairs ~ gender, data = Affair) t1 #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: affairs by gender #&gt; t = -0.3, df = 600, p-value = 0.8 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.607 0.452 #&gt; sample estimates: #&gt; mean in group female mean in group male #&gt; 1.42 1.50 Der p-Wert ist mit 0.774 &gt; \\(\\alpha\\). Daher wird die \\(H_0\\) beibehalten. Auf Basis der Stichprobendaten entscheiden wir uns für die \\(H_0\\). Entsprechend umschließt das 95%-KI die Null. Da die Differenz nicht signifikant ist, kann argumentiert werden, dass wir d auf 0 schätzen müssen. Man kann sich den d-Wert auch z.B. von {MBESS} schätzen lassen. Dafür brauchen wir die Anzahl an Männer und Frauen: 315, 286. library(MBESS) ci.smd(ncp = t1$statistic, n.1 = 315, n.2 = 286) #&gt; $Lower.Conf.Limit.smd #&gt; [1] -0.184 #&gt; #&gt; $smd #&gt; t #&gt; -0.0235 #&gt; #&gt; $Upper.Conf.Limit.smd #&gt; [1] 0.137 Das Konfidenzintervall ist zwar relativ klein (die Schätzung also aufgrund der recht großen Stichprobe relativ präzise), aber der Schätzwert für d smd liegt sehr nahe bei Null. Das stärkt unsere Entscheidung, von einer Gleichheit der Populationen (Männer vs. Frauen) auszugehen. 14.2.12 Kinderlose Ehe vs. Ehen mit Kindern Rechnen Sie die Regressionsanalyse getrennt für kinderlose Ehe und Ehen mit Kindern! Hier geht es im ersten Schritt darum, die entsprechenden Teil-Mengen der Datenmatrix zu erstellen. Das kann man natürlich mit Excel o.ä. tun. Alternativ könnte man es in R z.B. so machen: Affair2 &lt;- Affair[Affair$children == &quot;yes&quot;, ] lm7 &lt;- lm(affairs~ rating, data = Affair2) summary(lm7) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ rating, data = Affair2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.190 -1.488 -0.587 -0.488 11.413 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 5.091 0.570 8.93 &lt; 2e-16 *** #&gt; rating -0.901 0.144 -6.25 9.8e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.34 on 428 degrees of freedom #&gt; Multiple R-squared: 0.0837, Adjusted R-squared: 0.0815 #&gt; F-statistic: 39.1 on 1 and 428 DF, p-value: 9.84e-10 Affair3 &lt;- Affair[Affair$children == &quot;no&quot;, ] lm8 &lt;- lm(affairs~ rating, data = Affair3) summary(lm8) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ rating, data = Affair3) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.55 -1.05 -0.55 -0.55 11.45 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.045 0.914 3.33 0.0011 ** #&gt; rating -0.499 0.208 -2.40 0.0177 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.68 on 169 degrees of freedom #&gt; Multiple R-squared: 0.0328, Adjusted R-squared: 0.0271 #&gt; F-statistic: 5.74 on 1 and 169 DF, p-value: 0.0177 Übrigens, einfacher geht das “Subsetten” so: library(dplyr) Affair4 &lt;- filter(Affair, children == &quot;yes&quot;) head(Affair4) #&gt; X affairs gender age yearsmarried children religiousness education #&gt; 1 3 0 female 32 15 yes 1 12 #&gt; 2 4 0 male 57 15 yes 5 18 #&gt; 3 8 0 male 57 15 yes 2 14 #&gt; 4 9 0 female 32 15 yes 4 16 #&gt; 5 11 0 male 37 15 yes 2 20 #&gt; 6 12 0 male 27 4 yes 4 18 #&gt; occupation rating #&gt; 1 1 4 #&gt; 2 6 5 #&gt; 3 4 4 #&gt; 4 1 2 #&gt; 5 7 2 #&gt; 6 6 4 14.2.13 Halodries Rechnen Sie die Regression nur für “Halodries”; d.h. für Menschen mit Seitensprüngen. Dafür müssen Sie alle Menschen ohne Affären aus den Datensatz entfernen. Also, rechnen wir nochmal die Standardregression (lm1). Probieren wir den Befehl filter dazu nochmal aus: Affair5 &lt;- filter(Affair, affairs != 0) lm9 &lt;- lm(affairs ~ rating, data = Affair5) summary(lm9) #&gt; #&gt; Call: #&gt; lm(formula = affairs ~ rating, data = Affair5) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6.06 -3.52 -0.06 3.69 7.48 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.757 1.023 8.56 1.3e-14 *** #&gt; rating -0.848 0.280 -3.03 0.0029 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.14 on 148 degrees of freedom #&gt; Multiple R-squared: 0.0584, Adjusted R-squared: 0.052 #&gt; F-statistic: 9.18 on 1 and 148 DF, p-value: 0.00289 14.2.14 logistische Regression Berechnen Sie für eine logistische Regression mit “Affäre ja vs. nein” als Kriterium, wie stark der Einfluss von Geschlecht, Kinderstatus, Ehezufriedenheit und Ehedauer ist! Affair %&gt;% mutate(affairs_dichotom = if_else(affairs == 0, 0, 1)) %&gt;% glm(affairs_dichotom ~gender + children + rating + yearsmarried, data = ., family = &quot;binomial&quot;) -&gt; lm10 summary(lm10) #&gt; #&gt; Call: #&gt; glm(formula = affairs_dichotom ~ gender + children + rating + #&gt; yearsmarried, family = &quot;binomial&quot;, data = .) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.420 -0.764 -0.617 -0.443 2.171 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.0537 0.4299 0.12 0.90 #&gt; gendermale 0.2416 0.1966 1.23 0.22 #&gt; childrenyes 0.3935 0.2831 1.39 0.16 #&gt; rating -0.4654 0.0874 -5.33 1e-07 *** #&gt; yearsmarried 0.0221 0.0212 1.04 0.30 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 675.38 on 600 degrees of freedom #&gt; Residual deviance: 630.26 on 596 degrees of freedom #&gt; AIC: 640.3 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Wenn if_else unbekannt ist, lohnt sich ein Blick in die Hilfe mit ?if_else (dplyr muss vorher geladen sein). Aha, signifikant ist die Ehezufriedenheit: Je größer rating desto geringer die Wahrscheinlickeit für affairs_dichotom. Macht Sinn! Übrigens, die Funktion lm und glm spucken leider keine brave Tabelle in Normalform aus. Aber man leicht eine schöne Tabelle (data.frame) bekommen mit dem Befehl tidy aus broom: library(broom) tidy(lm10) #&gt; term estimate std.error statistic p.value #&gt; 1 (Intercept) 0.0537 0.4299 0.125 9.01e-01 #&gt; 2 gendermale 0.2416 0.1966 1.229 2.19e-01 #&gt; 3 childrenyes 0.3935 0.2831 1.390 1.64e-01 #&gt; 4 rating -0.4654 0.0874 -5.327 9.97e-08 #&gt; 5 yearsmarried 0.0221 0.0212 1.040 2.99e-01 Und Tabellen (d.h. brave Dataframes) kann man sich schön ausgeben lassen z.B. mit dem Befehl knitr::kable: library(knitr) tidy(lm10) %&gt;% kable term estimate std.error statistic p.value (Intercept) 0.054 0.430 0.125 0.901 gendermale 0.242 0.197 1.229 0.219 childrenyes 0.394 0.283 1.390 0.164 rating -0.465 0.087 -5.327 0.000 yearsmarried 0.022 0.021 1.040 0.299 14.2.15 Zum Abschluss Visualisieren wir mal was! Ok, wie wäre es damit: Affair %&gt;% select(affairs, gender, children, rating) %&gt;% ggplot(aes(x = affairs, y = rating)) + geom_jitter(aes(color = gender, shape = children)) Affair %&gt;% mutate(rating_dichotom = ntile(rating, 2)) %&gt;% ggplot(aes(x = yearsmarried, y = affairs)) + geom_jitter(aes(color = gender)) + geom_smooth() Puh. Geschafft! 14.3 Befehlsübersicht Funktion Beschreibung data Lädt Daten aus einem Datensatz chisq.test Rechnet einen \\(\\chi^2\\)-Test compute.es::chies Liefert Effektstärkemaße für einen \\(\\chi^2\\)-Test glm Rechnet eine generalisiertes lineares Modell (logistische Regression) exp Delogarithmiert einen Ausdruck coef Liefert die Koeffizienten von einem Objekt des Typs lm oder glm zurück. predict Macht eine Vorhersage für ein Objekt des Typs lm oder glm psych::describe Liefert eine Reihe zentraler Statistiken is.na Zeigt an, ob ein Vektor fehlende Werte beinhaltet dplyr::summarise_each Führt summarise für jede Spalte aus t.test Rechnet einen t-Test MBESS:ci.smd Berechnet Cohens d dplyr::ntile Teilt einen Wertebereich in n gleich große Teile auf und gibt für jeden Fall an, in welchem Teil er sich befindet 14.3.1 Versionshinweise und SessionInfo Datum erstellt: 2017-04-20 R Version: 3.3.2 dplyr Version: 0.5.0.9002 sessionInfo() #&gt; R version 3.3.2 (2016-10-31) #&gt; Platform: x86_64-apple-darwin13.4.0 (64-bit) #&gt; Running under: macOS Sierra 10.12.4 #&gt; #&gt; locale: #&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 #&gt; #&gt; attached base packages: #&gt; [1] stats graphics grDevices utils datasets base #&gt; #&gt; other attached packages: #&gt; [1] knitr_1.15.1 broom_0.4.2 MBESS_4.2.0 corrplot_0.77 #&gt; [5] dplyr_0.5.0 ggplot2_2.2.1 psych_1.7.3.21 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] Rcpp_0.12.10 magrittr_1.5 mnormt_1.5-5 #&gt; [4] munsell_0.4.3 colorspace_1.3-2 lattice_0.20-34 #&gt; [7] R6_2.2.0 highr_0.6 stringr_1.2.0 #&gt; [10] plyr_1.8.4 tools_3.3.2 parallel_3.3.2 #&gt; [13] grid_3.3.2 nlme_3.1-130 gtable_0.2.0 #&gt; [16] DBI_0.6-1 htmltools_0.3.5 assertthat_0.1 #&gt; [19] yaml_2.1.14 lazyeval_0.2.0.9000 rprojroot_1.2 #&gt; [22] digest_0.6.12 tibble_1.3.0 bookdown_0.3 #&gt; [25] tidyr_0.6.1 reshape2_1.4.2 codetools_0.2-15 #&gt; [28] evaluate_0.10 rmarkdown_1.4 labeling_0.3 #&gt; [31] stringi_1.1.5 methods_3.3.2 scales_0.4.1 #&gt; [34] backports_1.0.5 foreign_0.8-67 Darum haben wir Pclass in eine Faktor-Variable umgewandelt. Die “erste Klasse” ist jetzt die Referenzklasse, also sozusagen x = 0. Hätten wir Pclass als numerische Variable beibehalten, so würde der Achsenabschnitt die Überlebensrat für die “nullte” Klasse geben, was wenig Sinn macht.↩ "],
["iv-ungeleitetes-modellieren.html", "IV UNGELEITETES MODELLIEREN", " IV UNGELEITETES MODELLIEREN Benötigte Pakte: library(tidyverse) library(cluster) "],
["vertiefung-clusteranalyse.html", "Kapitel 15 Vertiefung: Clusteranalyse 15.1 Einführung 15.2 Intuitive Darstellung der Clusteranalayse 15.3 Euklidische Distanz 15.4 Daten 15.5 Distanzmaße mit R berechnen 15.6 k-Means Clusteranalyse 15.7 Übung: B3 Datensatz 15.8 Literatur 15.9 Verweise", " Kapitel 15 Vertiefung: Clusteranalyse 15.1 Einführung Das Ziel einer Clusteranalyse ist es, Gruppen von Beobachtungen (d. h. Cluster) zu finden, die innerhalb der Cluster möglichst homogen, zwischen den Clustern möglichst heterogen sind. Um die Ähnlichkeit von Beobachtungen zu bestimmen, können verschiedene Distanzmaße herangezogen werden. Für metrische Merkmale wird z. B. häufig die euklidische Metrik verwendet, d. h., Ähnlichkeit und Distanz werden auf Basis des euklidischen Abstands bestimmt. Aber auch andere Abstände wie “Manhatten” oder “Gower” sind möglich. Letztere haben den Vorteil, dass sie nicht nur für metrische Daten sondern auch für gemischte Variablentypen verwendet werden können. Wir werden uns hier auf den euklischen Abstand konzentrieren. 15.2 Intuitive Darstellung der Clusteranalayse Betrachten Sie das folgende Streudiagramm (die Daten sind frei erfunden; “simuliert”, sagt der Statistiker). Es stellt den Zusammenhang von Lernzeit (wieviel ein Student für eine Statistikklausur lernt) und dem Klausurerfolg (wie viele Punkte ein Student in der Klausur erzielt) dar. Sehen Sie Muster? Lassen sich Gruppen von Studierenden mit bloßem Auge abgrenzen (Abb. 15.1)? Abbildung 15.1: Ein Streudiagramm - sehen Sie Gruppen (Cluster) ? Färben wir das Diagramm mal ein (Abb. 15.2). Abbildung 15.2: Ein Streudiagramm - mit drei Clustern Nach dieser “Färbung”, d.h. nach dieser Aufteilung in drei Gruppen, scheint es folgende “Cluster”, “Gruppen” oder “Typen” von Studierenden zu geben: “Blaue Gruppe”: Fälle dieser Gruppe lernen wenig und haben wenig Erfolg in der Klausr. Tja. “Rote Gruppe”: Fälle dieser Gruppe lernen viel; der Erfolg ist recht durchwachsen. “Grüne Gruppe”: Fälle dieser Gruppe lernen mittel viel und erreichen einen vergleichsweise großen Erfolg in der Klausur. Drei Gruppen scheinen ganz gut zu passen. Wir hätten theoretisch auch mehr oder weniger Gruppen unterteilen können. Die Clusteranalyse gibt keine definitive Anzahl an Gruppen vor; vielmehr gilt es, aus theoretischen und statistischen Überlegungen heraus die richtige Anzahl auszuwählen (dazu gleich noch mehr). Unterteilen wir zur Illustration den Datensatz einmal in bis zu 9 Cluster (Abb. 15.3). Abbildung 15.3: Unterschiedliche Anzahlen von Clustern im Vergleich Das “X” soll den “Mittelpunkt” des Clusters zeigen. Der Mittelpunkt ist so gewählt, dass die Distanz von jedem Punkt zum Mittelpunkt möglichst kurz ist. Dieser Abstand wird auch “Varianz innerhalb des Clusters” oder kurz “Varianz within” bezeichnet. Natürlich wird diese Varianz within immer kleiner, je größer die Anzahl der Cluster wird. Abbildung 15.4: Die Summe der Varianz within in Abhängigkeit von der Anzahl von Clustern. Ein Screeplot. Die vertikale gestrichtelte Linie zeigt an, wo die Einsparung an Varianz auf einmal “sprunghaft” weniger wird - just an jedem Knick bei x=3; dieser “Knick” wird auch “Ellbogen” genannt (da sage einer, Statistiker haben keine Phantasie). Man kann jetzt sagen, dass 3 Cluster eine gute Lösung seien, weil mehr Cluster die Varianz innerhalb der Cluster nur noch wenig verringern. Diese Art von Diagramm wird als “Screeplot” bezeihchnet. Fertig! 15.3 Euklidische Distanz Aber wie weit liegen zwei Punkte entfernt? Betrachten wir ein Beispiel. Anna und Berta sind zwei Studentinnen, die eine Statistikklausur geschrieben habenschreiben mussten (bedauernswert). Die beiden unterscheiden sich sowohl in Lernzeit als auch in Klausurerfolg. Aber wie sehr unterscheiden sie sich? Wie groß ist der “Abstand” zwischen Anna und Berta (vgl. Abb. 15.5)? Abbildung 15.5: Distanz zwischen zwei Punkten in der Ebene Eine Möglichkeit, die Distanz zwischen zwei Punkten in der Ebene (2D) zu bestimmen, ist der Satz des Pythagoras (leise Trompetenfanfare). Generationen von Schülern haben diese Gleichung geliebt: \\[c^2 = a^2 + b^2\\]. In unserem Beispiel heißt das \\(c^2 = 3^2+4^2 = 25\\). Folglich ist \\(\\sqrt{c^2}=\\sqrt{25}=5\\). Der Abstand oder der Unterschied zwischen Anna und Berta beträgt also 5 - diese Art, den Abstand zu berechnen, nennt man den euklidischen Abstand. Aber kann man den euklidischen Abstand auch in 3D (Raum) verwenden? Oder gar in Räumen mehr mehr Dimensionen??? Betrachten wir den Versuch, zwei Dreiecke in 3D zu zeichnen. Stellen wir uns vor, zusätzlich zu Lernzeit und Klausurerfolg hätten wir als 3. Merkmal der Studentinnen noch “Statistikliebe” erfasst (Bertas Statistikliebe ist um 2 Punkte höher als Annas). Abbildung 15.6: Pythagoras in 3D Sie können sich Punkt \\(A\\) als Ecke eines Zimmers vorstellen; Punkt \\(B\\) schwebt dann in der Luft, in einiger Entfernung zu \\(A\\). Wieder suchen wir den Abstand zwischen den Punkten \\(A\\) und \\(B\\). Wenn wir die Länge \\(e\\) wüssten, dann hätten wir die Lösung; \\(e\\) ist der Abstand zwischen \\(A\\) und \\(B\\). Im orange farbenen Dreieck gilt wiederum der Satz von Pythagoras: \\(c^2+d^2=e^2\\). Wenn wir also \\(c\\) und \\(d\\) wüssten, so könnten wir \\(e\\) berechnen… \\(c\\) haben wir ja gerade berechnet (5) und \\(d\\) ist einfach der Unterschied in Statistikliebe zwischen Anna und Berta (2)! Also \\[e^2 = c^2 + d^2\\] \\[e^2 = 5^2 + 2^2\\] \\[e^2 = 25 + 4\\] \\[e = \\sqrt{29} \\approx 5.4\\] Ah! Der Unterschied zwischen den beiden Studentinnen beträgt also ~5.4! Intuitiv gesprochen, “schalten wir mehrere Pythagoras-Sätze hintereinander”. Abbildung 15.7: Pythagoras in Reihe geschaltet Das geht nicht nur für “zwei Dreiecke hintereinander”, sondern der Algebra ist es wurscht, wie viele Dreiecke das sind. Um den Abstand zweier Objekte mit k Merkmalen zu bestimmen, kann der euklische Abstand berechnet werden mit. Bei k=3 Merkmalen lautet die Formel dann \\[e^2 = a^2 + b^2 + d^2\\]. Bei mehr als 3 Merkmalen erweitert sich die Formel entsprechend. Dieser Gedanken ist mächtig! Wir können von allen möglichen Objekten den Unterschied bzw. die (euklidische) Distanz ausrechnen! Betrachten wir drei Professoren, die einschätzen sollten, wir sehr sie bestimmte Filme mögen (1: gar nicht; 10: sehr). Die Filme waren: “Die Sendung mit der Maus”, “Bugs Bunny”, “Rambo Teil 1”, “Vom Winde verweht” und “MacGyver”. profs &lt;- data_frame( film1 = c(9, 1, 8), film2 = c(8, 2, 7), film3 = c(1, 8, 3), film4 = c(2, 3, 2), film5 = c(7, 2, 6) ) Betrachten Sie die Film-Vorlieben der drei Professoren. Gibt es ähnliche Professoren hinsichtlich der Vorlieben? Welche Professoren haben eingen größeren “Abstand” in ihren Vorlieben? Wir könnten einen “fünffachen Pythagoras” zu Rate ziehen. Praktischerweise gibt es aber eine R-Funktion, die uns die Rechnerei abnimmt: dist(profs) #&gt; 1 2 #&gt; 2 13.23 #&gt; 3 2.65 10.77 Offenbar ist der (euklidische) Abstand zwischen Prof. 1 und 2 groß (13.2); zwischen Prof 2 und 3 auch recht groß (10.8). Aber der Abstand zwischen Prof. 1 und 3 ist relativ klein! Endlich hätten wir diese Frage auch geklärt. Sprechen Sie Ihre Professoren auf deren Filmvorlieben an… 15.4 Daten Schauen wir uns eine Clusteranalyse praktisch an. Wir werden einen simulierten Datensatz aus Chapman &amp; Feit (2015): R for Marketing Research and Analytics. Springer analysieren (http://r-marketing.r-forge.r-project.org). Näheres dazu siehe Kapitel 5 dort. Sie können ihn von https://goo.gl/eUm8PI als csv-Datei herunterladen; oder, wenn sich die Datei im Unterordner data/ (relativ zu ihrem Arbeitsverzeichnis) befindet: segment &lt;- read.csv2(&quot;data/segment.csv&quot;) Wir verwenden die Variante read.csv2, da es sich um eine “deutsche” CSV-Datei handelt. Ein Überblick über die Daten verschafft uns die Funktion glimpse. glimpse(segment) #&gt; Observations: 300 #&gt; Variables: 7 #&gt; $ Alter &lt;dbl&gt; 50.2, 40.7, 43.0, 40.3, 41.1, 40.2, 39.5, 35.7,... #&gt; $ Geschlecht &lt;fctr&gt; Mann, Mann, Frau, Mann, Frau, Mann, Frau, Mann... #&gt; $ Einkommen &lt;dbl&gt; 51356, 64411, 71615, 42728, 71641, 60325, 54746... #&gt; $ Kinder &lt;int&gt; 0, 3, 2, 1, 4, 2, 5, 1, 1, 0, 3, 4, 0, 2, 6, 0,... #&gt; $ Eigenheim &lt;fctr&gt; Nein, Nein, Ja, Nein, Nein, Ja, Nein, Nein, Ne... #&gt; $ Mitgliedschaft &lt;fctr&gt; Nein, Nein, Nein, Nein, Nein, Nein, Ja, Ja, Ne... #&gt; $ Segment &lt;fctr&gt; Gemischte Vorstadt, Gemischte Vorstadt, Gemisc... 15.5 Distanzmaße mit R berechnen Auf Basis der drei metrischen Merkmale (d. h. Alter, Einkommen und Kinder) ergeben sich für die ersten sechs Beobachtungen folgende Abstände: dist(head(segment)) #&gt; 1 2 3 4 5 #&gt; 2 19941.8 #&gt; 3 30946.1 11004.3 #&gt; 4 13179.5 33121.3 44125.6 #&gt; 5 30985.9 11044.0 39.9 44165.3 #&gt; 6 13700.4 6241.5 17245.8 26879.9 17285.5 Sie können erkennen, dass die Beobachtungen 5 und 3 den kleinsten Abstand haben, während 5 und 4 den größten haben. Allerdings zeigen die Rohdaten auch, dass die euklidischen Abstände von der Skalierung der Variablen abhängen (Einkommen streut stärker als Kinder). Daher kann es evt. sinnvoll sein, die Variablen vor der Analyse zu standardisieren (z. B. über scale()). Mit der Funktion daisy() aus dem Paket cluster kann man sich den Abstand zwischen den Objekten ausgeben lassen. Die Funktion errechnet auch Abstandsmaße, wenn die Objekte unterschiedliche Skalenniveaus aufweisen. daisy(head(segment)) #&gt; Dissimilarities : #&gt; 1 2 3 4 5 #&gt; 2 0.307 #&gt; 3 0.560 0.390 #&gt; 4 0.219 0.184 0.502 #&gt; 5 0.516 0.220 0.242 0.404 #&gt; 6 0.401 0.206 0.239 0.268 0.426 #&gt; #&gt; Metric : mixed ; Types = I, N, I, I, N, N, N #&gt; Number of objects : 6 15.6 k-Means Clusteranalyse Beim k-Means Clusterverfahren handelt es sich um eine bestimmte Form von Clusteranalysen; zahlreiche Alternativen existieren, aber die k-Means Clusteranalyse ist recht verbreitet. Im Gegensatz zur z.B. der hierarchischen Clusteranalyse um ein partitionierendes Verfahren. Die Daten werde in k Cluster aufgeteilt – dabei muss die Anzahl der Cluster im vorhinein feststehen. Ziel ist es, dass die Quadratsumme der Abweichungen der Beobachtungen im Cluster zum Clusterzentrum minimiert wird. Der Ablauf des Verfahrens ist wie folgt: Zufällige Beobachtungen als Clusterzentrum Zuordnung der Beobachtungen zum nächsten Clusterzentrum (Ähnlichkeit, z. B. über die euklidische Distanz) Neuberechnung der Clusterzentren als Mittelwert der dem Cluster zugeordneten Beobachtungen Dabei werden die Schritte 2. und 3. solange wiederholt, bis sich keine Änderung der Zuordnung mehr ergibt – oder eine maximale Anzahl an Iterationen erreicht wurde. Hinweis: Die (robuste) Funktion pam() aus dem Paket cluster kann auch mit allgemeinen Distanzen umgehen. Außerdem für gemischte Variablentypen gut geeignet: Das Paket clustMixType. Zur Vorbereitung überführen wir die nominalen Merkmale in logische, d. h. binäre Merkmale, und löschen die Segmente sowie das Ergebnis der hierarchischen Clusteranalyse: segment.num &lt;- segment %&gt;% mutate(Frau = Geschlecht == &quot;Frau&quot;) %&gt;% mutate(Eigenheim = Eigenheim ==&quot;Ja&quot;) %&gt;% mutate(Mitgliedschaft = Mitgliedschaft == &quot;Ja&quot;) %&gt;% dplyr::select(-Geschlecht, -Segment) Über die Funktion mutate() werden Variablen im Datensatz erzeugt oder verändert. Über select() werden einzene Variablen ausgewählt. Die “Pfeife” %&gt;% übergeben das Ergebnis der vorherigen Funktion an die folgende. Aufgrund von (1.) hängt das Ergebnis einer k-Means Clusteranalyse vom Zufall ab. Aus Gründen der Reproduzierbarkeit sollte daher der Zufallszahlengenerator gesetzt werden. Außerdem bietet es sich an verschiedene Startkonfigurationen zu versuchen. in der Funktion kmeans() erfolgt dies durch die Option nstart=. Hier mit k=4 Clustern: set.seed(1896) seg.k &lt;- kmeans(segment.num, centers = 4, nstart = 10) seg.k #&gt; K-means clustering with 4 clusters of sizes 111, 26, 58, 105 #&gt; #&gt; Cluster means: #&gt; Alter Einkommen Kinder Eigenheim Mitgliedschaft Frau #&gt; 1 42.9 46049 1.649 0.505 0.1081 0.568 #&gt; 2 56.4 85973 0.385 0.538 0.0385 0.538 #&gt; 3 27.0 22608 1.224 0.276 0.2069 0.414 #&gt; 4 43.6 62600 1.505 0.457 0.1238 0.590 #&gt; #&gt; Clustering vector: #&gt; [1] 1 4 4 1 4 4 4 1 2 4 1 1 4 4 1 1 1 1 1 4 4 4 1 4 1 1 1 1 4 1 4 4 1 1 2 #&gt; [36] 1 4 1 1 4 4 4 1 4 4 4 4 1 1 1 1 1 2 1 1 4 4 4 4 1 4 1 4 1 1 1 1 4 4 4 #&gt; [71] 4 1 1 4 1 1 4 4 4 4 1 4 1 3 1 4 1 1 1 1 4 4 4 1 1 4 1 4 4 4 3 3 3 3 3 #&gt; [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 #&gt; [141] 3 3 3 3 3 3 3 3 3 3 1 2 4 2 2 4 1 1 2 2 4 4 1 1 4 2 4 4 1 2 2 3 4 1 2 #&gt; [176] 2 4 2 3 4 4 4 1 1 1 1 1 1 4 3 1 4 4 4 4 1 1 1 2 4 4 1 2 4 4 1 4 2 1 2 #&gt; [211] 4 3 4 2 2 4 2 1 4 3 1 2 2 4 2 4 4 1 4 4 1 1 1 1 1 3 1 1 4 1 4 3 1 4 1 #&gt; [246] 4 1 4 1 4 4 4 4 1 1 1 4 4 1 1 1 1 1 1 4 1 1 1 1 1 2 4 4 1 4 1 1 1 1 2 #&gt; [281] 4 4 4 4 1 4 1 4 4 4 1 4 1 4 1 4 1 1 4 1 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 3.18e+09 2.22e+09 1.69e+09 2.81e+09 #&gt; (between_SS / total_SS = 90.6 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; #&gt; [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; #&gt; [9] &quot;ifault&quot; Neben der Anzahl Beobachtungen im Cluster (z. B. 26 in Cluster 2) werden auch die Clusterzentren ausgegeben. Diese können dann direkt verglichen werden. Sie sehen z. B., dass das Durchschnittsalter in Cluster 3 mit 27 am geringsten ist. Der Anteil der Eigenheimbesitzer ist mit 54 % in Cluster 2 am höchsten. Einen Plot der Scores auf den beiden ersten Hauptkomponenten können Sie über die Funktion clusplot() aus dem Paket cluster erhalten. clusplot(segment.num, seg.k$cluster, color = TRUE, shade = TRUE, labels = 4) Wie schon im deskriptiven Ergebnis: Die Cluster 1 und 4 unterscheiden sich (in den ersten beiden Hauptkomponenten) nicht wirklich. Vielleicht sollten dies noch zusammengefasst werden, d. h., mit centers=3 die Analyse wiederholt werden?63 15.7 Übung: B3 Datensatz Der B3 Datensatz Heilemann, U. and Münch, H.J. (1996): West German Business Cycles 1963-1994: A Multivariate Discriminant Analysis. CIRET–Conference in Singapore, CIRET–Studien 50. enthält Quartalsweise Konjunkturdaten aus (West-)Deutschland. Er kann von https://goo.gl/0YCEHf heruntergeladen werden. Wenn die Konjunkturphase PHASEN nicht berücksichtigt wird, wie viele Cluster könnte es geben? Ändert sich das Ergebnis, wenn die Variablen standardisiert werden? Führen Sie eine k-Means Clusteranalyse mit 4 Clustern durch. Worin unterscheiden sich die gefundenen Segmente? 15.8 Literatur Chris Chapman, Elea McDonnell Feit (2015): R for Marketing Research and Analytics, Kapitel 11.3 Reinhold Hatzinger, Kurt Hornik, Herbert Nagel (2011): R – Einführung durch angewandte Statistik. Kapitel 12 15.9 Verweise Diese Übung orientiert sich am Beispiel aus Kapitel 11.3 aus Chapman and Feit (2015) und steht unter der Lizenz Creative Commons Attribution-ShareAlike 3.0 Unported. Der Code steht unter der Apache Lizenz 2.0 Der erste Teil dieser Übung basiert auf diesem Skript: https://cran.r-project.org/web/packages/broom/vignettes/kmeans.html Eine weiterführende, aber gut verständliche Einführung findet sich bei James, Witten, Hastie, and Tibshirani (2013c). Literaturverzeichnis "],
["vi-anhang.html", "VI Anhang", " VI Anhang "],
["probeklausur.html", "Kapitel 16 Probeklausur 16.1 Fragen 16.2 Lösungen", " Kapitel 16 Probeklausur Aussagen sind entweder als “richtig” oder als “falsch” zu beantworten. Offene Fragen verlangen einen “Text” als Antwort. 16.1 Fragen Bei install.packages spielt der Parameter dependencies = TRUE in der Praxis keine Rolle. Dateien mit der Endung .R sind keine Textdateien. Der Befehl read.csv kann auch Dateien einlesen, die nicht lokal, sondern auf einem Server im Internet gespeichert sind. Fehlende Werte werden in R durch NA kodiert. Um Variablen einen Wert zuzuweisen, kann man in R den Zuweisungspfeil &lt;- verwenden. Die deutsche Version von R verwendet im Standard das Komma als Dezimaltrennzeichen. Statistisches Modellieren verwendet die Abduktion als zentrale Denkfigur. Eine Abduktion führt zu sicheren Schlüssen. Das CSV-Format ist identisch zum Excel-Format, was sich auch darin zeigt, dass Excel CSV-Datein oft problemlos öffnet. Das Arbeitsverzeichnis (engl. working directory) ist der Ort, in dem R eine Datei, die Sie aufrufen, vermutet - sofern kein anderer Pfad angegeben ist. In einer Tabelle in Normalform steht in jeder Zeile eine Variable und in jeder Spalte eine Beobachtung. Die Funktion filter filtert Spalten aus einer Tabelle. Die Funktion select lässt Spalten sowohl anhand ihres Namens als auch ihrer Nummer (Position in der Tabelle) auswählen. Die Funktion group_by gruppiert eine Tabelle anhand der Werte einer diskreten Variablen. Die Funktion group_by akzeptiert nur Faktorvariablen als Gruppierungsvariablen. Die Funktion summarise darf nur für Funktionen verwendet werden, welche genau einen Wert zurückliefern. Was sind drei häufige Operationen der Datenaufbereitung? Um Korrelationen mit R zu berechnen, kann man die Funktion corrr::correlate verwenden. corrr::correlate liefert stets einen Dataframe zurück. Tibbles sind eine spezielle Art von Dataframes. Was zeigt uns “Anscombes Quartett”? ggplot unterscheidet drei Bestandtteile eines Diagramms: Daten, Geome und Transformationen. Um eine kontinuierliche Variable zu plotten, wird häufig ein Histrogramm verwendet. Das Geom tile zeigt drei Variablen. Geleitetes Modellieren kann unterteilt werden in prädiktives und explikatives Modellieren. Der Befehl scale verschiebt den Mittelwert einer Verteilung auf 0 und skaliert die sd auf 1. Mit “binnen” im Sinne der Datenanalyse ist gemeint, eine kategoriale Variable in eine kontinuierliche zu überführen. Die Gleichung y = ax + b lässt sich in R darstellen als y ~ ax + b. \\(R^2\\), auch Bestimmtheitsmaß oder Determinationskoeffizient genannt, gibt die Vorhersagegüte im Verhältnis zu einem “Nullmodell” an. Bei der logistischen Regression gilt: Bei \\(β0&gt;0\\) ist die Wahrscheinlichkeit kleiner als 50% gibt, dass das modellierte Ereignis eintritt, wenn alle anderen Prädiktoren Null sind. Die logistische Regression sollte nicht verwendet werden, wenn die abhängige Variable dichotom ist. 32.Die logistische Regression stellt den Zusammenhang zwischen Prädiktor und Kriterium nicht mit einer Geraden, sondern mit einer “s-förmigen” Kurve dar. Bevor die Koeffizienten der logistischen Regression als Odds Ration interpretiert werden können, müssen sie “delogarithmiert” werden. Unter “delogarithmieren” versteht man, die Umkehrfunktion der e-Funktion auf eine Gleichung anzuwenden. Wendet man die “normale” Regression an, um eine dichotome Variable als Kriterium zu modellieren, so kann man Wahrscheinlichkeiten größer als 1 und kleiner als 0 bekommen. Eine typische Idee der Clusteranalyse lautet, die Vaerianz innerhalb der Cluster jeweils zu maximieren. Bei einer k-means-Clusteranalyse darf man nicht die Anzahl der Cluster vorab festlegen; vielmehr ermittelt der Algorithmus die richtige Anzahl der Cluster. Für die Wahl der “richtigen” Anzahl der Cluster kann das “Ellbogen-Kriterium” als Entscheidungsgrundlage herangezogen werden. Ein “Screeplot” stellt die Varianz innerhalb der Cluster als Funktion der Anzahl der Cluster dar (im Rahmen der Clusteranalyse). Die euklidische Distanz zwischen zwei Objekten in der Ebene lässt sich mit dem Satz des Pythagoras berechnen. 16.2 Lösungen Falsch Falsch Richtig Richtig Richtig Falsch Richtig Falsch Falsch Richtig Falsch Falsch Falsch Richtig Richtig Falsch Richtig Auf fehlende Werte prüfen, Fälle mit fehlenden Werte löschen, Fehlende Werte ggf. ersetzen,Nach Fehlern suche, Ausreiser identifizieren, Hochkorrelierte Variablen finden, z-Standardisieren, Quasi-Konstante finden, Auf Normalverteilung prüfen, Werte umkodieren und “binnen”. Richtig Richtig Richtig Es geht hier um vier Datensätze mit zwei Variablen (Spalten; X und Y). Offenbar sind die Datensätze praktisch identisch: Alle X haben den gleichen Mittelwert und die gleiche Varianz; dasselbe gilt für die Y. Die Korrelation zwischen X und Y ist in allen vier Datensätzen gleich. Allerdings erzählt eine Visualisierung der vier Datensätze eine ganz andere Geschichte. Falsch Richtig Richtig Falsch Richtig Falsch Richtig Richtig Falsch Falsch Richtig Richtig Falsch. Richtig wäre: Die Umkehrfunktion des Logarithmus, also die e-Funktion, auf eine Gleichung anzuwenden. Richtig Falsch Falsch. Richtig wäre: Man gibt die Anzahl der Cluster vor. Dann vergleicht man die Varianz within der verschiedenen Lösungen. Richtig Richtig Richtig "],
["literaturverzeichnis.html", "Kapitel 17 Literaturverzeichnis", " Kapitel 17 Literaturverzeichnis "],
["literaturverzeichnis-1.html", "Literaturverzeichnis", " Literaturverzeichnis "]
]
