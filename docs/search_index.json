[
["klassifizierende-regression.html", "Kapitel 12 Klassifizierende Regression 12.1 Vorbereitung 12.2 Problemstellung 12.3 Die Idee der logistischen Regression 12.4 Welche Unterschiede zur linearen Regression gibt es in der Ausgabe? 12.5 Interpretation der Koeffizienten 12.6 Kategoriale Variablen 12.7 Multiple logistische Regression 12.8 Modell- bzw. Klassifikationsgüte 12.9 Vertiefung 12.10 Übung: Rot- oder Weißwein? 12.11 Verweise", " Kapitel 12 Klassifizierende Regression Lernziele: Die Idee der logistischen Regression verstehen. Die Koeffizienten der logistischen Regression interpretieren können. Vertiefungen wie Modellgüte kennen. Für dieses Kapitel benötigen Sie folgende Pakete: library(SDMTools) library(ROCR) library(tidyverse) 12.1 Vorbereitung Hier werden wir den Datensatz Aktienkauf der Universität Zürich (Universität Zürich, Methodenberatung) analysieren. Es handelt es sich um eine finanzpsychologische Fragestellung. Es wurde untersucht, welche Variablen mit der Wahrscheinlichkeit, dass jemand Aktien erwirbt, zusammenhängen. Insgesamt wurden 700 Personen befragt. Folgende Daten wurden erhoben: Aktienkauf (0 = nein, 1 = ja) Jahreseinkommen (in Tausend CHF), Risikobereitschaft (Skala von 0 bis 25) Interesse an der aktuellen Marktlage (Skala von 0 bis 45). Importieren Sie zunächst die Daten. Aktien &lt;- read_csv(&quot;data/Aktien.csv&quot;) 12.2 Problemstellung Können wir anhand der Risikobereitschaft abschätzen, ob die Wahrscheinlichkeit für einen Aktienkauf steigt? Schauen wir uns zunächst ein Streudiagramm an (Abb. 12.1). p1 &lt;- ggplot(aes(y = Aktienkauf, x = Risikobereitschaft), data = Aktien) + geom_point() p1 Abbildung 12.1: Streudiagramm von Risikobereitschaft und Aktienkauf Berechnen wir dann eine normale Regression. lm1 &lt;- lm(Aktienkauf ~ Risikobereitschaft, data = Aktien) summary(lm1) #&gt; #&gt; Call: #&gt; lm(formula = Aktienkauf ~ Risikobereitschaft, data = Aktien) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.684 -0.243 -0.204 0.348 0.814 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.18246 0.02001 9.12 &lt; 2e-16 *** #&gt; Risikobereitschaft 0.05083 0.00762 6.67 5.2e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.427 on 698 degrees of freedom #&gt; Multiple R-squared: 0.0599, Adjusted R-squared: 0.0586 #&gt; F-statistic: 44.5 on 1 and 698 DF, p-value: 5.25e-11 Der Zusammenhang scheint nicht sehr ausgeprägt zu sein. Lassen Sie uns dennoch ein lineare Regression durchführen und das Ergebnis auswerten und graphisch darstellen. p1 + geom_abline(intercept = .18, slope = .05, color = &quot;red&quot;) Abbildung 12.2: Regressionsgerade für Aktien-Modell Der Schätzer für die Steigung für Risikobereitschaft ist signifikant. Das Bestimmtheitsmaß \\(R^2\\) ist allerdings sehr niedrig, aber wir haben bisher ja auch nur eine unabhängige Variable für die Erklärung der abhängigen Variable herangezogen. Doch was bedeutet es, dass die Wahrscheinlichkeit ab einer Risikobereitsschaft von ca. 16 über 1 liegt? Wahrscheinlichkeiten müssen zwischen 0 und 1 liegen. Daher brauchen wir eine Funktion, die das Ergebnis einer linearen Regression in einen Bereich von 0 bis 1 “umbiegt” (die sogenannte Linkfunktion). Eine häufig dafür verwendete Funktion ist die logistische Funktion. Im einfachsten Fall: \\[p(y=1)=\\frac{e^x}{1+e^x}\\] Exemplarisch können wir die logistische Funktion für einen Bereich von \\(\\eta=-10\\) bis \\(+10\\) darstellen (vgl. 12.3). Der Graph der logistischen Funktion ähnelt einem langgestreckten S (“Ogive” genannt). Abbildung 12.3: Die logistische Regression beschreibt eine ‘s-förmige’ Kurve 12.3 Die Idee der logistischen Regression Die logistische Regression ist eine Anwendung des allgemeinen linearen Modells (general linear model, GLM). Die Modellgleichung lautet: \\[p(y_i=1)=L\\bigl(\\beta_0+\\beta_1\\cdot x_{i1}+\\dots+\\beta_K\\cdot x_{ik}\\bigr)+\\epsilon_i\\] \\(L\\) ist die Linkfunktion, in unserer Anwendung die logistische Funktion. \\(x_{ik}\\) sind die beobachten Werte der unabhängigen Variablen \\(X_k\\). \\(k\\) sind die unabhängigen Variablen \\(1\\) bis \\(K\\). Die Funktion glm führt die logistische Regression durch. glm1 &lt;- glm(Aktienkauf ~ Risikobereitschaft, family = binomial(&quot;logit&quot;), data = Aktien) Wir schauen uns zunächst den Plot an (Abb. 12.4. Abbildung 12.4: Modelldiagramm für den Aktien-Datensatz Es werden ein Streudiagramm der beobachten Werte sowie die Regressionslinie ausgegeben. Wir können so z. B. ablesen, dass ab einer Risikobereitschaft von etwa 7 die Wahrscheinlichkeit für einen Aktienkauf nach unserem Modell bei mehr als 50 % liegt. Die Zusammenfassung des Modells zeigt folgendes: summary(glm1) #&gt; #&gt; Call: #&gt; glm(formula = Aktienkauf ~ Risikobereitschaft, family = binomial(&quot;logit&quot;), #&gt; data = Aktien) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.653 -0.738 -0.677 0.825 1.823 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.4689 0.1184 -12.4 &lt; 2e-16 *** #&gt; Risikobereitschaft 0.2573 0.0468 5.5 3.8e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 804.36 on 699 degrees of freedom #&gt; Residual deviance: 765.86 on 698 degrees of freedom #&gt; AIC: 769.9 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Der Achsenabschnitt (intercept) des logits \\(\\eta\\) wird mit -1.47 geschätzt, die Steigung in Richtung Risikobereitschaft mit 0.26. Die (Punkt-)Prognose für die Wahrscheinlickeit eines Aktienkaufs \\(p(y=1)\\) benötigt anders als in der linearen Regression noch die Linkfunktion und ergibt sich somit zu: \\[p(\\texttt{Aktienkauf}=1)=\\frac{1}{1+e^{-(-1.47 + 0.26 \\cdot \\texttt{Risikobereitschaft})}}\\] Die p-Werte der Koeffizienten können in der Spalte Pr(&gt;|z|) abgelesen werden. 12.4 Welche Unterschiede zur linearen Regression gibt es in der Ausgabe? Es gibt kein \\(R^2\\) im Sinne einer erklärten Streuung der \\(y\\)-Werte, da die beobachteten \\(y\\)-Werte nur \\(0\\) oder \\(1\\) annehmen können. Das Gütemaß bei der logistischen Regression ist das Akaike Information Criterion (AIC). Hier gilt allerdings: je kleiner, desto besser. (Anmerkung: es kann ein Pseudo-\\(R^2\\) berechnet werden – kommt später.) Es gibt keine F-Statistik (oder ANOVA) mit der Frage, ob das Modell als Ganzes signifikant ist. (Anmerkung: es kann aber ein vergleichbarer Test durchgeführt werden – kommt später.) 12.5 Interpretation der Koeffizienten 12.5.1 y-Achsenabschnitt (Intercept) \\(\\beta_0\\) Für \\(\\beta_0&gt;0\\) gilt, dass selbst wenn alle anderen unabhängigen Variablen \\(0\\) sind, es eine Wahrscheinlichkeit von mehr als 50% gibt, dass das modellierte Ereignis eintritt. Für \\(\\beta_0&lt;0\\) gilt entsprechend das Umgekehrte. 12.5.2 Steigung \\(\\beta_i\\) mit \\(i=1,2,...,K\\) Für \\(\\beta_i&gt;0\\) gilt, dass mit zunehmenden \\(x_i\\) die Wahrscheinlichkeit für das modellierte Ereignis steigt. Bei \\(\\beta_i&lt;0\\) nimmt die Wahrscheinlichkeit entsprechend ab. Eine Abschätzung der Änderung der Wahrscheinlichkeit (relatives Risiko, relative risk \\(RR\\)) kann über das Chancenverhältnis (Odds Ratio \\(OR\\)) gemacht werden.1 Es ergibt sich vereinfacht \\(e^{\\beta_i}\\). Die Wahrscheinlichkeit ändert sich näherungsweise um diesen Faktor, wenn sich \\(x_i\\) um eine Einheit erhöht. Hinweis: \\(RR\\approx OR\\) gilt nur, wenn der Anteil des modellierten Ereignisses in den beobachteten Daten sehr klein (\\(&lt;5\\%\\)) oder sehr groß ist (\\(&gt;95\\%\\)). 12.5.3 Aufgabe Berechnen Sie das relative Risiko für unser Beispielmodell, wenn sich die Risikobereitschaft um 1 erhöht (Funktion exp()). Vergleichen Sie das Ergebnis mit der Punktprognose für Risikobereitschaft\\(=7\\) im Vergleich zu Risikobereitschaft\\(=8\\). Lösung: # aus Koeffizient abgeschätzt exp(coef(glm1)[2]) #&gt; Risikobereitschaft #&gt; 1.29 In Worten: “Mit jedem Punkt mehr Risikobereitschaft steigen die Chancen (das OR) für Aktienkauf um 1.293”. # mit dem vollständigen Modell berechnet predict(glm1, data.frame(Risikobereitschaft = 1), type = &quot;response&quot;) #&gt; 1 #&gt; 0.229 predict(glm1, data.frame(Risikobereitschaft = 8), type = &quot;response&quot;) #&gt; 1 #&gt; 0.643 Bei einer Risikobereitschaft von 1 beträgt die Wahrscheinlichkeit für \\(y=1\\), d.h. für das Ereignis “Aktienkauf”, 0.23. Bei einer Risikobereitschaft von 8 liegt diese Wahrscheinlichkeit bei 0.64. Sie sehen also, die ungefähr abgeschätzte Änderung der Wahrscheinlichkeit weicht hier doch deutlich von der genau berechneten Änderung ab. Der Anteil der Datensätze mit Risikobereitschaft\\(=1\\) liegt allerdings auch bei 0.26. 12.6 Kategoriale Variablen Wie schon in der linearen Regression können auch in der logistschen Regression kategoriale Variablen als unabhängige Variablen genutzt werden. Als Beispiel nehmen wir den Datensatz tips und versuchen abzuschätzen, ob sich die Wahrscheinlichkeit dafür, dass ein Raucher bezahlt hat (smoker == yes), in Abhängigkeit vom Wochentag ändert. Laden Sie dazu zunächst den Trinkgeld-Datensatz. tips &lt;- read.csv(&quot;data/tips.csv&quot;) Zunächst ein Plot (Abb. ??). qplot(x = smoker, y = day, data = tips, geom = &quot;jitter&quot;) Die relativen Häufigkeiten zeigt folgende Tabelle: Probieren wir die logistische Regression aus: glmtips &lt;- glm(smoker ~ day, family = binomial(&quot;logit&quot;), data = tips) summary(glmtips) #&gt; #&gt; Call: #&gt; glm(formula = smoker ~ day, family = binomial(&quot;logit&quot;), data = tips) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.765 -0.801 -0.758 1.207 1.665 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.322 0.563 2.35 0.01883 * #&gt; daySat -1.391 0.602 -2.31 0.02093 * #&gt; daySun -2.420 0.622 -3.89 1e-04 *** #&gt; dayThur -2.295 0.631 -3.64 0.00027 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 324.34 on 243 degrees of freedom #&gt; Residual deviance: 298.37 on 240 degrees of freedom #&gt; AIC: 306.4 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Auch hier können wir die Koeffizienten in Relation zur Referenzkategorie (hier: Freitag) interpretieren. Die Wahrscheinlichkeit ist an einem Samstag niedriger, der Wert für daySat ist negativ. Eine Abschätzung erhalten wir wieder mit \\(e^{\\beta_i}\\): exp(coef(glmtips)[2]) #&gt; daySat #&gt; 0.249 Daher ist das Chancenverhältnis (Odds Ratio), dass am Samstag ein Raucher am Tisch sitzt, näherungsweise um den Faktor 0.25 niedriger als am Freitag2: \\[{OR=\\frac{\\frac{P(Raucher|Samstag)}{1-P(Raucher|Samstag)}} {\\frac{P(Raucher|Freitag)}{1-P(Raucher|Freitag)}} =\\frac{\\frac{0.483}{0.517}} {\\frac{0.79}{0.21}} \\approx 0.249}\\] Die Wahrscheinlichkeit für einen Raucher am Samstag können wir uns wieder so ausgeben lassen: predict(glmtips, list(day = &quot;Sat&quot;), type = &quot;response&quot;) #&gt; 1 #&gt; 0.483 12.7 Multiple logistische Regression Wir kehren wieder zurück zu dem Datensatz Aktienkauf. Können wir unser Model glm1 mit nur einer erklärenden Variable verbessern, indem weitere unabhängige Variablen hinzugefügt werden? glm2 &lt;- glm(Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse, family = binomial(&quot;logit&quot;), data = Aktien) summary(glm2) #&gt; #&gt; Call: #&gt; glm(formula = Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse, #&gt; family = binomial(&quot;logit&quot;), data = Aktien) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.130 -0.715 -0.539 0.518 3.214 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.66791 0.27903 -5.98 2.3e-09 *** #&gt; Risikobereitschaft 0.34781 0.08822 3.94 8.1e-05 *** #&gt; Einkommen -0.02157 0.00564 -3.83 0.00013 *** #&gt; Interesse 0.08520 0.01775 4.80 1.6e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 804.36 on 699 degrees of freedom #&gt; Residual deviance: 679.01 on 696 degrees of freedom #&gt; AIC: 687 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Alle Schätzer sind signifkant zum 0.1 %-Niveau (*** in der Ausgabe). Zunehmende Risikobereitschaft (der Einfluss ist im Vergleich zum einfachen Modell stärker geworden) und zunehmendes Interesse erhöhen die Wahrscheinlichkeit für einen Aktienkauf. Steigendes Einkommen hingegen senkt die Wahrscheinlichkeit. Ist das Modell besser als das einfache? Ja, da der AIC-Wert von 769.86 auf 687.01 gesunken ist. 12.8 Modell- bzw. Klassifikationsgüte Logistische Regressionsmodelle werden häufig zur Klassifikation verwendet, z.B. ob der Kredit für einen Neukunden ein “guter” Kredit ist oder nicht. Daher sind die Klassifikationseigenschaften bei logistischen Modellen wichtige Kriterien. Hierzu werden die aus dem Modell ermittelten Wahrscheinlichkeiten ab einem Schwellenwert (cutpoint), häufig \\(0.5\\), einer geschätzten \\(1\\) zugeordnet, unterhalb des Schwellenwertes einer \\(0\\). Diese aus dem Modell ermittelten Häufigkeiten werden dann in einer sogenannten Konfusionsmatrix (confusion matrix) mit den beobachteten Häufigkeiten verglichen. Daher sind wichtige Kriterien eines Modells, wie gut diese Zuordnung erfolgt. Dazu werden die Sensitivität (True Positive Rate, TPR), also der Anteil der mit \\(1\\) geschätzten an allen mit \\(1\\) beobachten Werten, und die Spezifität (True Negative Rate) berechnet. Ziel ist es, dass beide Werte möglichst hoch sind. Sie können die Konfusionsmatrix mit dem Paket confusion.matrix() aus dem Paket SDMTools. Ein Parameter ist cutpoint, der standardmäßig auf \\(0.5\\) steht. (cm &lt;- confusion.matrix(Aktien$Aktienkauf, glm1$fitted.values)) #&gt; obs #&gt; pred 0 1 #&gt; 0 509 163 #&gt; 1 8 20 #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;confusion.matrix&quot; sensitivity(cm) #&gt; [1] 0.109 specificity(cm) #&gt; [1] 0.985 Wenn die Anteile der \\(1\\) in den beobachteten Daten sehr gering sind (z. B. bei einem medizinischem Test auf eine seltene Krankheit, Klicks auf einen Werbebanner oder Kreditausfall), kommt eine Schwäche der logistischen Regression zum Tragen: Das Modell wird so optimiert, dass die Wahrscheinlichkeiten \\(p(y=1)\\) alle unter \\(0.5\\) liegen. Das würde zu einer Sensitität von \\(0\\) und einer Spezifiät von \\(1\\) führen. Daher kann es sinnvoll sein, den Cutpoint zu varieren. Daraus ergibt sich ein verallgemeinertes Gütemaß, die ROC-Kurve (Return Operating Characteristic) und den daraus abgeleiteten AUC-Wert (Area Under Curve). Hierzu wird der Cutpoint zwischen 0 und 1 variiert und die Sensitivität gegen \\(1-\\)Spezifität (welche Werte sind als \\(1\\) modelliert worden unter den beobachten \\(0\\), False Positive Rate, FPR). Um diese Werte auszugeben, benötigen Sie das Paket ROCR und die Funktion performance(). # ggf. install.packages(&quot;ROCR&quot;) library(ROCR) # Ein für die Auswertung notwendiges prediction Objekt anlegen pred &lt;- prediction(glm1$fitted.values, Aktien$Aktienkauf) # ROC Kurve perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf) abline(0,1, col = &quot;grey&quot;) # Area under curve (ROC-Wert) performance(pred,&quot;auc&quot;)@y.values #&gt; [[1]] #&gt; [1] 0.636 AUC liegt zwischen \\(0.5\\), wenn das Modell gar nichts erklärt (im Plot die graue Linie) und \\(1\\). Hier ist der Wert also recht gering. Akzeptable Werte liegen bei \\(0.7\\) und größer, gute Werte sind es ab \\(0.8\\).3 12.9 Vertiefung 12.9.1 Modellschätzung Das Modell wird nicht wie bei der lineare Regression über die Methode der kleinsten Quadrate (OLS) geschätzt, sondern über die Maximum Likelihood Methode. Die Koeffizienten werden so gewählt, dass die beobachteten Daten am wahrscheinlichsten (Maximum Likelihood) werden. Das ist ein iteratives Verfahren (OLS erfolgt rein analytisch), daher wird in der letzten Zeile der Ausgabe auch die Anzahl der Iterationen (Fisher Scoring Iterations) ausgegeben. Die Devianz des Modells (Residual deviance) ist \\(-2\\) mal die logarithmierte Likelihood. Die Nulldevianz (Null deviance) ist die Devianz eines Nullmodells, d. h., alle \\(\\beta\\) außer der Konstanten sind 0. 12.9.2 Likelihood Quotienten Test Der Likelihood Quotienten Test (Likelihood Ratio Test, LR-Test) vergleicht die Likelihood \\(L_0\\) des Nullmodels mit der Likelihood \\(L_{\\beta}\\) des geschätzten Modells. Die Prüfgröße des LR-Tests ergibt sich aus: \\[{T=-2\\cdot ln\\left( \\frac{L_0}{L_{\\beta}}\\right)}\\] \\(T\\) ist näherungsweise \\(\\chi ^2\\)-verteilt mit \\(k\\) Freiheitsgraden. In R können Sie den Test mit lrtest() aufrufen. Sie benötigen dazu das Paket lmtest. library(lmtest) lrtest(glm2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse #&gt; Model 2: Aktienkauf ~ 1 #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 4 -340 #&gt; 2 1 -402 -3 125 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Das Modell glm2 ist als Ganzes signifikant, der p-Wert ist sehr klein. Den Likelihood Quotienten Test können Sie auch verwenden, um zwei Modelle miteinander zu vergleichen, z. B., wenn Sie eine weitere Variable hinzugenommen haben und wissen wollen, ob die Verbesserung auch signifikant war. lrtest(glm1, glm2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Aktienkauf ~ Risikobereitschaft #&gt; Model 2: Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 2 -383 #&gt; 2 4 -340 2 86.9 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ja, die Modelle glm1 (mit einer erklärenden Variable) und glm2 unterscheiden sich signifikant voneinander. 12.9.3 Pseudo-\\(R^2\\) Verschiedene Statistiker haben versucht, aus der Likelihood eine Größe abzuleiten, die dem \\(R^2\\) der linearen Regression entspricht. Exemplarisch sei hier McFaddens \\(R^2\\) gezeigt: \\[{R^2=1-\\frac{ln(L_{\\beta})}{ln(L_0)}}\\] Wie bei bei dem \\(R^2\\) der linearen Regression liegt der Wertebereich zwischen 0 und 1. Ab einem Wert von 0,4 kann die Modellanpassung als gut eingestuft werden. Wo liegen \\(R^2\\) der beiden Modelle glm1 und glm2? Sie können es direkt berechnen oder das Paket BaylorEdPsych verwenden. # direkte Berechnung 1 - glm1$deviance/glm1$null.deviance #&gt; [1] 0.0479 1 - glm2$deviance/glm2$null.deviance #&gt; [1] 0.156 # ggf. install.packages(&quot;BaylorEdPsych&quot;) library(BaylorEdPsych) PseudoR2(glm1) #&gt; McFadden Adj.McFadden Cox.Snell Nagelkerke #&gt; 0.0479 0.0404 0.0535 0.0783 #&gt; McKelvey.Zavoina Effron Count Adj.Count #&gt; 0.0826 0.0584 0.7557 0.0656 #&gt; AIC Corrected.AIC #&gt; 769.8624 769.8796 PseudoR2(glm2) #&gt; McFadden Adj.McFadden Cox.Snell Nagelkerke #&gt; 0.1558 0.1434 0.1640 0.2400 #&gt; McKelvey.Zavoina Effron Count Adj.Count #&gt; 0.2828 0.1845 0.7614 0.0874 #&gt; AIC Corrected.AIC #&gt; 687.0068 687.0644 Insgesamt ist die Modellanpassung, auch mit allen Variablen, als schlecht zu bezeichnen. Hinweis: Die Funktion PseudoR2(model) zeigt verschiedene Pseudo-\\(R^2\\) Statistiken, die jeweils unter bestimmten Bedingungen vorteilhaft einzusetzen sind. Für weitere Erläuterungen sei auf die Literatur verwiesen. 12.10 Übung: Rot- oder Weißwein? Der Datensatz untersucht den Zusammenhang zwischen der Qualität und physiochemischen Eigenschaften von portugisieschen Rot- und Weißweinen (Cortez u. a. 2009). Sie können in unter https://goo.gl/Dkd7nK herunterladen. Die Originaldaten finden Sie im UCI Machine Learning Repository4. Versuchen Sie anhand geeigneter Variablen, Rot- und Weißweine (richtig) zu klassifizieren5. Zusatzaufgabe: Die Originaldaten bestehen aus einem Datensatz für Weißweine und einem für Rotweine. Laden Sie diese, beachten Sie die Fehlermeldung und beheben die damit verbundenen Fehler und fassen beide Datensätze zu einem gemeinsamen Datensatz zusammen, in dem eine zusätzliche Variable color aufgenommen wird (Rot = 0, Weiß = 1). 12.11 Verweise Dieses Kapitel basiert teilweise auf Übungen zum Buch OpenIntro (Diez, Barr, und Cetinkaya-Rundel 2014) unter der Lizenz Creative Commons Attribution-ShareAlike 3.0 Unported. Literaturverzeichnis "]
]
