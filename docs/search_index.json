[
["index.html", "Praxis der Datenanalyse Vorwort", " Praxis der Datenanalyse Sebastian Sauer 2017-02-16 Vorwort Statistik heute; was ist das? Sicherlich haben sich die Schwerpunkte von gestern zu heute verschoben. Wenig überraschend spielt der Computer eine immer größere Rolle und die Daten werden vielseitiger und massiger. Entsprechend sind neue Verfahren nötig - und vorhanden, in Teilen - um auf diese neue Situation einzugehen. Einige Verfahren werden daher weniger wichtig, z.B. der p-Wert und der t-Test. In diesem Kurs lernen Sie einige praktische Aspekte der modernen Datenanalyse. R-Pseudo-Syntax R ist (momentan) die führende Umgebung für Datenanalyse. Entsprechend zentral ist R in diesem Kurs. Zugebenermaßen braucht es etwas Zeit, bis man ein paar Brocken “Errisch” spricht. Um den Einstieg zu erleichern, ist Errisch auf Deutsch übersetzt an einigen Stellen, wo mir dies besonders hilfreich erschien. Diese Stellen sind mit diesem Symbol gekennzeichnet (für R-Pseudo-Syntax). Achtung, Falle: Schwierige oder fehlerträchtige Stellen sind mit diesem Symbol markiert. Übungsaufgaben: Das Skript beinhaltet pro Kapitel Übungsaufgaben oder/und Testfragen. Auf diese wird so verwiesen. Ansonsten: Wenn Ihnen R diesen Smiley präsentiert, dann sind Sie am Ziel Ihrer Träume: . Dieses Skript wurde mit dem Paket bookdown (Xie 2015) erstellt, welches wiederum stark auf den Paketen knitr (Xie 2015) und rmarkdown (Allaire et al. 2016) beruht. Diese Pakete stellen verblüffende Funktionalität zur Verfügung als freie Software (frei wie in Bier und frei wie in Freiheit). Worum geht es in diesem Kurs Einführung in moderne Verfahren der Statistik Für Praktiker Betonung liegt auf “modern” und “Praktiker” Ziel Intuitives, grundlegendes Verständnis zu zentralen Konzepten Handwerkszeug zum selber Anwenden Unterschied zu anderen Kursen Wenig Formeln Keine/weniger “typischen” klassischen Methoden wie ANOVA, Poweranalyse etc. Aufzeigen von Problemen mit klassischen Verfahren Kritik am Status-Quo Didaktik Hands-on R Lernfragen Fallstudien Aktuelle Entwicklungen ausgerichtet References "],
["rahmen-teil-1.html", "Kapitel 1 Rahmen - Teil 1 1.1 Software 1.2 Bildnachweise 1.3 ERRRstkontakt 1.4 Was ist Statistik? Wozu ist sie gut? 1.5 Versionshinweise", " Kapitel 1 Rahmen - Teil 1 In diesem Skript geht es um die Praxis der Datenanalyse. Mit Rahmen ist das “Drumherum” der eigentlichen Datenanalyse gemeint. Dazu gehören einige praktische Vorbereitungen und ein paar Überlegungen. Zum Beispiel brauchen wir einen Überblick über das Thema. Voila: Datenanalyse, praktisch betrachtet, kann man in fünf Schritte einteilen (Wickham and Grolemund 2016). Zuerst muss man die Daten einlesen, die Daten also in R (oder einer anderen Software) verfügbar machen (laden). Fügen wir hinzu: In schöner Form verfügbar machen; man nennt dies auch tidy data1. Sobald die Daten in geeigneter Form in R geladen sind, folgt das Aufbereiten. Das beinhaltet Zusammenfassen, Umformen oder Anreichern je nach Bedarf. Ein nächster wesentlicher Schritt ist das Visualisieren der Daten. Ein Bild sagt bekanntlich mehr als viele Worte. Schließlich folgt das Modellieren oder das Hypothesen prüfen: Man überlegt sich, wie sich die Daten erklären lassen könnten. Zu beachten ist, dass diese drei Schritte - Aufbereiten, Visualisieren, Modellieren - keine starre Abfolge sind, sondern eher ein munteres Hin-und-Her-Springen, ein aufbauendes Abwechseln. Der letzte Schritt ist das Kommunizieren der Ergebnisse der Analyse - nicht der Daten. Niemand ist an Zahlenwüsten interessiert; es gilt, spannende Einblicke zu vermitteln. Der Prozess der Datenanalyse vollzieht sich nicht im luftleeren Raum, sondern ist in einem Rahmen eingebettet. Dieser beinhaltet praktische Aspekte - wie Software, Datensätze - und grundsätzliche Überlegungen - wie Ziele und Grundannahmen. 1.1 Software Als Haupt-Analysewerkzeug nutzen wir R; daneben wird uns die sog. “Entwicklungsumgebung” RStudio einiges an komfortabler Funktionalität bescheren. Eine Reihe von R-Paketen (“Packages”; d.h. Erweiterungen) werden wir auch nutzen. R ist eine recht alte Sprache; viele Neuerungen finden in Paketen Niederschlag, da der “harte Kern” von R lieber nicht so stark geändert wird. Stellen Sie sich vor: Seit 29 Jahren nutzen Sie eine Befehl, der Ihnen einen Mittelwert ausrechnet, sagen wir die mittlere Anzahl von Tassen Kaffee am Tag. Und auf einmal wird der Mittelwert anders berechnet?! Eine Welt stürzt ein! Naja, vielleicht nicht ganz so tragisch in dem Beispiel, aber grundsätzlich sind Änderungen in viel benutzen Befehlen potenziell problematisch. Das ist wohl ein Grund, warum sich am “R-Kern” nicht so viel ändert. Die Innovationen in R passieren in den Paketen. Und es gibt viele davon; als ich diese Zeilen schreibe, sind es fast schon 10.000! Genauer: 9937 nach dieser Quelle: https://cran.r-project.org/web/packages/. 1.1.1 R und RStudio installieren Setzt natürlich voraus, dass R installiert ist. Sie können R unter https://cran.r-project.org herunterladen und installieren (für Windows, Mac oder Linux). RStudio finden Sie auf der gleichnamigen Homepage: https://www.rstudio.com; laden Sie die “Desktop-Version” für Ihr Betriebssystem herunter. Die Oberfläche von R, die “Console”, sieht so aus: Die Oberfläche von RStudio sieht (unter allen Betriebssystemen etwa gleich) so aus: 1.1.2 Hilfe! R tut nicht so wie ich das will Manntje, Manntje, Timpe Te, Buttje, Buttje inne See, myne Fru de Ilsebill will nich so, as ik wol will. Gebrüder Grimm, Märchen vom Fischer und seiner Frau2 Ihr R startet nicht oder nicht richtig? Die drei wichtigsten Heilmittel sind: Schließen Sie die Augen für eine Minute. Denken Sie an etwas Schönes und was Rs Problem sein könnte. Schalten Sie den Rechner aus und probieren Sie es morgen noch einmal. Googeln. Sorry für die schnottrigen Tipps. Aber: Es passiert allzu leicht, dass man Fehler wie diese macht: install.packages(dplyr) install.packages(&quot;dliar&quot;) install.packages(&quot;derpyler&quot;) install.packages(&quot;dplyr&quot;) # dependencies vergessen Keine Internet-Verbindung library(dplyr) # ohne vorher zu installieren Wenn R oder RStudio dann immer noch nicht starten oder nicht richtig laufen, probieren Sie dieses: Sehen Sie eine Fehlermeldung, die von einem fehlenden Paket spricht (z.B. “Package ‘Rcpp’ not available”) oder davon spricht, dass ein Paket nicht installiert werden konnte (z.B. “Package ‘Rcpp’ could not be installed” oder “es gibt kein Paket namens ‘Rcpp’” oder “unable to move temporary installation XXX to YYY”), dann tun Sie folgendes: Schließen Sie R und starten Sie es neu. Installieren Sie das oder die angesprochenen Pakete mit install.packages(&quot;name_des_pakets&quot;, dependencies = TRUE) oder mit dem entsprechenden Klick in RStudio. Starten Sie das entsprechende Paket mit library(paket_name). Gerade bei Windows 10 scheinen die Schreibrechte für R (und damit RStudio oder RComannder) eingeschränkt zu sein. Ohne Schreibrechte kann R aber nicht die Pakete (“packages”) installieren, die Sie für bestimmte R-Funktionen benötigen. Daher schließen Sie R bzw. RStudio und suchen Sie das Icon von R oder wenn Sie RStudio verwenden von RStudio. Rechtsklicken Sie das Icon und wählen Sie “als Administrator ausführen”. Damit geben Sie dem Programm Schreibrechte. Jetzt können Sie etwaige fehlende Pakete installieren. Ein weiterer Grund, warum R bzw. RStudio die Schreibrechte verwehrt werden könnnten (und damit die Installation von Paketen), ist ein Virenscanner. Der Virenscanner sagt, nicht ganz zu Unrecht: “Moment, einfach hier Software zu installieren, das geht nicht, zu gefährlich”. Grundsätzlich gut, in diesem Fall unnötig. Schließen Sie R/RStudio und schalten Sie dann den Virenscanner komplett (!) aus. Öffnen Sie dann R/RStudio wieder und versuchen Sie fehlende Pakete zu installieren. Läuft der RCommander unter Mac nicht, dann prüfen Sie, ob Sie X11 (synonym: XQuartz) installiert haben. X11 muss installiert sein, damit der RCommander unter Mac läuft. Die “app nap” Funktion beim Mac kann den RCommander empfindlich ausbremsen. Schalten Sie diese Funktion aus z.B. im RCommander über Tools - Manage Mac OS X app nap for R.app. 1.1.3 Die Denk- und Gefühlswelt von R Wenn Sie RStudio starten, startet R automatisch auch. Starten Sie daher, wenn Sie RStudio gestartet haben, nicht noch extra R. Damit hätten Sie sonst zwei Instanzen von R laufen, was zu Verwirrungen (bei R und beim Nutzer) führen kann. Ein neues R-Skript im RStudio können Sie z.B. öffnen mit File-New File-R Script. R-Skripte können Sie speichern (File-Save) und öffnen. R-Skripte sind einfache Textdateien, die jeder Texteditor verarbeiten kann. Nur statt der Endung txt, sind R-Skripte stolzer Träger der Endung R. Es bleibt aber eine schnöde Textdatei. Bei der Installation von Paketen mit install.packages(&quot;name_des_pakets&quot;) sollte stets der Parameter dependencies = TRUE angefügt werden. Also install.packages(&quot;name_des_pakets&quot;, dependencies = TRUE). Hintergrund ist: Falls das zu installierende Paket seinerseits Pakete benötigt, die noch nicht installiert sind (gut möglich), dann werden diese sog. “dependencies” gleich mitinstalliert (wenn Sie dependencies = TRUE setzen). Hier finden Sie weitere Hinweise zur Installation des RCommanders: http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html. Sie müssen online sein, um Packages zu installieren. Die “app nap” Funktion beim Mac kann den RCommander empfindlich ausbremsen. Schalten Sie diese Funktion aus z.B. im RCommander über Tools - Manage Mac OS X app nap for R.app. Verwenden Sie möglichst die neueste Version von R, RStudio und Ihres Betriebssystems. Ältere Versionen führen u.U. zu Problemen; je älter, desto Problem… Updaten Sie Ihre Packages regelmäßig z.B. mit update.packages() oder dem Button “Update” bei RStudio (Reiter Packages). R zu lernen kann hart sein. Ich weiß, wovon ich spreche. Wahrscheinlich eine spirituelle Prüfung in Geduld und Hartnäckigkeit… Tolle Gelegenheit, sich in diesen Tugenden zu trainieren :-) 1.1.4 Pakete installieren Ein R-Paket, welches für die praktische Datenanalyse praktisch ist, heißt dplyr. Wir werden viel mit diesem Paket arbeiten. Bitte installieren Sie es schon einmal, sofern noch nicht geschehen: install.packages(&quot;dplyr&quot;, dependencies = TRUE) Übrigens, das dependencies = TRUE sagt sinngemäß “Wenn das Funktionieren von dplyr noch von anderen Paketen abhängig ist (es also Abhängigkeiten (dependencies) gibt), dann installiere die gleich mal mit”. Beim Installieren von R-Paketen könnten Sie gefragt werden, welchen “Mirror” Sie verwenden möchten. Das hat folgenden Hintergrund: R-Pakete sind in einer Art “App-Store”, mit Namen CRAN (Comprehense R Archive Network) gespeichert. Damit nicht ein armer, kleiner Server überlastet wird, wenn alle Studis dieser Welt just gerade beschließen, ein Paket herunterzuladen, gibt es viele Kopien dieses Servers - die “Mirrors”, Spiegelbilder. Suchen Sie sich einfach einen aus, der in der Nähe ist. Nicht vergessen: Installieren muss man eine Software nur einmal; starten (laden) muss man sie jedes Mal, wenn man sie vorher geschlossen hat und wieder nutzen möchte: library(dplyr) Der Befehl bedeutet sinngemäß: “Hey R, geh in die Bücherei (library) und hole das Buch (package) dplyr!”. Wann benutzt man bei R Anführungszeichen? Das ist etwas verwirrend im Detail, aber die Grundegel lautet: wenn man Text anspricht. Im Beispiel oben “library(dplyr)” ist “dplyr” hier erst mal für R nichts Bekanntes, weil noch nicht geladen. Demnach müssten eigentlich Anführungsstriche stehen. Allerdings meinte ein Programmierer, dass es doch so bequemer ist. Hat er Recht. Aber bedenken Sie, dass es sich um die Ausnahme einer Regel handelt. Sie können also auch schreiben: library(“dplyr”) oder library(‘dplyr’); geht beides. Das Installieren und Starten anderer Pakete läuft genauso ab. Am besten installieren Sie alle Pakete, die wir in diesem Buch benötigen auf einmal, dann haben Sie Ruhe. 1.1.5 R-Pakete für dieses Buch In diesem Buch verwenden wir die folgenden R-Pakete; diese müssen installiert sein und geladen: Pakete #&gt; [1] &quot;tidyverse&quot; &quot;readr&quot; &quot;knitr&quot; &quot;stringr&quot; #&gt; [5] &quot;car&quot; &quot;nycflights13&quot; &quot;ISLR&quot; &quot;pdftools&quot; #&gt; [9] &quot;downloader&quot; &quot;ggdendro&quot; &quot;gridExtra&quot; &quot;tm&quot; #&gt; [13] &quot;tidytext&quot; &quot;lsa&quot; &quot;SnowballC&quot; &quot;wordcloud&quot; #&gt; [17] &quot;RColorBrewer&quot; &quot;okcupiddata&quot; &quot;reshape2&quot; &quot;wesanderson&quot; #&gt; [21] &quot;GGally&quot; &quot;titanic&quot; &quot;compute.es&quot; Anstelle alle einzeln zu laden (library verdaut nur ein Paket auf einmal), können wir mit etwas R-Judo alle auf einen Haps laden: lapply(Pakete, require, character.only = TRUE) lapply(Pakete, require, character.only = TRUE) #&gt; [[1]] #&gt; [1] TRUE #&gt; #&gt; [[2]] #&gt; [1] TRUE #&gt; #&gt; [[3]] #&gt; [1] TRUE #&gt; #&gt; [[4]] #&gt; [1] TRUE #&gt; #&gt; [[5]] #&gt; [1] TRUE #&gt; #&gt; [[6]] #&gt; [1] TRUE #&gt; #&gt; [[7]] #&gt; [1] TRUE #&gt; #&gt; [[8]] #&gt; [1] TRUE #&gt; #&gt; [[9]] #&gt; [1] TRUE #&gt; #&gt; [[10]] #&gt; [1] TRUE #&gt; #&gt; [[11]] #&gt; [1] TRUE #&gt; #&gt; [[12]] #&gt; [1] TRUE #&gt; #&gt; [[13]] #&gt; [1] TRUE #&gt; #&gt; [[14]] #&gt; [1] TRUE #&gt; #&gt; [[15]] #&gt; [1] TRUE #&gt; #&gt; [[16]] #&gt; [1] TRUE #&gt; #&gt; [[17]] #&gt; [1] TRUE #&gt; #&gt; [[18]] #&gt; [1] TRUE #&gt; #&gt; [[19]] #&gt; [1] TRUE #&gt; #&gt; [[20]] #&gt; [1] TRUE #&gt; #&gt; [[21]] #&gt; [1] TRUE #&gt; #&gt; [[22]] #&gt; [1] TRUE #&gt; #&gt; [[23]] #&gt; [1] TRUE Der Befehl heißt auf Deutsch: “Wende auf jedes Element von Pakete den Befehl library an”3. Hin und wieder ist es sinnvoll, die Pakete auf den neuesten Stand zu bringen; das geht mit update.packages(). 1.1.6 Datensätze Name des Datensatzes Quelle Beschreibung profiles {okcupiddata} Daten von einer Online-Singlebörse Wage {ISLR} Gehaltsdaten von US-amerikanischen Männern inf_test_short Hier4 Ergebnisse einer Statistikklausur flights {nycflights13} Abflüge von den New Yorker Flughäfen wo_men Hier5 Größe von Studierenden tips {reshape2} Trinkgelder in einem Restaurant extra Hier6 Umfrage zu Extraversion Wir verwenden zwei Methoden, um Datensätze in R zu laden. Zum einen laden wir Datensätze aus R-Paketen, z.B. aus dem Paket okcupiddata. Dazu muss das entsprechende Paket installiert und geladen sein. Mit dem Befehl data(name_des_datensatzes, packge = &quot;name_des_paketes&quot;), kann man dann die Daten laden. Das Laden eines Pakets lädt noch nicht die Daten des Paektes; dafür ist der Befehl data zuständig. library(okcupiddata) data(profiles, package = &quot;okcupiddata&quot;) Alternativ kann man die Daten als CSV- oder als XLS(X)-Datei importieren. Die Datei darf dabei sowohl auf einer Webseite als auch lokal (Festplatte, Stic…) liegen. Daten &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/tips.csv&quot;) Wir werden mit beiden Methoden arbeiten und “on the job” Details besprechen. 1.2 Bildnachweise Die Bildnachweise sind in folgenden Muster aufgebaut: Nummer (Verweis) des Bildes, Names des Autors, Titel, Quelle (URL), Lizenz, Abrufdatum. ??, Sebastian Unrau, ohne Titel, https://unsplash.com/photos/CoD2Q92UaEg, CC0, 2017-02-12 1.3 ERRRstkontakt Unser erster Kontakt mit R! Ein paar Anmerkungen vorweg: R unterscheidet zwischen Groß- und Kleinbuchstaben, d.h. Oma und oma sind zwei verschiedene Dinge für R! R verwendet den Punkt . als Dezimaltrennzeichen Fehlende Werte werden in R durch NA kodiert Kommentare werden mit dem Rautezeichen # eingeleitet; der Rest der Zeile von von R dann ignoriert. R wendet Befehle direkt an R ist objektorientiert, d. h. dieselbe Funktion hat evtl. je nach Funktionsargument unterschiedliche Rückgabewerte Hilfe zu einem Befehl erhält man über ein vorgestelltes Fragezeichen ? Zusätzliche Funktionalität kann über Zusatzpakete hinzugeladen werden. Diese müssen ggf. zunächst installiert werden Mit der Pfeiltaste nach oben können Sie einen vorherigen Befehl wieder aufrufen Sofern Sie das Skriptfenster verwenden: einzelne Befehle aus dem Skriptfenster in R Studio können Sie auch mit Str und Enter an die Console schicken 1.3.1 R als Taschenrechner Auch wenn Statistik nicht Mathe ist, so kann man mit R auch rechnen. Geben Sie zum Üben die Befehle in der R Konsole hinter der Eingabeaufforderung &gt; ein und beenden Sie die Eingabe mit Return bzw. Enter. 4+2 #&gt; [1] 6 Das Ergebnis wird direkt angezeigt. Bei x &lt;- 4+2 erscheint zunächst kein Ergebnis. Über &lt;- wird der Variable x der Wert 4+2 zugewiesen. Wenn Sie jetzt x eingeben, wird das Ergebnis #&gt; [1] 6 angezeigt. Sie können jetzt auch mit x weiterrechnen. x/4 #&gt; [1] 1.5 Vielleicht fragen Sie sich was die [1] vor dem Ergebnis bedeutet. R arbeitet vektororientiert, und die [1] zeigt an, dass es sich um das erste (und hier auch letzte) Element des Vektors handelt. 1.4 Was ist Statistik? Wozu ist sie gut? Zwei Fragen bieten sich sich am Anfang der Beschäftigung mit jedem Thema an: Was ist die Essenz des Themas? Warum ist das Thema (oder die Beschäftigung damit) wichtig? Was ist Stististik? Eine Antwort dazu ist, dass Statistik die Wissenschaft von Sammlung, Analyse, Interpretation und Kommunikation mithilfe mathematischer Verfahren ist und zur Entscheidungshilfe beitragen solle (The Oxford Dictionary of Statistical Terms 2006; Romeijn 2016). Damit hätten wir auch den Unterschied zur schnöden Datenanalyse (ein Teil der Statistik) herausgemeiselt. Statistik wird häufig in die zwei Gebiete deskriptive und inferierende Statistik eingeteilt. Erstere fasst viele Zahlen zusammen, so dass wir den Wald statt vieler Bäume sehen. Letztere verallgemeinert von den vorliegenden (sog. “Stichproben-”)Daten auf eine zugrunde liegende Grundmenge (Population). Dabei spielt die Wahrscheinlichkeitsrechnung und Zufallsvariablen eine große Rolle. 1.5 Versionshinweise sessionInfo() #&gt; R version 3.3.2 (2016-10-31) #&gt; Platform: x86_64-apple-darwin13.4.0 (64-bit) #&gt; Running under: macOS Sierra 10.12.3 #&gt; #&gt; locale: #&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 #&gt; #&gt; attached base packages: #&gt; [1] methods stats graphics grDevices utils datasets base #&gt; #&gt; other attached packages: #&gt; [1] compute.es_0.2-4 titanic_0.1.0 GGally_1.3.0 #&gt; [4] wesanderson_0.3.2 reshape2_1.4.2 okcupiddata_0.1.0 #&gt; [7] wordcloud_2.5 RColorBrewer_1.1-2 lsa_0.73.1 #&gt; [10] SnowballC_0.5.1 tidytext_0.1.2 tm_0.6-2 #&gt; [13] NLP_0.1-9 gridExtra_2.2.1 ggdendro_0.1-20 #&gt; [16] downloader_0.4 pdftools_1.0 ISLR_1.0 #&gt; [19] nycflights13_0.2.2 car_2.1-4 stringr_1.1.0 #&gt; [22] knitr_1.15.1 dplyr_0.5.0 purrr_0.2.2.9000 #&gt; [25] readr_1.0.0 tidyr_0.6.1 tibble_1.2 #&gt; [28] ggplot2_2.2.1.9000 tidyverse_1.1.1 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] httr_1.2.1 jsonlite_1.2 splines_3.3.2 #&gt; [4] modelr_0.1.0 assertthat_0.1 yaml_2.1.14 #&gt; [7] slam_0.1-40 backports_1.0.5 lattice_0.20-34 #&gt; [10] quantreg_5.29 digest_0.6.12 rvest_0.3.2 #&gt; [13] minqa_1.2.4 colorspace_1.3-2 htmltools_0.3.5 #&gt; [16] Matrix_1.2-8 plyr_1.8.4 psych_1.6.12 #&gt; [19] broom_0.4.1 SparseM_1.74 haven_1.0.0 #&gt; [22] bookdown_0.3 scales_0.4.1 lme4_1.1-12 #&gt; [25] MatrixModels_0.4-1 mgcv_1.8-16 nnet_7.3-12 #&gt; [28] lazyeval_0.2.0.9000 pbkrtest_0.4-6 mnormt_1.5-5 #&gt; [31] magrittr_1.5 readxl_0.1.1 evaluate_0.10 #&gt; [34] tokenizers_0.1.4 janeaustenr_0.1.4 nlme_3.1-130 #&gt; [37] MASS_7.3-45 forcats_0.2.0 xml2_1.1.1 #&gt; [40] foreign_0.8-67 tools_3.3.2 hms_0.3 #&gt; [43] munsell_0.4.3 grid_3.3.2 nloptr_1.0.4 #&gt; [46] rmarkdown_1.3 codetools_0.2-15 gtable_0.2.0 #&gt; [49] reshape_0.8.6 DBI_0.5-1 R6_2.2.0 #&gt; [52] lubridate_1.6.0 rprojroot_1.2 stringi_1.1.2 #&gt; [55] parallel_3.3.2 Rcpp_0.12.9 References "],
["tidy-data-daten-sauber-einlesen.html", "Kapitel 2 Tidy Data - Daten sauber einlesen 2.1 Daten in R importieren 2.2 Normalform einer Tabelle 2.3 Verweise", " Kapitel 2 Tidy Data - Daten sauber einlesen Abbildung 2.1: Daten sauber einlesen 2.1 Daten in R importieren In R kann man ohne Weiteres verschiedene, gebräuchliche (Excel) oder weniger gebräuchliche (Feather7) Datenformate einlesen. In RStudio lässt sich dies z.B. durch einen schnellen Klick auf Import Dataset im Reiter Environment erledigen. Dabei wird im Hintergrund das Paket readr verwendet (Wickham, Hester, and Francois 2016) (die entsprechende Syntax wird in der Konsole ausgegeben, so dass man sie sich anschauen und weiterverwenden kann). Am einfachsten ist es, eine Excel-Datei über die RStudio-Oberfläche zu importieren; das ist mit ein paar Klicks geschehen: Abbildung 2.2: Daten einlesen (importieren) mit RStudio Es ist für bestimmte Zwecke sinnvoll, nicht zu klicken, sondern die Syntax einzutippen. Zum Beispiel: Wenn Sie die komplette Analyse als Syntax in einer Datei haben (eine sog. “Skriptdatei”), dann brauchen Sie (in RStudio) nur alles auszuwählen und auf Run zu klicken, und die komplette Analyse läuft durch! Die Erfahrung zeigt, dass das ein praktisches Vorgehen ist. Die gebräuchlichste Form von Daten für statistische Analysen ist wahrscheinlich das CSV-Format. Das ist ein einfahes Format, basierend auf einer Textdatei. Schauen Sie sich mal diesen Auszug aus einer CSV-Datei an. &quot;ID&quot;,&quot;time&quot;,&quot;sex&quot;,&quot;height&quot;,&quot;shoe_size&quot; &quot;1&quot;,&quot;04.10.2016 17:58:51&quot;,NA,160.1,40 &quot;2&quot;,&quot;04.10.2016 17:58:59&quot;,&quot;woman&quot;,171.2,39 &quot;3&quot;,&quot;04.10.2016 18:00:15&quot;,&quot;woman&quot;,174.2,39 &quot;4&quot;,&quot;04.10.2016 18:01:17&quot;,&quot;woman&quot;,176.4,40 &quot;5&quot;,&quot;04.10.2016 18:01:22&quot;,&quot;man&quot;,195.2,46 Erkenenn Sie das Muster? Die erste Zeile gibt die “Spaltenköpfe” wieder, also die Namen der Variablen. Hier sind es 5 Spalten; die vierte heißt “shoe_size”. Die Spalten sind offenbar durch Komma , voneinander getrennt. Dezimalstellen sind in amerikanischer Manier mit einem Punkt . dargestellt. Die Daten sind “rechteckig”; alle Spalten haben gleich viele Zeilen und umgekehrt alle Spalten gleich viele Zeilen. Man kann sich diese Tabelle gut als Excel-Tabelle mit Zellen vorstellen, in denen z.B. “ID” (Zelle oben links) oder “46” (Zelle unten rechts) steht. An einer Stelle steht NA. Das ist Errisch für “fehlender Wert”. Häufig wird die Zelle auch leer gelassen, um auszudrücken, dass ein Wert hier fehlt (hört sich nicht ganz doof an). Aber man findet alle möglichen Ideen, um fehlende Werte darzustellen. Ich rate von allen anderen ab; führt nur zu Verwirrung. Lesen wir diese Daten jetzt ein: if (!file.exists(&quot;./data/wo_men.csv&quot;)){ daten &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/wo_men.csv&quot;) } else { daten &lt;- read.csv(&quot;./data/wo_men.csv&quot;) } head(daten) #&gt; X time sex height shoe_size #&gt; 1 1 04.10.2016 17:58:51 woman 160 40 #&gt; 2 2 04.10.2016 17:58:59 woman 171 39 #&gt; 3 3 04.10.2016 18:00:15 woman 174 39 #&gt; 4 4 04.10.2016 18:01:17 woman 176 40 #&gt; 5 5 04.10.2016 18:01:22 man 195 46 #&gt; 6 6 04.10.2016 18:01:53 woman 157 37 Wir haben zuerst geprüft, ob die Datei (wo_men.csv) im entsprechenden Ordner existiert oder nicht (das !-Zeichen heißt auf Errisch “nicht”). Falls die Datei nicht im Ordner existiert, laden wir sie mit read.csv herunter und direkt ins R hinein. Andernfalls (else) lesen wir sie direkt ins R hinein. Der Befehl read.csv liest also eine CSV-Datei, was uns jetzt nicht übermäßig überrascht. Aber Achtung: Wenn Sie aus einem Excel mit deutscher Einstellung eine CSV-Datei exportieren, wird diese CSV-Datei als Trennzeichen ; (Strichpunkt) und als Dezimaltrennzeichen , verwenden. Da der Befehl read.csv als Standard mit Komma und Punkt arbeitet, müssen wir die deutschen Sonderlocken explizit angeben, z.B. so: # daten_deutsch &lt;- read.csv(&quot;daten_deutsch.csv&quot;, sep = &quot;;&quot;, dec = &quot;.&quot;) Dabei steht sep (separator) für das Trennzeichen zwischen den Spalten und dec für das Dezimaltrennzeichen. R bietet eine Kurzfassung für read.csv mit diesen Parametern: read.csv2(&quot;daten_deutsch.csv&quot;). Übrigens: Wenn Sie keinen Pfad angeben, so geht R davon aus, dass die Daten im aktuellen Verzeichnis liegen. Das aktuelle Verzeichnis kann man mit getwd() erfragen und mit setwd() einstellen. Komfortabler ist es aber, das aktuelle Verzeichnis per Menü zu ändern. In RStudio: Session &gt; Set Working Directory &gt; Choose Directory ... (oder per Shortcut, der dort angezeigt wird). 2.2 Normalform einer Tabelle Tabellen in R werden als data frames (“Dataframe” auf Denglisch; moderner: als tibble, Tibble kurz für “Table-df”) bezeichnet. Tabellen sollten in “Normalform” vorliegen, bevor wir weitere Analysen starten. Unter Normalform verstehen sich folgende Punkte: Es handelt sich um einen data frame, also Spalten mit Namen und gleicher Länge; eine Datentabelle in rechteckiger Form In jeder Zeile steht eine Beobachtung, in jeder Spalte eine Variable Fehlende Werte sollten sich in leeren Tabellen niederschlagen Daten sollten nicht mit Farkbmarkierungen o.ä. kodiert werden keine Leerzeilen, keine Leerspalten am besten keine Sonderzeichen verwenden und keine Leerzeichen in Variablennamen und -werten, am besten nur Ziffern und Buchstaben und Unterstriche Variablennamen dürfen nicht mit einer Zahl beginnen Der Punkt “Jede Zeile eine Beobachtung, jede Spalte eine Variable” verdient besondere Beachtung. Betrachen Sie dieses Beispiel: knitr::include_graphics(&quot;./images/breit_lang.pdf&quot;) In der rechten Tabelle sind die Variablen Quartal und Umsatz klar getrennt; jede hat ihre eigene Spalte. In der linken Tabelle hingegen sind die beiden Variablen vermischt. Sie haben nicht mehr ihre eigene Spalte, sondern sind über vier Spalten verteilt. Die rechte Tabelle ist ein Beispiel für eine Tabelle in Normalform, die linke nicht. Abbildung 2.3: Illustration eines Datensatzes in Normalform Eine der ersten Aktionen einer Datenanalyse sollte also die “Normalisierung” Ihrer Tabelle sein. In R bietet sich dazu das Paket tidyr an, mit dem die Tabelle von Breit- auf Langformat (und wieder zurück) geschoben werden kann. Ein Beispiel dazu: meindf &lt;- read.csv(&quot;http://stanford.edu/~ejdemyr/r-tutorials/data/unicef-u5mr.csv&quot;) df_lang &lt;- gather(meindf, year, u5mr, U5MR.1950:U5MR.2015) df_lang &lt;- separate(df_lang, year, into = c(&quot;U5MR&quot;, &quot;year&quot;), sep = &quot;.&quot;) Die erste Zeile liest die Daten aus einer CSV-Datei ein; praktischerweise direkt von einer Webseite. Die zweite Zeile formt die Daten von breit nach lang um. Die neuen Spalten, nach der Umformung heißen dann year und u5mr (Sterblichkeit bei Kindern unter fünf Jahren). In die Umformung werden die Spalten U5MR 1950 bis U5MR 2015 einbezogen. Die dritte Zeile “entzerrt” die Werte der Spalte year; hier stehen die ehemaligen Spaltenköpfe. Man nennt sie auch key Spalte daher. Steht in einer Zelle von year bspw. U5MR 1950, so wird U5MR in eine Spalte mit Namen U5MR und 1950 in eine Spalte mit Namen year geschrieben. Im Cheatsheet von RStudio zum Thema Datenimport finden sich nützliche Hinweise.8 2.3 Verweise R for Data Science bietet umfangreiche Unterstützung zu diesem Thema (Wickham and Grolemund 2016). References "],
["daten-aufbereiten.html", "Kapitel 3 Daten aufbereiten 3.1 Typische Probleme 3.2 Daten aufbereiten mit dplyr 3.3 Fallstudie nycflights13 3.4 Checkliste zum Datenjudo 3.5 Verweise", " Kapitel 3 Daten aufbereiten Abbildung 3.1: Daten aufbereiten In diesem Kapitel benötigte Pakete: library(tidyverse) # Datenjudo library(corrr) # Korrelationen berechnen mit der Pfeife library(stringr) # Texte bearbeiten library(car) # für &#39;recode&#39; library(nycflights13) # Datensatz &#39;flights&#39; library(knitr) # für HTML-Tabellen library(gridExtra) # für Mehrfachplots Unter Daten aufbereiten im engeren Sinne ist gemeint, die Daten einer “Grundreinigung” zu unterziehen, dass sie für weitere Analysen in geeigneter Form sind. Daten zusammenfassen meint die deskriptive Statistik; Daten visualisieren ist das Erstellen von Diagrammen. Im Anschluss kann man die Daten modellieren. Ist das Explorieren von Daten auch nicht statistisch anspruchsvoll, so ist es trotzdem von großer Bedeutung und häufig recht zeitintensiv, vor allem das Daten aufbereiten. Eine Anekdote zur Relevanz der Exploration, die (so will es die Geschichte) mir an einer Bar nach einer einschlägigen Konferenz erzählt wurde (daher keine Quellenangebe, Sie verstehen…). Eine Computerwissenschaftlerin aus den USA (deutschen Ursprungs) hatte einen beeindruckenden “Track Record” an Siegen in Wettkämpfen der Datenanalyse. Tatsächlich hatte sie keine besonderen, raffinierten Modellierungstechniken eingesetzt; klassische Regression war ihre Methode der Wahl. Bei einem Wettkampf, bei dem es darum ging, Krebsfälle aus Krankendaten vorherzusagen (z.B. Röntgenbildern) fand sie nach langem Datenjudo heraus, dass in die “ID-Variablen” Information gesickert war, die dort nicht hingehörte und die sie nutzen konnte für überraschend (aus Sicht der Mitstreiter) gute Vorhersagen zu Krebsfällen. Wie war das möglich? Die Daten stammten aus mehreren Kliniken, jede Klinik verwendete ein anderes System, um IDs für Patienten zu erstellen. Überall waren die IDs stark genug, um die Anonymität der Patienten sicherzustellen, aber gleich wohl konnte man (nach einigem Judo) unterscheiden, welche ID von welcher Klinik stammte. Was das bringt? Einige Kliniken waren reine Screening-Zentren, die die Normalbevölkerung versorgte. Dort sind wenig Krebsfälle zu erwarten. Andere Kliniken jedoch waren Onkologie-Zentren für bereits bekannte Patienten oder für Patienten mit besonderer Risikolage. Wenig überraschen, dass man dann höhere Krebsraten vorhersagen kann. Eigentlich ganz einfach; besondere Mathe steht hier (zumindest in dieser Geschichte) nicht dahinter. Und, wenn man den Trick kennt, ganz einfach. Aber wie so oft ist es nicht leicht, den Trick zu finden. Sorgfältiges Datenjudo hat hier den Schlüssel zum Erfolg gebracht. 3.1 Typische Probleme Bevor man seine Statistik-Trickkiste so richtig schön aufmachen kann, muss man die Daten häufig erst noch in Form bringen. Das ist nicht schwierig in dem Sinne, dass es um komplizierte Mathe ginge. Allerdings braucht es mitunter recht viel Zeit und ein paar (oder viele) handwerkliche Tricks sind hilfreich. Hier soll das folgende Kapitel helfen. Mit “Datenjudo” (ein Fachbegriff aus der östlichen Zahlentheorie) ist gemeint, die Daten so “umzuformen”, “aufzubereiten”, oder “reinigen” , dass sie passend für statistische Analysen sind. Typische Probleme, die immer wieder auftreten, sind: Fehlende Werte: Irgend jemand hat auf eine meiner schönen Fragen in der Umfrage nicht geantwortet! Unerwartete Daten: Auf die Frage, wie viele Facebook-Freunde er oder sie habe, schrieb die Person “I like you a lot”. Was tun??? Daten müssen umgeformt werden: Für jede der beiden Gruppen seiner Studie hat Joachim einen Google-Forms-Fragebogen aufgesetzt. Jetzt hat er zwei Tabellen, die er “verheiraten” möchte. Geht das? Neue Spalten berechnen: Ein Student fragt nach der Anzahl der richtigen Aufgaben in der Statistik-Probeklausur. Wir wollen helfen und im entsprechenden Datensatz eine Spalte erzeugen, in der pro Person die Anzahl der richtig beantworteten Fragen steht. 3.2 Daten aufbereiten mit dplyr Es gibt viele Möglichkeiten, Daten mit R aufzubereiten; dplyr ist ein populäres Paket dafür. Eine zentrale Idee von dplyr ist, dass es nur ein paar wenige Grundbausteine geben sollte, die sich gut kombinieren lassen. Sprich: Wenige grundlegende Funktionen mit eng umgrenzter Funktionalität. Der Autor, Hadley Wickham, sprach einmal in einem Forum (citation needed), dass diese Befehle wenig können, das Wenige aber gut. Ein Nachteil dieser Konzeption kann sein, dass man recht viele dieser Bausteine kombinieren muss, um zum gewünschten Ergebnis zu kommen. Außerdem muss man die Logik des Baukastens gut verstanden habe - die Lernkurve ist also erstmal steiler. Dafür ist man dann nicht darauf angewiesen, dass es irgendwo “Mrs Right” gibt, die genau das kann, so wie ich das will. Außerdem braucht man sich auch nicht viele Funktionen merken. Es reicht einen kleinen Satz an Funktionen zu kennen (die praktischerweise konsistent in Syntax und Methodik sind). Willkommen in der Welt von dyplr! dplyr hat seinen Namen, weil es sich ausschließlich um Dataframes bemüht; es erwartet einen Dataframe als Eingabe und gibt einen Dataframe zurück9. Diese Bausteine sind typische Tätigkeiten im Umgang mit Daten; nichts Überraschendes. Schauen wir uns diese Bausteine näher an. 3.2.1 Zeilen filtern mit filter Häufig will man bestimmte Zeilen aus einer Tabelle filtern. Zum Beispiel man arbeitet für die Zigarettenindustrie und ist nur an den Rauchern interessiert (die im Übrigen unser Gesundheitssystem retten (Krämer 2011)), nicht an Nicht-Rauchern; es sollen die nur Umsatzzahlen des letzten Quartals untersucht werden, nicht die vorherigen Quartale; es sollen nur die Daten aus Labor X (nicht Labor Y) ausgewertet werden etc. Ein Sinnbild: Abbildung 3.2: Zeilen filtern Merke: Die Funktion filter filtert Zeilen aus einem Dataframe. Schauen wir uns einige Beispiel an; zuerst die Daten laden nicht vergessen. Achtung: “Wohnen” die Daten in einem Paket, muss dieses Paket installiert sein, damit man auf die Daten zugreifen kann. data(profiles, package = &quot;okcupiddata&quot;) # Das Paket muss installiert sein df_frauen &lt;- filter(profiles, sex == &quot;f&quot;) # nur die Frauen df_alt &lt;- filter(profiles, age &gt; 70) # nur die alten df_alte_frauen &lt;- filter(profiles, age &gt; 70, sex == &quot;f&quot;) # nur die alten Frauen, d.h. UND-Verknüpfung df_nosmoke_nodrinks &lt;- filter(profiles, smokes == &quot;no&quot; | drinks == &quot;not at all&quot;) # liefert alle Personen, die Nicht-Raucher *oder* Nicht-Trinker sind Gar nicht so schwer, oder? Allgemeiner gesprochen werden diejenigen Zeilen gefiltert (also behalten bzw. zurückgeliefert), für die das Filterkriterium TRUE ist. Manche Befehle wie filter haben einen Allerweltsnamen; gut möglich, dass ein Befehl mit gleichem Namen in einem anderen (geladenen) Paket existiert. Das kann dann zu Verwirrungen führen - und kryptischen Fehlern. Im Zweifel den Namen des richtigen Pakets ergänzen, und zwar zum Beispiel so: dplyr::filter(...). Einige fortgeschrittene Beispiele für filter: Man kann alle Elemente (Zeilen) filtern, die zu einer Menge gehören und zwar mit diesem Operator: %in%: filter(profiles, body_type %in% c(&quot;a little extra&quot;, &quot;average&quot;)) Besonders Textdaten laden zu einigen Extra-Überlegungen ein; sagen wir, wir wollen alle Personen filtern, die Katzen bei den Haustieren erwähnen. Es soll reichen, wenn cat ein Teil des Textes ist; also likes dogs and likes cats wäre OK (soll gefiltert werden). Dazu nutzen wir ein Paket zur Bearbeitung von Strings (Textdaten): filter(profiles, str_detect(pets, &quot;cats&quot;)) Ein häufiger Fall ist, Zeilen ohne fehlende Werte (NAs) zu filtern. Das geht einfach: profiles_keine_nas &lt;- na.omit(profiles) Aber was ist, wenn wir nur bei bestimmten Spalten wegen fehlender Werte besorgt sind? Sagen wir bei income und bei sex: filter(profiles, !is.na(income) | !is.na(sex)) 3.2.2 Spalten wählen mit select Das Gegenstück zu filter ist select; dieser Befehl liefert die gewählten Spalten zurück. Das ist häufig praktisch, wenn der Datensatz sehr “breit” ist, also viele Spalten enthält. Dann kann es übersichtlicher sein, sich nur die relevanten auszuwählen. Das Sinnbild für diesen Befehl: (#fig:fig-select, select_schema)Spalten auswählen Merke: Die Funktion select wählt Spalten aus einem Dataframe aus. if (!file.exists(&quot;./data/test_inf_short.csv&quot;)) { stats_test &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/test_inf_short.csv&quot;) } else { stats_test &lt;- read.csv(&quot;./data/test_inf_short.csv&quot;) } Hier haben wir erst geprüft, ob die Datei test_inf_short.csv existiert; falls nein, laden wir sie herunter. Andernfalls lesen wir sie aus dem lokalen Verzeichnis. select(stats_test, score) # Spalte `score` auswählen select(stats_test, score, study_time) # Splaten `score` und `study_time` auswählen select(stats_test, score:study_time) # dito select(stats_test, 5:6) Spalten 5 bis 6 auswählen Tatsächlich ist der Befehl select sehr flexibel; es gibt viele Möglichkeiten, Spalten auszuwählen. Im dplyr-Cheatsheet findet sich ein guter Überblick dazu.10 3.2.3 Zeilen sortieren mit arrange Man kann zwei Arten des Umgangs mit R unterscheiden: Zum einen der “interaktive Gebrauch” und zum anderen “richtiges Programmieren”. Im interaktiven Gebrauch geht es uns darum, die Fragen zum aktuell vorliegenden Datensatz (schnell) zu beantworten. Es geht nicht darum, eine allgemeine Lösung zu entwickeln, die wir in die Welt verschicken können und die dort ein bestimmtes Problem löst, ohne dass der Entwickler (wir) dabei Hilfestellung geben muss. “Richtige” Software, wie ein R-Paket oder Microsoft Powerpoint, muss diese Erwartung erfüllen; “richtiges Programmieren” ist dazu vonnöten. Natürlich sind in diesem Fall die Ansprüche an die Syntax (der “Code”, hört sich cooler an) viel höher. In dem Fall muss man alle Eventualitäten voraussehen und sicherstellen, dass das Programm auch beim merkwürdigsten Nutzer brav seinen Dienst tut. Wir haben hier, beim interaktiven Gebrauch, niedrigere Ansprüche bzw. andere Ziele. Beim interaktiven Gebrauch von R (oder beliebigen Analyseprogrammen) ist das Sortieren von Zeilen eine recht häufige Tätigkeit. Typisches Beispiel wäre der Lehrer, der eine Tabelle mit Noten hat und wissen will, welche Schüler die schlechtesten oder die besten sind in einem bestimmten Fach. Oder bei der Prüfung der Umsätze nach Filialen möchten wir die umsatzstärksten sowie -schwächsten Niederlassungen kennen. Ein R-Befehl hierzu ist arrange; einige Beispiele zeigen die Funktionsweise am besten: arrange(stats_test, score) %&gt;% head() # liefert die *schlechtesten* Noten zurück #&gt; X V_1 study_time self_eval interest score #&gt; 1 234 23.01.2017 18:13:15 3 1 1 17 #&gt; 2 4 06.01.2017 09:58:05 2 3 2 18 #&gt; 3 131 19.01.2017 18:03:45 2 3 4 18 #&gt; 4 142 19.01.2017 19:02:12 3 4 1 18 #&gt; 5 35 12.01.2017 19:04:43 1 2 3 19 #&gt; 6 71 15.01.2017 15:03:29 3 3 3 20 arrange(stats_test, -score) %&gt;% head() # liefert die *besten* Noten zurück #&gt; X V_1 study_time self_eval interest score #&gt; 1 3 05.01.2017 23:33:47 5 10 6 40 #&gt; 2 7 06.01.2017 14:25:49 NA NA NA 40 #&gt; 3 29 12.01.2017 09:48:16 4 10 3 40 #&gt; 4 41 13.01.2017 12:07:29 4 10 3 40 #&gt; 5 58 14.01.2017 15:43:01 3 8 2 40 #&gt; 6 83 16.01.2017 10:16:52 NA NA NA 40 arrange(stats_test, interest, score) %&gt;% head() #&gt; X V_1 study_time self_eval interest score #&gt; 1 234 23.01.2017 18:13:15 3 1 1 17 #&gt; 2 142 19.01.2017 19:02:12 3 4 1 18 #&gt; 3 221 23.01.2017 11:40:30 1 1 1 23 #&gt; 4 230 23.01.2017 16:27:49 1 1 1 23 #&gt; 5 92 17.01.2017 17:18:55 1 1 1 24 #&gt; 6 107 18.01.2017 16:01:36 3 2 1 24 Einige Anmerkungen. Die generelle Syntax lautet arrange(df, Spalte1, ...), wobei df den Dataframe bezeichnet und Spalte1 die erste zu sortierende Spalte; die Punkte ... geben an, dass man weitere Parameter übergeben kann. Am wichtigsten ist hier, dass man weitere Spalten übergeben kann. Dazu gleich mehr. Standardmäßig sortiert arrange aufsteigend (weil kleine Zahlen im Zahlenstrahl vor den großen Zahlen kommen). Möchte man diese Reihenfolge umdrehen (große Werte zuert), so kann man ein Minuszeichen vor den Namen der Spalte setzen. Gibt man zwei oder mehr Spalten an, so werden pro Wert von Spalte1 die Werte von Spalte2 sortiert etc; man betrachte den Output des Beispiels oben dazu. Aber was heißt dieses komisch Symbol: %&gt;%? Diese sogenannte “Pfeife” lässt sich mit “und dann” ins Deutshce übersetzen. Also: sortiere(diese_Tabelle, nach_dieser_Spalte) UND DANN zeig_die_ersten_Zeilen Der Befehl head zeigt dier ersten paar Zeilen eines Dataframes.11 Merke: Die Funktion arrange sortiert die Zeilen eines Datafames. Ein Sinnbild zur Verdeutlichung: Abbildung 3.3: Spalten sortieren Ein ähnliches Ergebnis erhält mit man top_n(), welches die n größten Ränge widergibt: top_n(stats_test, 3) #&gt; X V_1 study_time self_eval interest score #&gt; 1 3 05.01.2017 23:33:47 5 10 6 40 #&gt; 2 7 06.01.2017 14:25:49 NA NA NA 40 #&gt; 3 29 12.01.2017 09:48:16 4 10 3 40 #&gt; 4 41 13.01.2017 12:07:29 4 10 3 40 #&gt; 5 58 14.01.2017 15:43:01 3 8 2 40 #&gt; 6 83 16.01.2017 10:16:52 NA NA NA 40 #&gt; 7 116 18.01.2017 23:07:32 4 8 5 40 #&gt; 8 119 19.01.2017 09:05:01 NA NA NA 40 #&gt; 9 132 19.01.2017 18:22:32 NA NA NA 40 #&gt; 10 175 20.01.2017 23:03:36 5 10 5 40 #&gt; 11 179 21.01.2017 07:40:05 5 9 1 40 #&gt; 12 185 21.01.2017 15:01:26 4 10 5 40 #&gt; 13 196 22.01.2017 13:38:56 4 10 5 40 #&gt; 14 197 22.01.2017 14:55:17 4 10 5 40 #&gt; 15 248 24.01.2017 16:29:45 2 10 2 40 #&gt; 16 249 24.01.2017 17:19:54 NA NA NA 40 #&gt; 17 257 25.01.2017 10:44:34 2 9 3 40 #&gt; 18 306 27.01.2017 11:29:48 4 9 3 40 top_n(stats_test, 3, interest) #&gt; X V_1 study_time self_eval interest score #&gt; 1 3 05.01.2017 23:33:47 5 10 6 40 #&gt; 2 5 06.01.2017 14:13:08 4 8 6 34 #&gt; 3 43 13.01.2017 14:14:16 4 8 6 36 #&gt; 4 65 15.01.2017 12:41:27 3 6 6 22 #&gt; 5 110 18.01.2017 18:53:02 5 8 6 37 #&gt; 6 136 19.01.2017 18:22:57 3 1 6 39 #&gt; 7 172 20.01.2017 20:42:46 5 10 6 34 #&gt; 8 214 22.01.2017 21:57:36 2 6 6 31 #&gt; 9 301 27.01.2017 08:17:59 4 8 6 33 Gibt man keine Spalte an, so bezieht sich top_n auf die letzte Spalte im Datensatz. Da sich hier mehrere Personen den größten Rang (Wert 40) teilen, bekommen wir nicht 3 Zeilen zurückgeliefert, sondern entsprechend mehr. 3.2.4 Datensatz gruppieren mit group_by Einen Datensatz zu gruppieren ist ebenfalls eine häufige Angelegenheit: Was ist der mittlere Umsatz in Region X im Vergleich zu Region Y? Ist die Reaktionszeit in der Experimentalgruppe kleiner als in der Kontrollgruppe? Können Männer schneller ausparken als Frauen? Man sieht, dass das Gruppieren v.a. in Verbindung mit Mittelwerten oder anderen Zusammenfassungen sinnvol ist; dazu im nächsten Abschnitt mehr. Abbildung 3.4: Datensätze nach Subgruppen aufteilen In der Abbildung wurde der Datensatz anhand der Spalte Fach in mehrere Gruppen geteilt. Wir könnten uns als nächstes z.B. Mittelwerte pro Fach - d.h. pro Gruppe (pro Ausprägung von Fach) - ausgeben lassen; in diesem Fall vier Gruppen (Fach A bis D). test_gruppiert &lt;- group_by(stats_test, interest) test_gruppiert #&gt; Source: local data frame [306 x 6] #&gt; Groups: interest [7] #&gt; #&gt; X V_1 study_time self_eval interest score #&gt; &lt;int&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 05.01.2017 13:57:01 5 8 5 29 #&gt; 2 2 05.01.2017 21:07:56 3 7 3 29 #&gt; 3 3 05.01.2017 23:33:47 5 10 6 40 #&gt; # ... with 303 more rows Schaut man sich nun den Datensatz an, sieht man erstmal wenig Effekt der Gruppierung. R teilt uns lediglich mit Groups: interest [7], dass es die Gruppen gibt, aber es gibt keine extra Spalte oder sonstige Anzeichen der Gruppierung. Aber keine Sorge, wenn wir gleich einen Mittelwert ausrechnen, bekommen wir den Mittelwert pro Gruppe! Merke: Mit group_by teilt man einen Datensatz in Gruppen ein, entsprechend der Werte einer mehrerer Spalten. 3.2.5 Eine Spalte zusammenfassen mit summarise Vielleicht die wichtigste oder häufigte Tätigkeit in der Analyse von Daten ist es, eine Spalte zu einem Wert zusammenzufassen. Anders gesagt: Einen Mittelwert berechnen, den größten (kleinsten) Wert heraussuchen, die Korrelation berechnen oder eine beliebige andere Statistik ausgeben lassen. Die Gemeinsamkeit dieser Operaitonen ist, dass sie eine Spalte zu einem Wert zusammenfassen, “aus Spalte mach Zahl”, sozusagen. Daher ist der Name des Befehls summarise ganz passend. Genauer gesagt fasst dieser Befehl eine Spalte zu einer Zahl zusammen anhand einer Funktion wie mean oder max. Hierbei ist jede Funktion erlaubt, die eine Spalte als Input verlangt und eine Zahl zurückgibt; andere Funktionen sind bei summarise nicht erlaubt. Abbildung 3.5: Spalten zu einer Zahl zusammenfassen summarise(stats_test, mean(score)) #&gt; mean(score) #&gt; 1 31.1 Man könnte diesen Befehl so ins Deutsche übersetzen: Fasse aus Tabelle stats_test die Spalte score anhand des Mittelwerts zusammen. Nicht vergessen, wenn die Spalte score fehlende Werte hat, wird der Befehl mean standardmäßig dies mit NA quittieren. Jetzt können wir auch die Gruppierung nutzen: test_gruppiert &lt;- group_by(stats_test, interest) summarise(test_gruppiert, mean(score)) #&gt; # A tibble: 7 × 2 #&gt; interest `mean(score)` #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 28.3 #&gt; 2 2 29.7 #&gt; 3 3 30.8 #&gt; # ... with 4 more rows Der Befehl summarise erkennt also, wenn eine (mit group_by) gruppierte Tabelle vorliegt. Jegliche Zusammenfassung, die wir anfordern, wird anhand der Gruppierungsinformation aufgeteilt werden. In dem Beispiel bekommen wir einen Mittelwert für jeden Wert von interest. Interessanterweise sehen wir, dass der Mittelwert tendenziell größer wird, je größer interest wird. Alle diese dplyr-Befehle geben einen Dataframe zurück, was praktisch ist für weitere Verarbeitung. In diesem Fall heißen die Spalten interst und mean(score). Zweiter Name ist nicht so schön, daher ändern wir den wie folgt: Jetzt können wir auch die Gruppierung nutzen: test_gruppiert &lt;- group_by(stats_test, interest) summarise(test_gruppiert, mw_pro_gruppe = mean(score, na.rm = TRUE)) #&gt; # A tibble: 7 × 2 #&gt; interest mw_pro_gruppe #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 28.3 #&gt; 2 2 29.7 #&gt; 3 3 30.8 #&gt; # ... with 4 more rows Nun heißt die zweite Spalte mw_pro_Gruppe. na.rm = TRUE veranlasst, bei fehlenden Werten trotzdem einen Mittelwert zurückzuliefern (die Zeilen mit fehlenden Werten werden in dem Fall ignoriert). Grundsätzlich ist die Philosophie der dplyr-Befehle: “Mach nur eine Sache, aber die dafür gut”. Entsprechend kann summarise nur Spalten zusammenfassen, aber keine Zeilen. Merke: Mit summarise kann man eine Spalte eines Dataframes zu einem Wert zusammenfassen. 3.2.6 Zeilen zählen mit n und count Ebenfalls nützlich ist es, Zeilen zu zählen. Im Gegensatz zum Standardbefehle nrow versteht der dyplr-Befehl nauch Gruppierungen. n darf nur innerhalb von summarise oder ähnlichen dplyr-Befehlen verwendet werden. summarise(stats_test, n()) #&gt; n() #&gt; 1 306 summarise(test_gruppiert, n()) #&gt; # A tibble: 7 × 2 #&gt; interest `n()` #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 30 #&gt; 2 2 47 #&gt; 3 3 66 #&gt; # ... with 4 more rows nrow(stats_test) #&gt; [1] 306 Außerhalb von gruppierten Datensätzen ist nrow meist praktischer. Praktischer ist der Befehl count, der nichts anderes ist als die Hintereinanderschaltung von group_by und n. Mit count zählen wir die Häufigkeiten nach Gruppen; Gruppen sind hier zumeist die Werte einer auszuzählenden Variablen (oder mehrerer auszuzählender Variablen). Das macht count zu einem wichtigen Helfer bei der Analyse von Häufigkeitsdaten. dplyr::count(stats_test, interest) #&gt; # A tibble: 7 × 2 #&gt; interest n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 30 #&gt; 2 2 47 #&gt; 3 3 66 #&gt; # ... with 4 more rows dplyr::count(stats_test, study_time) #&gt; # A tibble: 6 × 2 #&gt; study_time n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 31 #&gt; 2 2 49 #&gt; 3 3 85 #&gt; 4 4 56 #&gt; 5 5 17 #&gt; 6 NA 68 dplyr::count(stats_test, interest, study_time) #&gt; Source: local data frame [29 x 3] #&gt; Groups: interest [?] #&gt; #&gt; interest study_time n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 12 #&gt; 2 1 2 7 #&gt; 3 1 3 8 #&gt; # ... with 26 more rows Allgemeiner formuliert lautet die Syntax: count(df, Spalte1, ...), wobei df der Dataframe ist und Spalte1 die erste (es können mehrere sein) auszuzählende Spalte. Gibt man z.B. zwei Spalten an, so wird pro Wert der 1. Spalte die Häufigkeiten der 2. Spalte ausgegeben. Merke: n und count zählen die Anzahl der Zeilen, d.h. die Anzahl der Fälle. 3.2.7 Die Pfeife Die zweite Idee kann man salopp als “Durchpfeifen” bezeichnen; ikonographisch mit diesem Symbol dargestellt %&gt;%12. Der Begriff “Durchpfeifen” ist frei vom Englischen “to pipe” übernommen. Hierbei ist gemeint, einen Datensatz sozusagen auf ein Fließband zu legen und an jedem Arbeitsplatz einen Arbeitsschritt auszuführen. Der springende Punkt ist, dass ein Dataframe als “Rohstoff” eingegeben wird und jeder Arbeitsschritt seinerseits wieder einen Datafram ausgiebt. Damit kann man sehr schön, einen “Flow” an Verarbeitung erreichen, außerdem spart man sich Tipparbeit und die Syntax wird lesbarer. Damit das Durchpfeifen funktioniert, benötigt man Befehle, die als Eingabe einen Dataframe erwarten und wieder einen Dataframe zurückliefern. Das Schaubild verdeutlich beispielhaft eine Abfolge des Durchpfeifens. Abbildung 3.6: Das ‘Durchpeifen’ Die sog. “Pfeife” (pipe: %&gt;%) in Anspielung an das berühmte Bild von René Magritte, verkettet Befehle hintereinander. Das ist praktisch, da es die Syntax vereinfacht. Vergleichen Sie mal diese Syntax filter(summarise(group_by(filter(stats_test, !is.na(score)), interest), mw = mean(score)), mw &gt; 30) mit dieser stats_test %&gt;% filter(!is.na(score)) %&gt;% group_by(interest) %&gt;% summarise(mw = mean(score)) %&gt;% filter(mw &gt; 30) #&gt; # A tibble: 4 × 2 #&gt; interest mw #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 3 30.8 #&gt; 2 5 32.5 #&gt; 3 6 34.0 #&gt; 4 NA 33.1 Es ist hilfreich, diese “Pfeifen-Syntax” in deutschen Pseudo-Code zu übersetzen. Nimm die Tabelle “stats_test” UND DANN filtere alle nicht-fehlenden Werte UND DANN gruppiere die verbleibenden Werte nach “interest” UND DANN bilde den Mittelwert (pro Gruppe) für “score” UND DANN liefere nur die Werte größer als 30 zurück. Die Pfeife zerlegt die “russische Puppe”, also ineinander verschachelteten Code, in sequenzielle Schritte und zwar in der richtigen Reihenfolge (entsprechend der Abarbeitung). Wir müssen den Code nicht mehr von innen nach außen lesen (wie das bei einer mathematischen Formel der Fall ist), sondern können wie bei einem Kochrezept “erstens …, zweitens .., drittens …” lesen. Die Pfeife macht die Syntax einfacher. Natürlich hätten wir die verschachtelte Syntax in viele einzelne Befehle zerlegen können und jeweils eine Zwischenergebnis speichern mit dem Zuweisungspfeil &lt;- und das Zwischenergebnis dann explizit an den nächsten Befehl weitergeben. Eigentlich macht die Pfeife genau das - nur mit weniger Tipparbeit. Und auch einfacher zu lesen. Flow! 3.2.7.1 Werte umkodieren und “binnen” mit car::recode Manchmal möchte man z.B. negativ gepolte Items umdrehen oder bei kategoriellen Variablen kryptische Bezeichnungen in sprechendere umwandeln (ein Klassiker ist 1 in maennlich bzw. 2 in weiblich oder umgekehrt, kann sich niemand merken). Hier gibt es eine Reihe praktischer Befehle, z.B. recode aus dem Paket car. Übrigens: Wenn man explizit angeben möchte, aus welchem Paket ein Befehl stammt (z.B. um Verwechslungen zu vermeiden), gibt man Paketnamen::Befehlnamen an. Schauen wir uns ein paar Beispiele zum Umkodieren an. stats_test$score_fac &lt;- car::recode(stats_test$study_time, &quot;5 = &#39;sehr viel&#39;; 2:4 = &#39;mittel&#39;; 1 = &#39;wenig&#39;&quot;, as.factor.result = TRUE) stats_test$score_fac &lt;- car::recode(stats_test$study_time, &quot;5 = &#39;sehr viel&#39;; 2:4 = &#39;mittel&#39;; 1 = &#39;wenig&#39;&quot;, as.factor.result = FALSE) stats_test$study_time &lt;- car::recode(stats_test$study_time, &quot;5 = &#39;sehr viel&#39;; 4 = &#39;wenig&#39;; else = &#39;Hilfe&#39;&quot;, as.factor.result = TRUE) head(stats_test$study_time) #&gt; [1] sehr viel Hilfe sehr viel Hilfe wenig Hilfe #&gt; Levels: Hilfe sehr viel wenig Der Befehle recode ist wirklich sehr prkatisch; mit : kann man “von bis” ansprechen (das ginge mit c() übrigens auch); else für “ansonsten” ist möglich und mit as.factor.result kann man entweder einen Faktor oder eine Text-Variable zurückgeliefert bekommen. Der ganze “Wechselterm” steht in Anführungsstrichen (&quot;). Einzelne Teile des Wechselterms sind mit einem Strichpunkt (;) voneinander getrennt. Das klassiche Umkodieren von Items aus Fragebögen kann man so anstellen; sagen wir interest soll umkodiert werden: stats_test$no_interest &lt;- car::recode(stats_test$interest, &quot;1 = 6; 2 = 5; 3 = 4; 4 = 3; 5 = 2; 6 = 1; else = NA&quot;) glimpse(stats_test$no_interest) #&gt; num [1:306] 2 4 1 5 1 NA NA 4 2 2 ... Bei dem Wechselterm muss man aufpassen, nichts zu verwechseln; die Zahlen sehen alle ähnlich aus… Testen kann man den Erfolg des Umpolens mit dplyr::count(stats_test, interest) #&gt; # A tibble: 7 × 2 #&gt; interest n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 30 #&gt; 2 2 47 #&gt; 3 3 66 #&gt; # ... with 4 more rows dplyr::count(stats_test, no_interest) #&gt; # A tibble: 7 × 2 #&gt; no_interest n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 9 #&gt; 2 2 45 #&gt; 3 3 41 #&gt; # ... with 4 more rows Scheint zu passen. Noch praktischer ist, dass man so auch numerische Variablen in Bereiche aufteilen kann (“binnen”): stats_test$Ergebnis &lt;- car::recode(stats_test$score, &quot;1:38 = &#39;durchgefallen&#39;; else = &#39;bestanden&#39;&quot;) Natürlich gibt es auch eine Pfeifen komptatible Version, um Variablen umzukodieren bzw. zu binnen: dplyr::recode13. Die Syntax ist allerdings etwas weniger komfortabel (da strenger), so dass wir an dieser Stelle bei car::recode bleiben. 3.3 Fallstudie nycflights13 Schauen wir uns einige Beispiele der Datenaufbereitung mittels dplyr an. Wir verwenden hier den Datensatz flightsaus dem Package nycflights13. Der Datensatz ist recht groß (~300.000 Zeilen und 19 Spalten); wenn man ihn als Excel importiert, kann eine alte Möhre von Computer schon in die Knie gehen. Beim Import als CSV habe ich noch nie von Problemen gehört; beim Import via Package ebenfalls nicht. Werfen wir einen ersten Blick in die Daten: data(flights) glimpse(flights) #&gt; Observations: 336,776 #&gt; Variables: 19 #&gt; $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... #&gt; $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... #&gt; $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... #&gt; $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 55... #&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 60... #&gt; $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2... #&gt; $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 7... #&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 7... #&gt; $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -... #&gt; $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;,... #&gt; $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79... #&gt; $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN... #&gt; $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;... #&gt; $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;... #&gt; $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138... #&gt; $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 94... #&gt; $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5,... #&gt; $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, ... #&gt; $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013... Der Befehl data lädt Daten aus einem zuvor gestarteten Paket. Achtung, Fallstudie. Sie sind der/die Assistent_in des Chefs der New Yorker Flughäfen. Ihr Chef kommt gut gelaunt ins Büro und sagt, dass er diesen Schnarchnasen einheizen wolle und sagt, sie sollen ihm mal schnell die Flüge mit der größten Verspätung raussuchen. Nix schickes, aber zacki-zacki… flights %&gt;% arrange(arr_delay) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 5 7 1715 1729 -14 1944 #&gt; 2 2013 5 20 719 735 -16 951 #&gt; 3 2013 5 2 1947 1949 -2 2209 #&gt; # ... with 3.368e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Hm, übersichtlicher wäre es wahrscheinllich, wenn wir weniger Spalten anschauen müssten. Am besten neben der Verspätung nur die Information, die wir zur Identifizierung der Schuldigen… will sagen der gesuchten Flüge benötigen flights %&gt;% arrange(arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) #&gt; # A tibble: 336,776 × 8 #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 -86 VX 5 7 1715 N843VA 193 SFO #&gt; 2 -79 VX 5 20 719 N840VA 11 SFO #&gt; 3 -75 UA 5 2 1947 N851UA 612 LAX #&gt; # ... with 3.368e+05 more rows Da Zahlen in ihrer natürlichen Form von klein nach groß sortiert sind, sortiert arrange in ebendieser Richtung. Wir können das umdrehen mit einem Minuszeichen vor der zu sortierenden Spalte: flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) #&gt; # A tibble: 336,776 × 8 #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1272 HA 1 9 641 N384HA 51 HNL #&gt; 2 1127 MQ 6 15 1432 N504MQ 3535 CMH #&gt; 3 1109 MQ 1 10 1121 N517MQ 3695 ORD #&gt; # ... with 3.368e+05 more rows Eine kleine Zugabe: Mit dem Befehl knitr::kable kann man einen Dateframe automatisch in eine (einigermaßen) schöne Tabelle ausgeben lassen. Oh halt, wir wollen keine Tabelle mit 300.000 Zeilen (der Chef ist kein Freund von Details). Also begrenzen wir die Ausgabe auf die ersten 10 Plätze. flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% filter(row_number() &lt; 11) %&gt;% kable() arr_delay carrier month day dep_time tailnum flight dest 1272 HA 1 9 641 N384HA 51 HNL 1127 MQ 6 15 1432 N504MQ 3535 CMH 1109 MQ 1 10 1121 N517MQ 3695 ORD 1007 AA 9 20 1139 N338AA 177 SFO 989 MQ 7 22 845 N665MQ 3075 CVG 931 DL 4 10 1100 N959DL 2391 TPA 915 DL 3 17 2321 N927DA 2119 MSP 895 DL 7 22 2257 N6716C 2047 ATL 878 AA 12 5 756 N5DMAA 172 MIA 875 MQ 5 3 1133 N523MQ 3744 ORD “Geht doch”, war die Antwort des Chefs, als sie die Tabelle rübergeben (er mag auch keine Emails). “Ach ja”, raunt der Chef, als Sie das Zimmer verlassen wollen, “hatte ich erwähnt, dass ich die gleiche Auswertung für jeden Carrier brauche? Reicht bis in einer halben Stunde”. Wir gruppieren also den Datensatz nach der Fluggesellschaft (carrier) und filtern dann die ersten 3 Zeilen (damit die Tabelle für den Chef nicht zu groß wird). Wie jeder dplyr-Befehl wird die vorherige Gruppierung berücksichtigt und daher die Top-3-Zeilen pro Gruppe, d.h. pro Fluggesellschaft, ausgegeben. flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% group_by(carrier) %&gt;% filter(row_number() &lt; 4) #&gt; Source: local data frame [48 x 8] #&gt; Groups: carrier [16] #&gt; #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1272 HA 1 9 641 N384HA 51 HNL #&gt; 2 1127 MQ 6 15 1432 N504MQ 3535 CMH #&gt; 3 1109 MQ 1 10 1121 N517MQ 3695 ORD #&gt; # ... with 45 more rows Vielleicht gefällt dem Chef diese Darstellung (sortiert nach carrier) besser: flights %&gt;% arrange(-arr_delay) %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% group_by(carrier) %&gt;% filter(row_number() &lt; 4) %&gt;% arrange(carrier) #&gt; Source: local data frame [48 x 8] #&gt; Groups: carrier [16] #&gt; #&gt; arr_delay carrier month day dep_time tailnum flight dest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 744 9E 2 16 757 N8940E 3798 CLT #&gt; 2 458 9E 7 24 1525 N927XJ 3538 MSP #&gt; 3 421 9E 7 10 2054 N937XJ 3325 DFW #&gt; # ... with 45 more rows Da Sie den Chef gut kennen, berechnen Sie gleich noch die durchschnittliche Verspätung pro Fluggesellschaft. flights %&gt;% select(arr_delay, carrier, month, day, dep_time, tailnum, flight, dest) %&gt;% group_by(carrier) %&gt;% summarise(delay_mean = mean(arr_delay, na.rm = TRUE)) %&gt;% arrange(-delay_mean) %&gt;% kable() carrier delay_mean F9 21.921 FL 20.116 EV 15.796 YV 15.557 OO 11.931 MQ 10.775 WN 9.649 B6 9.458 9E 7.380 UA 3.558 US 2.130 VX 1.764 DL 1.644 AA 0.364 HA -6.915 AS -9.931 Der Chef ist zufrieden. Sie können sich wieder wichtigeren Aufgaben zuwenden… 3.4 Checkliste zum Datenjudo Fassen wir einige wesentliche Arbeitsschritte der Datenaufbereitung zusammen14. 3.4.1 Auf fehlende Werte prüfen* Das geht recht einfach mit summarise(meine_daten). Der Befehl liefert für jede Spalte die Anzahl der fehlenden Werte zurück. wo_men &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/wo_men.csv&quot;) summary(wo_men) #&gt; time sex height shoe_size #&gt; 11.10.2016 12:31:59: 2 man :18 Min. : 2 Min. :35.0 #&gt; 11.10.2016 12:32:10: 2 woman:82 1st Qu.:163 1st Qu.:38.0 #&gt; 11.10.2016 12:32:32: 2 NA&#39;s : 1 Median :168 Median :39.0 #&gt; 11.10.2016 12:32:33: 2 Mean :165 Mean :39.8 #&gt; 11.10.2016 12:32:42: 2 3rd Qu.:174 3rd Qu.:40.0 #&gt; 04.10.2016 17:58:51: 1 Max. :364 Max. :88.0 #&gt; (Other) :90 NA&#39;s :1 NA&#39;s :1 3.4.2 Fälle mit fehlenden Werte löschen Weist eine Variable (Spalte) “wenig” fehlende Werte auf, so kann es schlau sein, nichts zu tun. Eine andere Möglichkeit besteht darin, alle entsprechenden Zeilen zu löschen. Man sollte aber schauen, wie viele Zeilen dadurch verloren gehen. nrow(wo_men) #&gt; [1] 101 wo_men %&gt;% na.omit %&gt;% nrow #&gt; [1] 100 Hier verlieren wir nur 1 Zeile, das verschmerzen wir. Welche eigentlich? wo_men %&gt;% rownames_to_column %&gt;% # Zeilennummer werden eine eigene Spalte filter(!complete.cases(.)) # Nur die nicht-kompletten Fälle filtern #&gt; rowname time sex height shoe_size #&gt; 1 86 11.10.2016 12:44:06 &lt;NA&gt; NA NA Man beachte, dass der Punkt . für den Datensatz steht, wie er vom letzten Schritt weitergegeben wurde. Natürlich könnten wir diesen Datensatz jetzt als neues Objekt speichern und damit weiter arbeiten. 3.4.3 Fehlende Werte ggf. ersetzen Ist die Anzahl der fehlenden Werte zu groß, als dass wir es verkraften könnten, die Zeilen zu löschen, so können wir die fehlenden Werte ersetzen. Allein, das ist ein weites Feld und übersteigt den Anspruch dieses Kurses15. Eine einfache, aber nicht die beste Möglichkeit, besteht darin, die fehlenden Werte durch einen repräsentativen Wert, z.B. den Mittelwert der Spalte, zu ersetzen. wo_men$height &lt;- replace(wo_men$height, is.na(wo_men$height), mean(wo_men$height, na.rm = TRUE)) replace ersetzt Werte aus dem Vektor wo_men$height alle Werte, für die is.na(wo_men$height) wahr ist. Diese Werte werden durch den Mittelwert der Spalte ersetzt16. 3.4.4 Nach Fehlern suchen Leicht schleichen sich Tippfehler oder andere Fehler ein. Man sollte darauf prüfen; so könnte man sich ein Histogramm ausgeben lassen pro Variable, um “ungewöhnliche” Werte gut zu erkennen. Meist geht das besser als durch das reine Betrachten von Zahlen. Gibt es wenig unterschiedliche Werte, so kann man sich auch die unterschiedlichen Werte ausgeben lassen. wo_men %&gt;% count(shoe_size) %&gt;% head # nur die ersten paar Zeilen #&gt; # A tibble: 6 × 2 #&gt; shoe_size n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 35.0 1 #&gt; 2 36.0 6 #&gt; 3 36.5 1 #&gt; 4 37.0 14 #&gt; 5 38.0 26 #&gt; 6 39.0 18 3.4.5 Ausreiser identifizieren Ähnlich zu Fehlern, steht man Ausreisern häufig skeptisch gegenüber. Allerdings kann man nicht pauschal sagen, das Extremwerte entfernt werden sollen: Vielleicht war jemand in der Stichprobe wirklich nur 1.20m groß? Hier gilt es, begründet und nachvollziehbar im Einzelfall zu entscheiden. Histogramme und Boxplots sind wieder ein geeignetes Mittel, um Ausreiser zu finden. 3.4.6 Hochkorrelierte Variablen finden Haben zwei Leute die gleiche Meinung, so ist einer von beiden überflüssig - wird behauptet. Ähnlich bei Variablen; sind zwei Variablen sehr hoch korreliert (&gt;.9, als grober (!) Richtwert), so bringt die zweite kaum Informationszuwachs zur ersten. Und kann ausgeschlossen werden. Oder man fasst ähnliche Variablen zusammen. wo_men %&gt;% select(height, shoe_size) %&gt;% correlate() -&gt; km # Korrelationsmatrix berechnen km #&gt; # A tibble: 2 × 3 #&gt; rowname height shoe_size #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 height NA 0.553 #&gt; 2 shoe_size 0.553 NA km %&gt;% shave() %&gt;% # Oberes Dreieck ist redundant, wird &quot;abrasiert&quot; rplot() # Korrelationsplot Die Funktion correlate stammt aus dem Paket corrr17, welches vorher installiert und geladen sein muss. Hier ist die Korrelation nicht zu groß, so dass wir keine weiteren Schritte unternehmen. 3.4.7 z-Standardisieren Für eine Reihe von Analysen ist es wichtig, die Skalierung der Variablen zur vereinheitlichen. Die z-Standardisierung ist ein übliches Vorgehen. Dabei wird der Mittelwert auf 0 transformiert und die SD auf 1; man spricht - im Falle von (hinreichend) normalverteilten Variablen - jetzt von der Standardnormalverteilung. Unterscheiden sich zwei Objekte A und B in einer standardnormalverteilten Variablen, so sagt dies nur etwas zur relativen Position von A zu B innerhalb ihrer Verteilung aus - im Gegensatz zu den Rohwerten. wo_men %&gt;% select_if(is.numeric) %&gt;% # Spalte nur auswählen, wenn numerisch scale() %&gt;% # z-standardisieren head() # nur die ersten paar Zeilen abdrucken #&gt; height shoe_size #&gt; [1,] -0.132 0.0405 #&gt; [2,] 0.146 -0.1395 #&gt; [3,] 0.221 -0.1395 #&gt; [4,] 0.272 0.0405 #&gt; [5,] 0.751 1.1204 #&gt; [6,] -0.208 -0.4994 Dieser Befehl liefert zwei z-standardisierte Spalten zurück. Kommoder ist es aber, alle Spalten des Datensatzes zurück zu bekommen, wobei zusätzlich die z-Werte aller numerischen Variablen hinzugekommen sind: wo_men %&gt;% mutate_if(is.numeric, funs(&quot;z&quot; = scale)) %&gt;% head #&gt; time sex height shoe_size height_z shoe_size_z #&gt; 1 04.10.2016 17:58:51 woman 160 40 -0.132 0.0405 #&gt; 2 04.10.2016 17:58:59 woman 171 39 0.146 -0.1395 #&gt; 3 04.10.2016 18:00:15 woman 174 39 0.221 -0.1395 #&gt; 4 04.10.2016 18:01:17 woman 176 40 0.272 0.0405 #&gt; 5 04.10.2016 18:01:22 man 195 46 0.751 1.1204 #&gt; 6 04.10.2016 18:01:53 woman 157 37 -0.208 -0.4994 Der Befehl mutate berechnet eine neue Spalte; mutate_if tut dies, wenn die Spalte numerisch ist. Die neue Spalte wird berechnet als z-Transformierung der alten Spalte; zum Spaltenname wird ein “_z&quot; hinzugefügt. Natürlich hätten wir auch mit select “händisch” die relevanten Spalten auswählen können. 3.4.8 Quasi-Konstante finden Hat eine Variable nur einen Wert, so verdient sie die Ehrenbezeichnung “Variable” nicht wirklich. Haben wir z.B. nur Männer im Datensatz, so kann das Geschlecht nicht für Unterschiede im Einkommen verantwortlich sein. Besser die Variable Geschlecht dann zu entfernen. Auch hier sind Histogramme oder Boxplots von Nutzen zur Identifiktion von (Quasi-)Konstanten. Alternativ kann man sich auch pro die Streuung (numerische Variablen) oder die Anzahl unterschiedlicher Werte (qualitative Variablen) ausgeben lassen. 3.4.9 Auf Normalverteilung prüfen Einige statistische Verfahren gehen von normalverteilten Variablen aus, daher macht es Sinn, Normalverteilung zu prüfen. Perfekte Normalverteilung ist genau so häufig, wie perfekte Kreise in der Natur. Entsprechend werden Signifikanztests, die ja auf perfekte Normalverteilung prüfen, immer signifikant sein, sofern die Stichrprobe groß genug ist. Daher ist meist zweckmäßiger, einen graphischen “Test” durchzuführen: Histogramm oder eine Dichte-Diagramm als “glatt geschmiergelte” Variante des Histogramms bieten sich an. wo_men %&gt;% ggplot() + aes(x = height) + geom_density() -&gt; p1 wo_men %&gt;% ggplot() + aes(x = shoe_size) + geom_density() -&gt; p2 grid.arrange(p1, p2, ncol = 2) Während die Körpergröße sehr deutlich normalverteilt ist, ist die Schuhgröße recht schief. Bei schiefen Verteilung können Transformationen Abhilfe schaffen. Hier erscheint die Schiefe noch erträglich, so dass wir keine weiteren Maßnahmen einleiten. 3.4.10 Mittelwerte pro Zeile berechnen Um Umfragedaten auszuwerten, will man häufig einen Mittelwert pro Zeile berechnen. Normalerweise fasst man eine Spalte zu einer Zahl zusammen; aber jetzt, fassen wir eine Zeile zu einer Zahl zusammen. Der häufigste Fall ist, wie gesagt, einen Mittelwert zu bilden für jede Person. Nehmen wir an, wir haben eine Befragung zur Extraversion durchgeführt und möchten jetzt den mittleren Extraversions-Wert pro Person (d.h. pro Zeile) berechnen. extra &lt;- read.csv(&quot;data/extra.csv&quot;) extra_items &lt;- extra %&gt;% select(i01:i10) extra$extra_mw &lt;- rowMeans(extra_items) Da der Datensatz über 28 Spalten verfügt, wir aber nur 10 Spalten heranziehen möchten, um Zeilen auf eine Zahl zusammenzufassen, bilden wir als Zwischenschritt einen “schmäleren” Datensatz, extra_items. Im Anschluss berechnen wir mit rowMeans die Mittelwerte pro Zeile (engl. “row”). 3.4.11 U extra %&gt;% count(n_faceb) 3.5 Verweise Eine schöne Demonstration der Mächtigkeit von dplyr findet sich hier18. Die GUI “exploratory” ist ein “klickbare” Umsetzung von dplyr, mächtig, modern und sieht cool aus: https://exploratory.io. R for Data Science bietet umfangreiche Unterstützung zu diesem Thema (Wickham and Grolemund 2016). References "],
["daten-visualisieren.html", "Kapitel 4 Daten visualisieren 4.1 Häufige Arten von Diagrammen 4.2 Farblehre 4.3 Prinzipien 4.4 Erweiterungen für ggplot 4.5 Fallstudie 4.6 Verweise", " Kapitel 4 Daten visualisieren Ein Bild sagt bekanntlich mehr als 1000 Worte. Schauen wir uns zur Verdeutlichung das berühmte Beispiel von Anscombe19 an. Es geht hier um vier Datensätze mit zwei Variablen (Spalten; X und Y). Offenbar sind die Datensätze praktisch identisch: Alle X haben den gleichen Mittelwert und die gleiche Varianz; dasselbe gilt für die Y. Die Korrelation zwischen X und Y ist in allen vier Datensätzen gleich. Allerdings erzählt eine Visualisierung der vier Datensätze eine ganz andere Geschichte. Offenbar “passieren” in den vier Datensätzen gänzlich unterschiedliche Dinge. Dies haben die Statistiken nicht aufgedeckt; erst die Visualisierung erhellte uns… Kurz: Die Visualisierung ist ein unverzichtbares Werkzeug, um zu verstehen, was in einem Datensatz (und damit in der zugrundeliengenden “Natur”) passiert. Es gibt viele Möglichkeiten, Daten zu visualieren (in R). Wir werden uns hier auf einen Weg bzw. ein Paket konzentrieren, der komfortabel, aber mächtig ist und gut zum Prinzip des Durchpfeifens passt: ggplot2. Laden wir dazu den Datensatz nycflights::flights. data(flights, package = &quot;nycflights13&quot;) qplot(x = carrier, y = arr_delay, geom = &quot;boxplot&quot;, data = flights) Offenbar gibt es viele Extremwerte, was die Verspätung betrifft. Das erscheint mir nicht unplausibel (Schneesturm im Winter, Flugzeug verschwunden…). Vor dem Hintergrund der Extremwerte erscheinen die mittleren Verspätungen (Mediane) in den Boxplots als ähnlich. Vielleicht ist der Unterschied zwischen den Monaten ausgeprägter? qplot(x = factor(month), y = arr_delay, geom = &quot;boxplot&quot;, data = flights) Kaum Unterschied; das spricht gegen die Schneesturm-Idee als Grund für Verspätung. Aber schauen wir uns zuerst die Syntax von qplot näher an. “q” in qplot steht für “quick”. Tatsächlich hat qplot einen großen Bruder, ggplot20, der deutlich mehr Funktionen aufweist - und daher auch die umfangreichere (=komplexere) Syntax. Fangen wir mit qplot an. Diese Syntax des letzten Beispiels ist recht einfach, nämlich: qplot (x = X_Achse, y = Y_Achse, data = mein_dataframe, geom = &quot;ein_geom&quot;) Wir definieren mit x, welche Variable der X-Achse des Diagramms zugewiesen werden soll, z.B. month; analog mit Y-Achse. Mit data sagen wir, in welchem Dataframe die Spalten “wohnen” und als “geom” ist die Art des statistischen “geometrischen Objects” gemeint, also Punkte, Linien, Boxplots, Balken… 4.1 Häufige Arten von Diagrammen Unter den vielen Arten von Diagrammen und vielen Arten, diese zu klassifizieren greifen wir uns ein paar häufige Diagramme heraus und schauen uns diese der Reihe nach an. 4.1.1 Eine kontinuierliche Variable Schauen wir uns die Verteilung der Schuhgrößen von Studierenden an. wo_men &lt;- read.csv(&quot;data/wo_men.csv&quot;) qplot(x = shoe_size, data = wo_men) Weisen wir nur der X-Achse (aber nicht der Y-Achse) eine kontinuierliche Variable zu, so wählt ggplot2 automatisch als Geom automatisch ein Histogramm; wir müssen daher nicht explizieren, dass wir ein Histogramm als Geom wünschen (aber wir könnten es hinzufügen). Alternativ wäre ein Dichtediagramm hier von Interesse: # qplot(x = shoe_size, data = wo_men) wie oben qplot(x = shoe_size, data = wo_men, geom = &quot;density&quot;) Was man sich merken muss, ist, dass hier nur das Geom mit Anführungsstrichen zu benennen ist, die übrigen Parameter ohne. Vielleicht wäre es noch schön, beide Geome zu kombinieren in einem Diagramm. Das ist etwas komplizierter; wir müssen zum großen Bruder ggplot umsteigen, da qplot nicht diese Funktionen anbietet. ggplot(data = wo_men) + aes(x = shoe_size) + geom_histogram(aes(y = ..density..), alpha = .7) + geom_density(color = &quot;blue&quot;) Zuerst haben wir mit dem Parameter data den Dateframe benannt. aes definiert, welche Variablen welchen Achsen (oder auch z.B. Füllfarben) zugewiesen werden. Hier sagen wir, dass die Schuhgröße auf X-Achse stehen soll. Das +-Zeichen trennt die einzelnen Bestandteile des ggplot-Aufrufs voneinander. Als nächstes sagen wir, dass wir gerne ein Histogram hätten: geom_histogram. Dabei soll aber nicht wie gewöhnlich auf der X-Achse die Häufigkeit stehen, sondern die Dichte. ggplot berechnet selbständig die Dichte und nennt diese Variable ..density..; die vielen Punkte sollen wohl klar machen, dass es sich nicht um eine “normale” Variable aus dem eigenen Dateframe handelt, sondern um eine “interne” Varialbe von ggplot - die wir aber nichtsdestotrotz verwenden können. alpha bestimmt die “Durchsichtigkeit” eines Geoms; spielen Sie mal etwas damit herum. Schließlich malen wir noch ein blaues Dichtediagramm über das Histogramm. Wünsche sind ein Fass ohne Boden… Wäre es nicht schön, ein Diagramm für Männer und eines für Frauen zu haben, um die Verteilungen vergleichen zu können? qplot(x = shoe_size, data = wo_men, geom = &quot;density&quot;, color = sex) qplot(x = shoe_size, data = wo_men, geom = &quot;density&quot;, fill = sex, alpha = I(.7)) Hier sollten vielleicht noch die Extremwerte entfernt werden, um den Blick auf das Gros der Werte nicht zu verstellen: wo_men %&gt;% filter(shoe_size &lt;= 47) -&gt; wo_men2 qplot(x = shoe_size, data = wo_men2, geom = &quot;density&quot;, fill = sex, alpha = I(.7)) Besser. Man kann das Durchpfeifen auch bis zu qplot weiterführen: wo_men %&gt;% filter(shoe_size &lt;= 47) %&gt;% qplot(x = shoe_size, data = ., geom = &quot;density&quot;, fill = sex, alpha = I(.7)) Die Pfeife versucht im Standard, das Endprodukt des lezten Arbeitsschritts an den ersten Parameter des nächsten Befehls weiterzugeben. Ein kurzer Blick in die Hilfe von qplot zeigt, dass der erste Parameter nicht data ist, sondern x. Daher müssen wir explizit sagen, an welchen Parameter wir das Endprodukt des lezten Arbeitsschritts geben wollen. Netterweise müssen wir dafür nicht viel tippen: Mit einem schlichten Punkt . können wir sagen “nimm den Dataframe, so wie er vom letzten Arbeitsschritt ausgegeben wurde”. Mit fill = sex sagen wir qplot, dass er für Männer und Frauen jeweils ein Dichtediagramm erzeugen soll; jedem Dichtediagramm wird dabei eine Farbe zugewiesen (die uns ggplot2 im Standard voraussucht). Mit anderen Worten: Die Werte von sex werden der Füllfarbe der Histogramme zugeordnet. Anstelle der Füllfarbe hätten wir auch die Linienfarbe verwenden können; die Syntax wäre dann: color = sex. 4.1.2 Zwei kontinuierliche Variablen Ein Streudiagramm ist die klassiche Art, zwei metrische Variablen darzustellen. Das ist mit qplot einfach: qplot(x = height, y = shoe_size, data = wo_men) Wir weisen wieder der X-Achse und der Y-Achse eine Variable zu; handelt es sich in beiden Fällen um Zahlen, so wählt ggplot2 automatisch ein Streudiagramm - d.h. Punkte als Geom (geom = &quot;point&quot;). Wir sollten aber noch die Extremwerte herausnehmen: wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% qplot(x = height, y = shoe_size, data = .) Der Trend ist deutlich erkennbar: Je größer die Person, desto länger die Füß´. Zeichnen wir noch eine Trendgerade ein. wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% qplot(x = height, y = shoe_size, data = .) + geom_smooth(method = &quot;lm&quot;) Synonym könnten wir auch schreiben: wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% ggplot() + aes(x = height, y = shoe_size) + geom_point() + geom_smooth(method = &quot;lm&quot;) Da ggplot als ersten Parameter die Daten erwartet, kann die Pfeife hier problemlos durchgereicht werden. Innerhalb eines ggplot-Aufrufs werden die einzelen Teile durch ein Pluszeichen + voneinander getrennt. Nachdem wir den Dataframe benannt haben, definieren wir die Zuweisung der Variablen zu den Achsen mit aes (“aes” wie “aesthetics”, also das “Sichtbare” eines Diagramms, die Achsen etc., werden definiert). Ein “Smooth-Geom” ist eine Linie, die sich schön an die Punkte anschmiegt, in diesem Falls als Gerade (lineares Modell, lm). Wenn man dies verdaut hat, wächst der Hunger nach einer Aufteilung in Gruppen. wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% qplot(x = height, y = shoe_size, color = sex, data = .) Mit color = sex sagen wir, dass die Linienfarbe (der Punkte) entsprechend der Stufen von sex eingefärbt werden sollen. Die genaue Farbwahl übernimmt ggplot2 für uns. 4.1.3 Eine diskrete Variable Bei diskreten Variablen, vor allem nominalen Variablen, geht es in der Regel darum, Häufigkeiten auszuzählen. Wie viele Männer und Frauen sind in dem Datensatz? qplot(x = sex, data = wo_men) Falls nur die X-Achse definiert ist und dort eine Faktorvariable oder eine Text-Variable steht, dann nimmt qplot automatisch ein Balkendiagramm als Geom. Entfernen wir vorher noch die fehlenden Werte: wo_men %&gt;% na.omit() %&gt;% qplot(x = sex, data = .) Wir könnten uns jetzt die Frage stellen, wie viele kleine und viele große Menschen es bei Frauen und bei den Männern gibt. Dazu müssen wir zuerst eine Variable wie “Größe gruppiert” erstellen mit zwei Werten: “klein” und “groß”. Nennen wir sie groesse_gruppe library(car) wo_men$groesse_gruppe &lt;- car::recode(wo_men$height, &quot;lo:175 = &#39;klein&#39;; else = &#39;gross&#39;&quot;) wo_men %&gt;% filter(height &gt; 150, height &lt; 210, shoe_size &lt; 55) %&gt;% na.omit -&gt; wo_men2 qplot(x = sex, fill = groesse_gruppe, data = wo_men2) In Worten sagt der recode-Befehl hier in etwa: “Kodiere wo_men$height um, und zwar vom kleinsten (lo) Wert bis 170 soll den Wert klein bekommen, ansonsten bekommt eine Größe den Wert gross”. Hier haben wir qplot gesagt, dass der die Balken entsprechend der Häufigkeit von groesse_gruppe füllen soll. Und bei den Frauen sind bei dieser Variablen die Wete klein häufig; bei den Männern hingegen die Werte gross. Schön wäre noch, wenn die Balken Prozentwerte angeben würden. Das geht mit qplot (so) nicht; wir schwenken auf ggplot um21. wo_men2 %&gt;% ggplot() + aes(x = sex, fill = groesse_gruppe) + geom_bar(position = &quot;fill&quot;) Die einzige Änderung in den Parametern ist position = &quot;fill&quot;. Dieser Parameter weist ggplot an, die Positionierung der Balken auf die Darstellung von Anteilen auszulegen. Damit haben alle Balken die gleiche Höhe, nämlich 100% (1). Aber die “Füllung” der Balken schwankt je nach der Häufigkeit der Werte von groesse_gruppe pro Balken (d.h. pro Wert von sex). Wir sehen, dass die Anteile von großen bzw. kleinen Menschen bei den beiden Gruppen (Frauen vs. Männer) unterschiedlich hoch ist. Dies spricht für einen Zusammenhang der beiden Variablen; man sagt, die Variablen sind abhängig (im statistischen Sinne). Je unterschiedlicher die “Füllhöhe”, desto stärker sind die Variablen (X-Achse vs. Füllfarbe) voneinander abhängig (bzw. desto stärker der Zusammenhang). 4.1.4 Zwei diskrete Variablen Arbeitet man mit nominalen Variablen, so sind Kontingenztabellen Täglich Brot. Z.B.: Welche Produkte wurden wie häufig an welchem Standort verkauft? Wie ist die Verteilung von Alkoholkonsum und Körperform bei Menschen einer Single-Börse. Bleiben wir bei letztem Beispiel. data(profiles, package = &quot;okcupiddata&quot;) profiles %&gt;% count(drinks, body_type) %&gt;% ggplot + aes(x = drinks, y = body_type, fill = n) + geom_tile() + theme(axis.text.x = element_text(angle = 90)) Was haben wir gemacht? Also: Nehme den Datensatz “profiles” UND DANN Zähle die Kombinationen von “drinks” und “body_type” UND DANN Erstelle ein ggplot-Plot UND DANN Weise der X-Achse “drinks” zu, der Y-Achse “body_type” und der Füllfarbe “n” UND DANN Male Fliesen UND DANN Passe das Thema so an, dass der Winkel für Text der X-Achse auf 90 Grad steht. Was sofort ins Auge sticht, ist dass “soziales Trinken”, nennen wir es mal so, am häfigsten ist, unabhängig von der Körperformm. Ansonsten scheinen die Zusammenhäng nicht sehr stark zu sein. 4.1.5 Zusammenfassungen zeigen Manchmal möchten wir nicht die Rohwerte einer Variablen darstellen, sondern z.B. die Mittelwerte pro Gruppe. Mittelwerte sind eine bestimmte Zusammenfassung einer Spalte; also fassen wir zuerst die Körpergröße zum Mittelwert zusammen - gruppiert nach Geschlecht. wo_men2 %&gt;% group_by(sex) %&gt;% summarise(Groesse_MW = mean(height)) -&gt; wo_men3 wo_men3 #&gt; # A tibble: 2 × 2 #&gt; sex Groesse_MW #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 man 183 #&gt; 2 woman 167 Diese Tabelle schieben wir jetzt in ggplot2; natürlich hätten wir das gleich in einem Rutsch durchpfeifen können. wo_men3 %&gt;% qplot(x = sex, y = Groesse_MW, data = .) Das Diagramm besticht nicht durch die Tiefe und Detaillierung. Wenn wir noch zusätzlich die Mittelwerte nach Groesse_Gruppe ausweisen, wird das noch überschaubar bleiben. wo_men2 %&gt;% group_by(sex, groesse_gruppe) %&gt;% summarise(Groesse_MW = mean(height)) %&gt;% qplot(x = sex, color = factor(groesse_gruppe), y = Groesse_MW, data = .) 4.2 Farblehre Erstens, nicht schaden - so könnte hier die Maßregel sein. Es ist leicht, zu grelle oder wenig kontrastierende Farben auszuwählen. Eine gute Farbauswahl (Palette) ist nicht so leicht und hängt vom Zweck der Darstellung ab. Cynthia Brewer22 hat einige schöne Farbpaletten zusammengestellt; diese sind in R und in ggplot2 über das Paket RcolorBrewer verfügbar. library(RColorBrewer) brewer.pal.info %&gt;% rownames_to_column %&gt;% rename(Name = rowname) %&gt;% kable Name maxcolors category colorblind BrBG 11 div TRUE PiYG 11 div TRUE PRGn 11 div TRUE PuOr 11 div TRUE RdBu 11 div TRUE RdGy 11 div FALSE RdYlBu 11 div TRUE RdYlGn 11 div FALSE Spectral 11 div FALSE Accent 8 qual FALSE Dark2 8 qual TRUE Paired 12 qual TRUE Pastel1 9 qual FALSE Pastel2 8 qual FALSE Set1 9 qual FALSE Set2 8 qual TRUE Set3 12 qual FALSE Blues 9 seq TRUE BuGn 9 seq TRUE BuPu 9 seq TRUE GnBu 9 seq TRUE Greens 9 seq TRUE Greys 9 seq TRUE Oranges 9 seq TRUE OrRd 9 seq TRUE PuBu 9 seq TRUE PuBuGn 9 seq TRUE PuRd 9 seq TRUE Purples 9 seq TRUE RdPu 9 seq TRUE Reds 9 seq TRUE YlGn 9 seq TRUE YlGnBu 9 seq TRUE YlOrBr 9 seq TRUE YlOrRd 9 seq TRUE Kontrastierende Darstellung (nominale/ qualitative Variablen) - z.B. Männer vs. Frauen display.brewer.all(type=&quot;qual&quot;) Sequenzielle Darstellung (unipolare numerische Variablen) - z.B. Preis oder Häufigkeit display.brewer.all(type=&quot;seq&quot;) Divergierende Darstellung (bipolare numerische Variablen) - z.B. semantische Potenziale oder Abstufung von “stimme überhaupt nicht zu” über “neutral” bis “stimme voll und ganz zu” display.brewer.all(type=&quot;div&quot;) In ggplot2 können wir folgendermaßen Paletten ändern. flights %&gt;% group_by(dest) %&gt;% count(dest) %&gt;% top_n(5) #&gt; # A tibble: 5 × 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ATL 17215 #&gt; 2 BOS 15508 #&gt; 3 LAX 16174 #&gt; 4 MCO 14082 #&gt; 5 ORD 17283 p1 &lt;- flights %&gt;% filter(dest %in% c(&quot;BOS&quot;, &quot;ATL&quot;, &quot;LAX&quot;)) %&gt;% ggplot() + aes(x = dest, y = arr_delay, color = dest) + geom_boxplot() + scale_color_brewer(palette = &quot;Set1&quot;) p2 &lt;- flights %&gt;% filter(dest %in% c(&quot;BOS&quot;, &quot;ATL&quot;, &quot;LAX&quot;, &quot;MCO&quot;, &quot;ORD&quot;)) %&gt;% ggplot() + aes(x = dest, y = arr_delay, fill = dest) + geom_boxplot() + scale_fill_brewer(palette = &quot;Set1&quot;) grid.arrange(p1, p2, ncol = 2) scale_color_brewer meint hier: “Ordne der Variablen, die für ‘color’ zuständig ist, hier sex, eine Farbe aus der Brewer-Palette ‘Set1’ zu”. Die Funktion wählt automatisch die richtige Anzahl von Farben. Man beachte, dass die Linienfarbe über color und die Füllfarbe über fill zugewiesen wird. Punkte haben nur eine Linienfarbe, keine Füllfarbe. Auch die Farbpaletten von Wes Anderson sind erbaulich23. Diese sind nicht “hart verdrahtet” in ggplot2, sondern werden über scale_XXX_manual zugewiesen (wobei XXX z.B. color oder fill sein kann). data(tips, package = &quot;reshape2&quot;) library(wesanderson) p1 &lt;- tips %&gt;% ggplot() + aes(x = total_bill, y = tip, color = day) + geom_point() + scale_color_manual(values = wes_palette(&quot;GrandBudapest&quot;)) + theme(legend.position = &quot;bottom&quot;) p2 &lt;- tips %&gt;% ggplot() + aes(x = total_bill, y = tip, color = day) + geom_point() + scale_color_manual(values = wes_palette(&quot;Chevalier&quot;)) + theme(legend.position = &quot;bottom&quot;) meine_farben &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;#009981&quot;, &quot;#32F122&quot;) p3 &lt;- tips %&gt;% ggplot() + aes(x = total_bill, y = tip, color = day) + geom_point() + scale_color_manual(values = meine_farben) + theme(legend.position = &quot;bottom&quot;) grid.arrange(p1, p2, p3, ncol = 3) Wer sich berufen fühlt, eigene Farben (oder die seiner Organisation zu verwenden), kommt auf ähnlichem Weg zu Ziel. Man definiere sich seine Palette, wobei ausreichend Farben definiert sein müssen. Diese weist man dann über scale_XXX_manual dann zu. Man kann einerseits aus den in R definierten Farben auswählen24 oder sich selber die RBG-Nummern (in Hexadezimal-Nummern) heraussuchen. 4.3 Prinzipien 4.4 Erweiterungen für ggplot 4.4.1 ggpairs Um eine Streudiagramm-Matrix darzustellen, ist der Befehl GGally::ggpairs praktisch: library(GGally) tips %&gt;% ggpairs(aes(color = sex), columns = c(&quot;total_bill&quot;, &quot;smoker&quot;, &quot;tip&quot;)) Dabei gibt man an, welche Variable (hier sex) für die Farben im Diagramm zuständig sein soll (wir ordnen den Werten von sex jeweils eine Farbe zu). Mit columns sagen wir, welche Spalten des Dataframes wir dargestellt haben möchten. Lassen wir diesen Parameter weg, so werden alle Spaltne des Dataframes dargestellt. 4.4.2 Weitere Hier25 finden sich viele weitere Ergänzungen für ggplot2. 4.5 Fallstudie Eine recht häufige Art von Daten in der Wirtschaft kommen von Umfragen in der Belegschaft. Diese Daten gilt es dann aufzubereiten und graphisch wiederzugeben. 4.5.1 Daten einlesen Hier laden wir einen Datensatz zu einer Online-Umfrage: data &lt;- read.csv(&quot;https://osf.io/meyhp/?action=download&quot;) Der DOI für diesen Datensatz ist 10.17605/OSF.IO/4KGZH. Der Datensatz besteht aus 10 Extraversions-Items (B5T nach Satow26) sowie einigen Verhaltenskorrelaten (zumindest angenommenen). Uns interessieren also hier nur die 10 Extraversions-Items, die zusammen Extraversion als Persönlichkeitseigenschaft messen (sollen). Wir werden die Antworte der Befragten darstelle, aber uns hier keine Gedanken über Messqualität u.a. machen. Die Umfrage kann hier27 eingesehen werden. Schauen wir uns die Daten mal an: glimpse(data) #&gt; Observations: 501 #&gt; Variables: 28 #&gt; $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ... #&gt; $ timestamp &lt;fctr&gt; 11.03.2015 19:17:48, 11.03.2015 19:18:05, ... #&gt; $ code &lt;fctr&gt; HSC, ERB, ADP, KHB, PTG, ABL, ber, hph, IH... #&gt; $ i01 &lt;int&gt; 3, 2, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 3... #&gt; $ i02r &lt;int&gt; 3, 2, 4, 3, 3, 2, 4, 3, 4, 4, 3, 4, 3, 3, 3... #&gt; $ i03 &lt;int&gt; 3, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 1... #&gt; $ i04 &lt;int&gt; 3, 2, 4, 4, 4, 4, 3, 3, 4, 4, 3, 3, 2, 4, 3... #&gt; $ i05 &lt;int&gt; 4, 3, 4, 3, 4, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3... #&gt; $ i06r &lt;int&gt; 4, 2, 1, 3, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3... #&gt; $ i07 &lt;int&gt; 3, 2, 3, 3, 4, 4, 2, 3, 3, 3, 2, 4, 2, 3, 3... #&gt; $ i08 &lt;int&gt; 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 4... #&gt; $ i09 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 2, 4, 4, 4... #&gt; $ i10 &lt;int&gt; 1, 1, 1, 2, 4, 3, 2, 1, 2, 3, 1, 3, 2, 3, 2... #&gt; $ n_facebook_friends &lt;int&gt; 250, 106, 215, 200, 100, 376, 180, 432, 200... #&gt; $ n_hangover &lt;int&gt; 1, 0, 0, 15, 0, 1, 1, 2, 5, 0, 1, 2, 20, 2,... #&gt; $ age &lt;int&gt; 24, 35, 25, 39, 29, 33, 24, 28, 29, 38, 25,... #&gt; $ sex &lt;fctr&gt; Frau, Frau, Frau, Frau, Frau, Mann, Frau, ... #&gt; $ extra_single_item &lt;int&gt; 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4... #&gt; $ time_conversation &lt;dbl&gt; 10, 15, 15, 5, 5, 20, 2, 15, 10, 10, 1, 5, ... #&gt; $ presentation &lt;fctr&gt; nein, nein, nein, nein, nein, ja, ja, ja, ... #&gt; $ n_party &lt;int&gt; 20, 5, 3, 25, 4, 4, 3, 6, 12, 5, 10, 5, 10,... #&gt; $ clients &lt;fctr&gt; , , , , , , , , , , , , , , , , , , , , , ... #&gt; $ extra_vignette &lt;fctr&gt; , , , , , , , , , , , , , , , , , , , , , ... #&gt; $ extra_description &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... #&gt; $ prop_na_per_row &lt;dbl&gt; 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0... #&gt; $ extra_mean &lt;dbl&gt; 2.9, 2.1, 2.6, 2.9, 3.2, 2.8, 2.8, 2.5, 3.2... #&gt; $ extra_median &lt;dbl&gt; 3.0, 2.0, 3.0, 3.0, 3.5, 3.0, 3.0, 2.5, 3.5... #&gt; $ client_freq &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... 4.5.2 Daten umstellen Wir haben ein Diagramm vor Augen (s.u.), bei dem auf der X-Achse die Items stehen (1,2,…,n) und auf der Y-Achse die Anzahl der Kreuze nach Kategorien. Viele Grafik-Funktionen sind nun so aufgebaut, dass auf der X-Achsen nur eine Variable steht. ggplot2, das wir hier verwenden, ist da keine Ausnahme. Wir müssen also die “breite” Tabelle (10 Spalten, pro Item eine) in eine “lange Spalte” umbauen: Eine Spalte heißt dann “Itemnummer” und die zweite “Wert des Items” oder so ähnlich. Also, los geht’s: Zuerst wählen wir aus der Fülle der Daten, die Spalten, die uns interessieren: Die 10 Extraversions-Items, in diesem Fall. data_items &lt;- select(data, i01:i10) Dann stellen wir die Daten von “breit” nach “lang” um, so dass die Items eine Variable bilden und damit für ggplot2 gut zu verarbeiten sind. data_long &lt;- gather(data_items, key = items, value = Antwort) data_long$Antwort &lt;- factor(data_long$Antwort) Den Befehl mit factor brauchen wir für zum Diagramm erstellen im Folgenden. Dieser Befehl macht aus den Zahlen bei der Variable Antwort eine nominale Variable (in R: factor) mit Text-Werten “1”, “2” und so weiter. Wozu brauchen wir das? Der Digrammbefehl unten kann nur mit nominalen Variablen Gruppierungen durchführen. Wir werden in dem Diagramm die Anzahl der Antworten darstellen - die Anzahl der Antworten nach Antwort-Gruppe (Gruppe mit Antwort “1” etc.). Keine Sorge, wenn sich das reichlich ungewöhnlich anhört. Sie müssen es an dieser Stelle nicht erfinden :-) Man gewöhnt sich daran einerseits; und andererseits ist es vielleicht auch so, dass diese Funktionen nicht perfekt sind, oder nicht aus unserer Sicht oder nur aus Sicht des Menschen, der die Funktion geschrieben hat. Jedenfalls brauchen wir hier eine factor Variable zur Gruppierung… Damit haben wir es schon! Jetzt wird gemalt. 4.5.3 Diagramme für Anteile Wir nutzen ggplot2, wie gesagt, und davon die Funktion qplot (q wie quick, nehme ich an.). ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) Was macht dieser ggplot Befehl? Schauen wir es uns in Einzelnen an: ggplot(data = ...): Wir sagen “Ich möchte gern die Funktion ggplot nutzen, um den Datensatz … zu plotten”. aes(...): Hier definieren wir die “aesthetics” des Diagramms, d.h. alles “Sichtbare”. Wir ordnen in diesem Fall der X-Achse die Variable items zu. Per Standardeinstellung geht ggplot davon aus, dass sie die Häufigkeiten der X-Werte auf der Y-Achse haben wollen, wenn Sie nichts über die Y-Achse sagen. Jetzt haben wir ein Koordinatensystem definiert (das noch leer ist). geom_bar(): “Hey R oder ggplot, jetzt male mal einen barplot in den ansonsten noch leeren plot”. aes(fill = Antwort): Genauer gesagt nutzen wir aes um einen sichtbaren Aspekte des Diagramms (wie die X-Achse) eine Variable des Datensatzes zuzuordnen. Jetzt sagen wir, dass die Füllung (im Balkendiagramm) durch die Werte von Antwort definiert sein sollen (also “1”, “2” etc.). position = &quot;fill&quot; sagt, dass die Gesamt-Höhe des Balken aufgeteilt werden soll mit den “Teil-Höhen” der Gruppen (Antwort-Kategorien 1 bis 4); wir hätten die Teil-Höhen auch nebeneinander stellen können. Vielleicht ist es schöner, die NAs erst zu entfernen. data_long &lt;- na.omit(data_long) Und dann noch mal plotten: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) 4.5.4 Um 90° drehen Dazu nehmen wir + coord_flip(), also “flippe das Koordinatensystem”. ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) + coord_flip() 4.5.5 Text-Labels für die Items Wir definieren die Texte (“Labels”) für die Items: item_labels &lt;- c(&quot;Ich bin das erste Item&quot;, &quot;Das zweite Item&quot;, &quot;Item 3 sdjfkladsjk&quot;, &quot;Ich bin ein krasser Couch-Potato UMKODIERT&quot;, &quot;i5 asf&quot;, &quot;i6 sdf&quot;, &quot;adfjks&quot;, &quot;sfjlkd&quot;, &quot;sdfkjl&quot;, &quot;sdfjkl&quot;) Jetzt hängen wir die Labels an die Items im Diagramm: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) + coord_flip() + scale_x_discrete(labels = item_labels) Man kann auch einen Zeilenumbruch in den Item-Labels erzwingen… wobei das führt uns schon recht weit, aber gut, zum Abschluss :-) item_labels &lt;- c(&quot;Ich bin das erste Item&quot;, &quot;Das zweite Item&quot;, &quot;Item 3 sdjfkladsjk&quot;, &quot;Ich bin ein krasser \\nCouch-Potato***mit Zeilenumbruch***&quot;, &quot;i5 asf&quot;, &quot;i6 sdf&quot;, &quot;adfjks&quot;, &quot;sfjlkd&quot;, &quot;sdfkjl&quot;, &quot;sdfjkl&quot;) Und wieder plotten: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;fill&quot;) + coord_flip() + scale_x_discrete(labels = item_labels, name = &quot;Extraversionsitems&quot;) + scale_y_continuous(name = &quot;Anteile&quot;) 4.5.6 Diagramm mit Häufigkeiten Ach so, schön wäre noch die echten Zahlen an der Y-Achse, nicht Anteile. Dafür müssen wir unseren Diagrammtyp ändern, bzw. die Art der Anordnung ändern. Mit position = &quot;fill&quot; wird der Anteil (also mit einer Summe von 100%) dargestellt. Wir können auch einfach die Zahlen/Häufigkeiten anzeigen, in dem wir die Kategorien “aufeinander stapeln” ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;stack&quot;) + coord_flip() + scale_x_discrete(labels = item_labels) 4.5.7 Farbschema Ja, die Wünsche hören nicht auf… Also, noch ein anderes Farbschema: ggplot(data = data_long) + aes(x = items) + geom_bar(aes(fill = Antwort), position = &quot;stack&quot;) + coord_flip() + scale_x_discrete(labels = item_labels) + scale_fill_brewer(palette = 17) 4.6 Verweise Edward Tufte gilt als Grand Seigneur der Datenvisualisierung; er hat mehrere lesenswerte Bücher zu dem Thema geschrieben (Tufte 2001; Tufte 2006; Tufte 1990). William Cleveland, ein amerikanischer Statistiker ist bekannt für seine grundlegenden, und weithin akzeptierten Ansätze für Diagramme, die die wesentliche Aussage schnörkellos transportieren (Cleveland 1993). References "],
["statistisches-modellieren.html", "Kapitel 5 Statistisches Modellieren 5.1 Was ist ein Modell? Was ist Modellieren? 5.2 Der p-Wert", " Kapitel 5 Statistisches Modellieren In diesem Kapitel benötigte Pakete: library(tidyverse) require(gridExtra) 5.1 Was ist ein Modell? Was ist Modellieren? Das Leben ist schwer… oder sagen wir: komplex. Um einen Ausschnitt der Wirklichkeit zu verstehen, erscheint es sinnvoll, sich einige als wesentlich erachteten Aspekte “herauszugreifen” bzw. auszusuchen und sich nur noch deren Zusammenspiel näher anzuschauen. Manche Aspekte der Wirklichkeit28 sind wirklicher als andere. Interessiert man sich für den Zusammenhang von Temperatur und Grundwasserspiegel, so sind diese Dinge direkt beobachtbar. Interessiert man sich hingegen für Lebensqualität und Zufriedenheit, so muss man diese Untersuchungsgegenstände erst konstruieren. Sprechen wir daher von Wirklichkeit lieber vorsichtiger vom Gegenstandsbereich, also den (konstruierten) Auszug der Wirklichkeit, für den sich die forschende Person interessiert. Abbildung 3.1: Modellieren Damit verstehen wir Modellieren als eine typische Aktivität von Menschen (Gigerenzer 1980), genauer eines Menschen mit einem bestimmten Ziel. Wir können gar nicht anders, als nur ein Modell unserer Umwelt zu machen. Vielfältige Medien kommen dazu in Frage: Bilder, Geschichten, Logik, Gleichungen. Wir werden uns hier auf numerische Modelle konzentrieren, weil es dort am einfachsten ist, die Informationen herauszuziehen. Schauen wir uns ein Beispiel aus der Datenanalyse an; laden Sie dazu zuerst den Datensatz wo_men. Abbildung 5.1: Ein Beispiel für Modellieren Im ersten Plot sehen wir - schon übersetzt in eine Datenvisualisierung - den Gegenstandsbereich. Dort sind einige Objekte zusammen mit ihren Relationen abgebildet (Gewicht vs. Körpergröße). Im zweiten Plot sehen wir ein (graphisches) Modell dazu. Noch ist das Modell recht unspezifisch; es wird nur postuliert, dass Körpergröße auf Schuhgröße einen Einfluss hat. Der rechte Plot spezifiziert nun diesen Einfluss: Es wird ein linearer Einfluss (eine Gerade) zwischen Größe und Schuhgröße unterstellt. Bei einem linearen Modell ist der Zuwachs der Ausgabevariablen konstant. Steigt eine Eingabevariable X um k, so steigt die Ausgabevariable ebenfalls um einen b*k, unabhängig vom Wert von X. Ein etwas aufwändigeres Modell könnte so aussehen: Abbildung 5.2: Ein etwas aufwändigeres Modell Allgemeiner formuliert, haben wir einen oder mehrere Eingabegrößen bzw. Prädiktoren{Prädiktoren}, von denen wir annehmen, dass sie einen Einfluss haben auf genau eine Zielgröße (Ausgabegröße) bzw. Kriterium. Modelle, wie wir sie betrachten werden, postulieren eine quantifizierbaren Zusammenhang zwischen diesen beiden Arten von Größen. Wir gehen dabei nicht davon aus, dass unsere Modelle perfekt sind, sondern dass Fehler passieren. Damit lassen sich unsere Modelle in drei Aspekte gliedern. Abbildung 5.3: Modelle mit schwarzer Kiste Die Einflussgrößen werden in einer “schwarzen Kiste”, die wir hier noch nicht näher benennen, irgendwie verwurstet, will sagen, verrechnet, so dass ein geschätzter Wert für das Kriterium, eine Vorhersage “hinten bei rauskommt”. Mathematischer ausgedrückt: \\[Y = f(X) + \\epsilon\\] Hier stehen \\(Y\\) für das Kriterium, \\(X\\) für den oder die Prädiktoren, \\(f\\) für die “schwarze Kiste” und \\(\\epsilon\\) für den Fehler, den wir bei unserer Vorhersage begehen. Die schwarze Kiste könnte man auch als die “datengenerierende Maschine” bezeichnen. Übrigens: Auf das Skalenniveau der Eingabe- bzw. Ausgabegrößen (qualitativ vs. quantitativ) kommt es hier nicht grundsätzlich an; es gibt Modelle für verschiedene Skalenniveaus bzw. Modelle, die recht anspruchslos sind hinsichtlich des Skalenniveaus (sowohl für Eingabe- als auch Ausgabegrößen). Was die Ausgabegröße (das Kriterium) betrifft, so “fühlen” qualitative Variablen von quantitativen Variablen anders an. Ein Beispiel zur Verdeutlichung: “Gehört Herr Bussi-Ness zur Gruppe der Verweigerer oder der Wichtigmacher?” (qualitatives Kriterium); “Wie hoch ist der Wichtigmacher-Score von Herrn Bussi-Ness?” (quantitatives Kriterium). Ein Modell mit qualitativem Kriterium bezeichnet man auch als Klassifikation; ein Modell mit quantitativem Kriterium bezeichnet man auch als Regression. Bei letzterem Begriff ist zu beachten, dass er doppelt verwendet wird. Neben der gerade genannten Bedeutung steht er auch für ein häufig verwendetes Modell - eigentlich das prototypische Modell - für quantitative Kriterien. 5.1.1 Ziele des Modellierens Man kann drei Arten von Zielen abgrenzen: Vorhersagen, Erklären und Reduzieren. Vorhersagen hat das Ziel, eine geschickte Black Box zu wählen (oder eine Black Box geschickt zu wählen), so dass der Vohersagefehler möglichst klein ist. Sicherlich wird der Vorhersagefehler nie Null sein. Das Innenleben der “schwarzen Kiste” interessiert uns hier nicht. Erklären bedeutet, zu verstehen, wie oder warum sich der Kriteriumswert so verändert, wie er es tut. Auf welche Art werden die Prädiktoren verrechnet, so dass eine bestimmter Kriteriumswert resultiert? Welche Präditkoren sind dabei (besonders) wichtig? Ist die Art der Verrechnung abhängig von den Werten der Prädiktoren? Hierbei interessiert uns vor allem die Beschaffenheit der schwarzen Kiste. Reduzieren meint, dass man die Fülle des Materials verringert, in dem man ähnliche Dinge zusammenfasst. Dabei kann man sowohl Observationen zusammen fassen (“Britta”, “Carla” und “Dina” zu “Frau” und “Joachim”, “Alois” und “Casper” zu “Mann”) oder auch Variablen zusammen fassen (“Frage 1”, “Frage 2” und “Frage 3” zu “Markentreue” etc.). Vorhersagen und Erklären haben gemein, dass Eingabegrößen genutzt werden, um Aussagen über einen Ausgabegröße zu treffen. Hat man einen Datensatz, so kann man prüfen, wie gut das Modell funktionert, also wie genau man die Ausgabewerte vorhergesagt hat. Das ist also eine Art “Lernen mit Anleitung” oder angeleitetes Lernen oder geleitetes Modellieren (engl. supervised learning). Abbildung @ref(fig: fig-blackbox) gibt diesen Fall wieder. Soll dem gegenüber das Ziel aber sein, die Daten zu reduzieren, also z.B. Kunden nach Persönlichkeit zu gruppieren, so ist die Lage anders. Es gibt keine Zielgröße. Wir wissen nicht, was die “wahre Kundengruppe” von Herrn Casper Bussi-Ness ist. Wir sagen eher, “OK, die drei Typen sind sich irgendwie ähnlich, sie werden wohl zum selben Typen von Kunden gehören”. Wir tappen in Dunkeln, was die “Warheit” ist. Unser Modell muss ohne Hinweise darauf, was richtig ist auskommen. Man spricht daher in diesem Fall von Lernen ohne Anleitung oder ungeleitetes Modellieren (engl. unsupervised learning). 5.1.2 Parametrische Modelle vs. non-parametrische Modelle Erklären wir das Innenleben der schwarzen Kiste genau, so spricht man von einem parametrischen Modell; spezifieren wir das Innenleben der schwarzen Kiste weniger genau, spricht man von einem non-parametrischen Modell29. Ein Beispiel für ein einfaches parametrisches Modell ist: Für je 5kg Körpergewicht steigt die Schuhgröße um 1 Nummer, wobei 50kg bei Schuhgröße 36 aufgehangen ist. Warum spricht man hier von Parametern? Die Vorhersage der Schuhgröße kann anhand von einer Einflussgröße, die klar benannt ist (Körpergewicht), berechnet werden. Wie gut das Modell ist (wie präzise die Vorhersage ist), ist jetzt erstmal egal. Das Modell ist sparsam, sagt man, in seiner Formulierung. Ein Beispiel für ein nicht-parametrisches Modell ist: Wenn Körpergewicht 50.00-50.01 dann Schuhgröße 36.01 Wenn Körpergewicht 50.02-50.03 dann Schuhgröße 36.02 Wenn Körpergewicht 50.04-50.05 dann Schuhgröße 36.03 Wenn … Dieses Modell ist in gewisser Weise auch recht einfach aufgebaut (wenig Hirnschmalz ist zum Verstehen nötig), aber andererseits recht komplex, da viel Platz nötig ist, dass Modell komplett aufzuschreiben. Außerdem: das Modell müsste jede seiner Zeilen mal anhand von Daten ausprobiert haben, um zu der jeweiligen Vorhersage zu kommen. Dafür sind viele Daten nötig. 5.1.3 Wann welches Modell? Tja, mit dieser Frage lässt sich ein Gutteil des Kopfzerbrechens in diesem Metier erfassen. Die einfache Antwort lautet: Es gibt kein “bestes Modell”, aber es mag für einen bestimmten Gegenstandsbereich, in einem bestimmten (historisch-kulturellen) Kontext, für ein bestimmtes Ziel und mit einer bestimmten Stichprobe ein best mögliches Modell geben. Dazu einige Eckpfeiler: Paramatrische Modelle sind häufig sparsamer als non-parametrische Modelle - hinsichtlich der Formulierung bzw. der Parameter und des Datenverbrauchs. Auch innerhalb der parametrischen bzw. non-parametrischen Modelle gibt es Unterschiede in der Sparsamkeit des Modells. Modelle mit weniger Parametern, also sparsame Modelle, sind grundsätzlich besser geeignet, wenn das Ziel Erklären ist. Wenn das Ziel Vorhersagen ist, so sollte man auch kompliziertere/ komplexe Modelle in Betracht ziehen. Man sollte stets mehrere Modelle vergleichen, um abzuschätzen, welches Modell in der aktuellen Situation geeigneter ist. 5.1.4 Einfache vs. komplexe Modelle Je komplexer ein Modell, desto besser passt sie meistens auf den Gegenstandsbereich. Eine grobe, holzschnittsartige Theorie ist doch schlechter als eine, die feine Nuancen berücksichtigt, oder nicht? Einiges spricht dafür; aber auch einiges dagegen. Schauen wir uns ein Problem mit komplexen Modellen an. Abbildung 5.4: Welches Modell passt am besten zu diesen Daten? Der 1. Plot (links) zeigt den Datensatz ohne Modell; der 2. Plot legt ein lineares Modell (Gerade) in die Daten. Der 3. Plot zeigt ein Modell, welches die Daten exakt erklärt - die Linie geht durch alle Punkte. Der 4. Plot zeigt ein Modell, welches die Punkte gut beschreibt, aber nicht exakt trifft. Welchem Modell würden Sie (am meisten) vertrauen? Das “blaue” Modell beschreibt die Daten sehr gut, aber hat das Modell überhaupt eine “Idee” vom Gegenstandsbereich, eine “Ahnung”, wie Y und X zusammenhängen, bzw. wie X einen Einfluss auf Y ausübt? Offenbar nicht. Das Modell ist “übergenau” oder zu komplex. Man spricht von Überanpassung (engl. overfitting). Das Modell scheint zufälliges, bedeutungsloses Rauschen zu ernst zu nehmen. Das Resultat ist eine zu wackelige Linie - ein schlechtes Modell, da wir wenig Anleitung haben, auf welche Y-Werte wir tippen müssten, wenn wir neue, unbekannte X-Werte bekämen. Was das “blaue Modell” zu detailverliebt ist, ist das “rote Modell” zu simpel. Die Gerade beschreibt die Y-Werte nur sehr schlecht. Man hätte gleich den Mittelwert von Y als Schätzwert für jedes einzelne \\(Y_i\\) hernehmen können. Dieses lineare Modell ist unterangepasst, könnte man sagen (engl. underfittting). Auch dieses Modell wird uns wenig helfen können, wenn es darum geht, zukünftige Y-Werte vorherzusagen (gegeben jeweils einen bestimmten X-Wert). Ah! Das grüne Modell scheint das Wesentliche, die “Essenz” der “Punktebewegung” zu erfassen. Nicht die Details, die kleinen Abweichungen, aber die “große Linie” scheint gut getroffen. Dieses Modell erscheint geeignet, zukünftige Werte gut zu beschreiben. Das grüne Modell ist damit ein Kompromiss aus Einfachheit und Komplexität und würde besonders passen, wenn es darum gehen sollte, zyklische Veränderungen zu erklären30. 5.1.5 Training- vs. Test-Stichprobe Wie wir gerade gesehen haben, kann man immer ein Modell finden, welches die vorhandenen Daten sehr gut beschreibt. Das gleicht der Tatsache, dass man im Nachhinein (also bei vorhandenen Daten) leicht eine Erklärung findet. Ob diese Erklärung sich in der Zukunft, bei unbekannten Daten bewahrheitet, steht auf einem ganz anderen Blatt. Daher sollte man immer sein Modell an einer Stichprobe entwickeln (“trainieren” oder “üben2) und an einer zweiten Stichprobe testen. Die erste Stichrpobe nennt man auch training sample (Training-Stichprobe) und die zweite test sample (Test-Stichprobe). Entscheidend ist, dass das Test-Sample beim Entwickeln des Modells unbekannt war. Die Güte des Modells sollte nur anhand eines - bislang nicht verwendeten - Test-Samples überprüft werden. Das Test-Sample darf bis zur Modellüberprüfung nicht analysiert werden. Die Modellgüte ist im Trainings-Sample meist deutlich besser als im Test-Sample. set.seed(42) train &lt;- wo_men %&gt;% sample_frac(.8, replace = FALSE) # Stichprobe von 80%, ohne Zurücklegen test &lt;- wo_men %&gt;% anti_join(train) # Alle Zeilen von &quot;wo_men&quot;, die nicht in &quot;train&quot; vorkommen Damit haben wir ein Trainings-Sample (train), in dem wir ein oder besser mehrere Modelle entwickeln können. Zur Beurteilung “wie gut” ein Modell ist, nehmen wir dann das Test-Sample (test). Dieses Vorgehen nennt man auch Kreuz-Validierung (engl. cross validation). So schön wie dieses Vorgehen auch ist, es ist nicht perfekt. Ein Nachteil ist, dass unsere Modellgüte wohl anders wäre, hätten wir andere Fälle im Test-Sample erwischt. Würden wir also ein neues Trainings-Sample und ein neues Test-Sample aus diesen Datensatz ziehen, so hätten wir wohl andere Ergebnisse. Was wenn diese Ergebnisse nun deutlich von den ersten abweichen? Dann würde unser Vertrauen in die die Modellgüte sinken. Wir bräuchten also noch ein Verfahren, welches Variabilität in der Modellgüte widerspiegelt. 5.2 Der p-Wert Abbildung 5.5: Der größte Statistiker des 20. Jahrhunderts (p &lt; .05) Der p-Wert ist die heilige Kuh der Forschung. Das ist nicht normativ, sondern deskriptiv gemeint. Der p-Wert entscheidet (häufig) darüber, was publiziert wird, und damit, was als Wissenschaft sichtbar ist - und damit, was Wissenschaft ist (wiederum deskriptiv, nicht normativ gemeint). Kurz: Dem p-Wert wird viel Bedeutung zugemessen. Abbildung 5.6: Der p-Wert wird oft als wichtig erachtet Was sagt uns der p-Wert? Eine gute intuitive Definition ist: Der p-Wert sagt, wie gut die Daten zur Nullhypothese passen. Je größer p, desto besser passen die Daten zur Nullhypothese. Allerdings hat der p-Wert seine Probleme. Vor allem: Er wird missverstanden. Jetzt kann man sagen, dass es dem p-Wert (dem armen) nicht anzulasten, dass andere/ einige ihm missverstehen. Auf der anderen Seite finde ich, dass sich Technologien dem Nutzer anpassen sollten (soweit als möglich) und nicht umgekehrt. Die (genaue) Definition des p-Werts ist aber auch so kompliziert, man kann sie leicht missverstehen: Der p-Wert gibt die Wahrscheinlichkeit P unserer Daten D an (und noch extremerer), unter der Annahme, dass die getestete Hypothese H wahr ist (und wenn wir den Versuch unendlich oft wiederholen würden, unter identischen Bedingungen und ansonsten zufällig). p = P(D|H) Viele Menschen - inkl. Professoren und Statistik-Dozenten - haben Probleme mit dieser Definition (Gigerenzer 2004). Das ist nicht deren Schuld: Die Definition ist kompliziert. Vielleicht denken viele, der p-Wert sage das, was tatsächlich interessant ist: die Wahrscheinlichkeit der (getesteten) Hypothese H, gegeben der Tatsache, dass bestimmte Daten D vorliegen. Leider ist das nicht die Definition des p-Werts. Also: \\[ P(D|H) \\ne P(H|D) \\] Der p-Wert ist für weitere Dinge kritisiert worden (Wagenmakers 2007, Briggs (2016)); z.B. dass die “5%-Hürde” einen zu schwachen Test für die getestete Hypothese bedeutet. Letzterer Kritikpunkt ist aber nicht dem p-Wert anzulasten, denn dieses Kriterium ist beliebig, könnte konservativer gesetzt werden und jegliche mechanisierte Entscheidungsmethode kann ausgenutzt werden. Ähnliches kann man zum Thema “P-Hacking” argumentieren (Head et al. 2015, Wicherts et al. (2016)); andere statistische Verfahren können auch gehackt werden. Ein wichtiger Anklagepunkt lautet, dass der p-Wert nicht nur eine Funktion der Effektgröße ist, sondern auch der Stichprobengröße. Sprich: Bei großen Stichproben wird jede Hypothese signifikant. Damit verliert der p-Wert an Nützlichkeit. Die Verteitigung argumentiert hier, dass das “kein Bug, sondern ein Feature” sei: Wenn man z.B. die Hypothese prüfe, dass der Gewichtsunteschied zwischen Männern und Frauen 0,000000000kg sei und man findet 0,000000123kg Unterschied, ist die getestete Hypothese falsch. Punkt. Der p-Wert gibt demnach das korrekte Ergebnis. Meiner Ansicht nach ist die Antwort zwar richtig, geht aber an den Anforderungen der Praxis vorbei. Meine Meinung ist, dass der p-Wert ein problematisch ist (und ein Dinosaurier) und nicht oder weniger benutzt werden sollte (das ist eine normative Aussage). Da der p-Wert aber immer noch der Platzhirsch auf vielen Forschungsauen ist, führt kein Weg um ihn herum. Er muss genau verstanden werden: Was er sagt und - wichtiger noch - was er nicht sagt. References "],
["geleitetes-modellieren.html", "Kapitel 6 Geleitetes Modellieren 6.1 Einfache lineare Regression 6.2 Klassifizierende Regression 6.3 Baumbasierte Verfahren 6.4 Ausblick 6.5 Fallstudie: Überleben auf der Titanic", " Kapitel 6 Geleitetes Modellieren 6.1 Einfache lineare Regression Wir werden weiter den Datensatz tips analysieren (Bryant and Smith 1995). Sofern noch nicht geschehen, können Sie in hier als csv-Datei herunterladen: tips &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/tips.csv&quot;) Zur Unterstützung der Analyse wird (wieder) das Paket mosaic verwendet; außerdem laden wir ggplot2 für qplot: library(mosaic) library(ggplot2) Wie hängen Trinkgeldhöhe tip und Rechnungshöhe total_bill zusammen? Kann die Höhe des Trinkgeldes als lineare Funktion der Rechnungshöhe linear modelliert werden? \\[tip_i=\\beta_0+\\beta_1\\cdot total\\_bill_i+\\epsilon_i\\] Zunächst eine visuelle Analyse mi Hilfe eines Scatterplots. qplot(y = tip, x = total_bill, data = tips) Es scheint einen positiven Zusammenhang zu geben. Modellieren wir die abhängige Variable tip (inhaltliche Entscheidung!) als lineare Funktion der unabhängigen Variable total_bill: LinMod.1 &lt;- lm(tip ~ total_bill, data=tips) summary(LinMod.1) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.198 -0.565 -0.097 0.486 3.743 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.92027 0.15973 5.76 2.5e-08 *** #&gt; total_bill 0.10502 0.00736 14.26 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 242 degrees of freedom #&gt; Multiple R-squared: 0.457, Adjusted R-squared: 0.454 #&gt; F-statistic: 203 on 1 and 242 DF, p-value: &lt;2e-16 Der Achsenabschnitt (intercept) wird mit 0.92 geschätzt, die Steigung in Richtung total_bill mit 0.11: steigt total_bill um einen Dollar, steigt im Durchschnitt tip um 0.11. Die (Punkt-)Prognose für tip lautet also tip = 0.92 + 0.11 * total_bill Die Koeffzienten werden dabei so geschätzt, dass \\(\\sum \\epsilon_i^2\\) minimiert wird. Dies wird auch als Kleinste Quadrate (Ordinary Least Squares, OLS) Kriterium bezeichnet. Eine robuste Regression ist z. B. mit der Funktion rlm() aus dem Paket MASS möglich. In mosaic kann ein solches Modell einfach als neue Funktion definiert werden: LinMod.1Fun &lt;- makeFun(LinMod.1) Die (Punkt-)Prognose für die Trinkgeldhöhe, bspw. für eine Rechnung von 30$ kann dann berechnet werden LinMod.1Fun(total_bill=30) #&gt; 1 #&gt; 4.07 also 4.07$. In mosaic kann die Modellgerade über plotModel(LinMod.1) betrachtet werden. Das Bestimmtheitsmaß R² ist mit 0.46 “ok”: 46-% der Variation des Trinkgeldes wird im Modell erklärt. 6.1.1 Überprüfung der Annahmen Aber wie sieht es mit den Annahmen aus? Die Linearität des Zusammenhangs haben wir zu Beginn mit Hilfe des Scatterplots “überprüft”. Zur Überprüfung der Normalverteilung der Residuen zeichnen wir ein Histogramm. Die Residuen können über den Befehl resid() aus einem Linearen Modell extrahiert werden. Hier scheint es zu passen: resid_df &lt;- data.frame(Residuen = resid(LinMod.1)) qplot(x = Residuen, data = resid_df) Konstante Varianz: Dies kann z. B. mit einem Scatterplot der Residuen auf der y-Achse und den angepassten Werten auf der x-Achse überprüft werden. Die angepassten (geschätzten) Werte werden über den Befehl fitted()31 extrahiert. Diese Annahme scheint verletzt zu sein (siehe unten): je größer die Prognose des Trinkgeldes, desto größer wirkt die Streuung der Residuen. Dieses Phänomen ließ sich schon aus dem ursprünglichen Scatterplot qplot(x = tip, y = total_bill, data=tips) erahnen. Das ist auch inhaltlich plausibel: je höher die Rechnung, desto höher die Varianz beim Trinkgeld. Die Verletzung dieser Annahme beeinflusst nicht die Schätzung der Steigung, sondern die Schätzung des Standardfehlers, also des p-Wertes des Hypothesentests, d. h., \\(H_0:\\beta_1=0\\). resid_df$fitted &lt;- fitted(LinMod.1) qplot(x = Residuen, y = fitted, data = resid_df) Extreme Ausreißer: Wie am Plot der Linearen Regression plotModel(LinMod.1) erkennbar, gibt es vereinzelt Ausreißer nach oben, allerdings ohne einen extremen Hebel. Hängt die Rechnungshöhe von der Anzahl der Personen ab? Bestimmt, aber wie? xyplot(total_bill ~ size, data=tips) Da bei diskreten metrischen Variablen (hier size) Punkte übereinander liegen können, sollte man “jittern” (“schütteln”), d. h., eine (kleine) Zufallszahl addieren: qplot(x = total_bill, y = size, data = tips, geom = &quot;jitter&quot;) Um wie viel Dollar steigt im Durchschnitt das Trinkgeld, wenn eine Person mehr am Tisch sitzt? Für wie aussagekräftig halten Sie Ihr Ergebnis aus 1.? 6.1.2 Regression mit kategorialen Werten Der Wochentag day ist eine kategoriale Variable. Wie sieht eine Regression des Trinkgeldes darauf aus? Zunächst grafisch: qplot(x = tip,y = day, data=tips) Und als Lineares Modell: LinMod.2 &lt;- lm(tip ~ day, data=tips) summary(LinMod.2) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ day, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.245 -0.993 -0.235 0.538 7.007 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.7347 0.3161 8.65 7.5e-16 *** #&gt; daySat 0.2584 0.3489 0.74 0.46 #&gt; daySun 0.5204 0.3534 1.47 0.14 #&gt; dayThur 0.0367 0.3613 0.10 0.92 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.38 on 240 degrees of freedom #&gt; Multiple R-squared: 0.0205, Adjusted R-squared: 0.00823 #&gt; F-statistic: 1.67 on 3 and 240 DF, p-value: 0.174 Die im Modell angegebenen Schätzwerte sind die Änderung der Trinkgeldprognose, wenn z. B. der Tag ein Samstag (daySat) im Vergleich zu einer Referenzkategorie. Dies ist in R das erste Element des Vektors der Faktorlevel. Welcher dies ist ist über den Befehl levels() zu erfahren levels(tips$day) #&gt; [1] &quot;Fri&quot; &quot;Sat&quot; &quot;Sun&quot; &quot;Thur&quot; hier also Fri (aufgrund der standardmäßig aufsteigenden alphanumerischen Sortierung). Dies kann über relevel() geändert werden. Soll z. B. die Referenz der Donnerstag, Thur sein: tips$day &lt;- relevel(tips$day, ref = &quot;Thur&quot;) levels(tips$day) #&gt; [1] &quot;Thur&quot; &quot;Fri&quot; &quot;Sat&quot; &quot;Sun&quot; Das Modell ändert sich entsprechend: LinMod.3 &lt;- lm(tip ~ day, data=tips) summary(LinMod.3) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ day, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.245 -0.993 -0.235 0.538 7.007 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.7715 0.1750 15.84 &lt;2e-16 *** #&gt; dayFri -0.0367 0.3613 -0.10 0.919 #&gt; daySat 0.2217 0.2290 0.97 0.334 #&gt; daySun 0.4837 0.2358 2.05 0.041 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.38 on 240 degrees of freedom #&gt; Multiple R-squared: 0.0205, Adjusted R-squared: 0.00823 #&gt; F-statistic: 1.67 on 3 and 240 DF, p-value: 0.174 sowie als Plot: plotModel(LinMod.3) Eine Alternative zu relevel() zur Bestimmung der Referenzkategorie ist es, innerhalb von factor() die Option levels= direkt in der gewünschten Sortierung zu setzen. day &lt;- factor(tips$day, levels=c(&quot;Thur&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)) Die (Punkt-)Prognose für die Trinkgeldhöhe, bspw. an einen Freitag kann dann berechnet werden LinMod.3Fun &lt;- makeFun(LinMod.3) LinMod.3Fun(day=&quot;Fri&quot;) #&gt; 1 #&gt; 2.73 Wie verändert sich die Rechnungshöhe im Durchschnitt, wenn die Essenszeit Dinner statt Lunch ist? Wie viel % der Variation der Rechnungshöhe können Sie durch die Essenszeit modellieren? 6.1.3 Multiple Regression Aber wie wirken sich die Einflussgrößen zusammen auf das Trinkgeld aus? LinMod.4 &lt;- lm(tip ~ total_bill + size + sex + smoker + day + time, data=tips) summary(LinMod.4) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill + size + sex + smoker + day + time, #&gt; data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.848 -0.573 -0.103 0.476 4.108 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.6416 0.4976 1.29 0.199 #&gt; total_bill 0.0945 0.0096 9.84 &lt;2e-16 *** #&gt; size 0.1760 0.0895 1.97 0.051 . #&gt; sexMale -0.0324 0.1416 -0.23 0.819 #&gt; smokerYes -0.0864 0.1466 -0.59 0.556 #&gt; dayFri 0.1623 0.3934 0.41 0.680 #&gt; daySat 0.0408 0.4706 0.09 0.931 #&gt; daySun 0.1368 0.4717 0.29 0.772 #&gt; timeLunch 0.0681 0.4446 0.15 0.878 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 235 degrees of freedom #&gt; Multiple R-squared: 0.47, Adjusted R-squared: 0.452 #&gt; F-statistic: 26.1 on 8 and 235 DF, p-value: &lt;2e-16 Interessant sind die negativen Vorzeichen vor den Schätzwerten für sexMale und smokerYes – anscheinend geben Männer und Raucher weniger Trinkgeld, wenn alle anderen Faktoren konstant bleiben. Bei einer rein univariaten Betrachtung wäre etwas anderes herausgekommen. summary(lm(tip ~ sex, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ sex, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.090 -1.090 -0.090 0.667 6.910 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.833 0.148 19.14 &lt;2e-16 *** #&gt; sexMale 0.256 0.185 1.39 0.17 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.38 on 242 degrees of freedom #&gt; Multiple R-squared: 0.0079, Adjusted R-squared: 0.0038 #&gt; F-statistic: 1.93 on 1 and 242 DF, p-value: 0.166 summary(lm(tip ~ smoker, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ smoker, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.009 -0.994 -0.100 0.558 6.991 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.9919 0.1128 26.52 &lt;2e-16 *** #&gt; smokerYes 0.0169 0.1828 0.09 0.93 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.39 on 242 degrees of freedom #&gt; Multiple R-squared: 3.51e-05, Adjusted R-squared: -0.0041 #&gt; F-statistic: 0.00851 on 1 and 242 DF, p-value: 0.927 Diese Umkehrung des modellierten Effektes liegt daran, dass es auch einen positiven Zusammenhang zur Rechnungshöhe gibt: summary(lm(total_bill ~ sex, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = total_bill ~ sex, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.99 -6.02 -1.94 3.99 30.07 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 18.057 0.946 19.08 &lt;2e-16 *** #&gt; sexMale 2.687 1.180 2.28 0.024 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.83 on 242 degrees of freedom #&gt; Multiple R-squared: 0.021, Adjusted R-squared: 0.0169 #&gt; F-statistic: 5.19 on 1 and 242 DF, p-value: 0.0236 summary(lm(total_bill ~ smoker, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = total_bill ~ smoker, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -17.69 -6.46 -1.89 4.58 30.05 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.188 0.723 26.53 &lt;2e-16 *** #&gt; smokerYes 1.568 1.172 1.34 0.18 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.89 on 242 degrees of freedom #&gt; Multiple R-squared: 0.00735, Adjusted R-squared: 0.00325 #&gt; F-statistic: 1.79 on 1 and 242 DF, p-value: 0.182 Im vollem Modell LinMod.4 sind alle unabhängigen Variablen berücksichtigt, die Koeffizienten beziehen sich dann immer auf: gegeben, die anderen Variablen bleiben konstant, d. h. ceteris paribus. Vergleichen wir mal zwei Modelle: LinMod.5a &lt;- lm(tip ~ sex, data=tips) coef(LinMod.5a) # Koeffizienten extrahieren #&gt; (Intercept) sexMale #&gt; 2.833 0.256 LinMod.5b &lt;- lm(tip ~ sex + total_bill, data=tips) coef(LinMod.5b) # Koeffizienten extrahieren #&gt; (Intercept) sexMale total_bill #&gt; 0.9333 -0.0266 0.1052 Ohne die Berücksichtigung der Kovariable/Störvariable Rechnungshöhe geben Male ein um im Durchschnitt 0.26$ höheres Trinkgeld, bei Kontrolle, d. h. gleicher Rechnungshöhe ein um 0.03$ niedrigeres Trinkgeld als die Referenzklasse Female (levels(tips$sex)[1]). 6.1.4 Inferenz in der linearen Regression Kehren wir noch einmal zur multivariaten Regression (LinMod.4) zurück. summary(LinMod.4) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill + size + sex + smoker + day + time, #&gt; data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.848 -0.573 -0.103 0.476 4.108 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.6416 0.4976 1.29 0.199 #&gt; total_bill 0.0945 0.0096 9.84 &lt;2e-16 *** #&gt; size 0.1760 0.0895 1.97 0.051 . #&gt; sexMale -0.0324 0.1416 -0.23 0.819 #&gt; smokerYes -0.0864 0.1466 -0.59 0.556 #&gt; dayFri 0.1623 0.3934 0.41 0.680 #&gt; daySat 0.0408 0.4706 0.09 0.931 #&gt; daySun 0.1368 0.4717 0.29 0.772 #&gt; timeLunch 0.0681 0.4446 0.15 0.878 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 235 degrees of freedom #&gt; Multiple R-squared: 0.47, Adjusted R-squared: 0.452 #&gt; F-statistic: 26.1 on 8 and 235 DF, p-value: &lt;2e-16 In der 4. Spalte der, mit Zeilennamen versehenen Tabelle Coefficients stehen die p-Werte der Nullhypothese, die unabhängige Variable hat, gegeben alle anderen Variablen im Modell, keinen linearen Einfluss auf die abhängige Variable: \\(H_0: \\beta_i=0\\). Zur Bestimmung des p-Wertes wird der Schätzer (Estimate) durch den Standardfehler (Std. Error) dividiert. Der resultierende t-Wert (t value) wird dann, zusammen mit der Anzahl an Freiheitsgraden zur Berechnung des p-Wertes (Pr(&gt;|t|)) verwendet. Ein einfacher t-Test! Zur schnelleren Übersicht finden sich dahinter “Sternchen” und “Punkte”, die die entsprechenden Signifikanzniveaus symbolisieren: *** bedeutet eine Irrtumswahrscheinlichkeit, Wahrscheinlichkeit für Fehler 1. Art, von unter 0.001, d. h. unter 0,1%. ** entsprechend 1%, * 5% und . 10%. Zum Signifikanzniveau von 10% sind hier also zwei Faktoren und der Achsenabschnitt ((Intercept)) signifikant – nicht notwendigerweise relevant: Rechnungshöhe total_bill sowie Anzahl Personen size. Beides wirkt sich linear positiv auf die Trinkgeldhöhe aus: Mit jedem Dollar Rechnungshöhe steigt im Mittelwert die Trinkgeldhöhe um 0.09 Dollar, mit jeder Person um 0.18 Dollar – gegeben alle anderen Faktoren bleiben konstant. Das Bestimmtheitsmaß R² (Multiple R-squared:) liegt bei 0.47, also 47% der Variation des Trinkgeldes wird im Modell erklärt. Außerdem wird getestet, ob alle Koeffizienten der unabhängigen Variablen gleich Null sind: \\[H_0: \\beta_1=\\beta_2=\\cdots=\\beta_k=0\\] Das Ergebnis des zugrundeliegenden F-Tests (vgl. Varianzanalyse) wird in der letzten Zeile angegeben (F-Statistic). Hier wird \\(H_0\\) also verworfen. 6.1.5 Erweiterungen 6.1.5.1 Modellwahl Das Modell mit allen Variablen des Datensatzes, d. h., mit 6 unabhängigen (LinMod.4) erklärt 47.01% der Variation, das Modell nur mit der Rechnungshöhe als erklärende Variable (LinMod.1) schon 45.66%, der Erklärungszuwachs liegt also gerade einmal bei 1.35 Prozentpunkten. In der Statistik ist die Wahl des richtigen Modells eine der größten Herausforderungen, auch deshalb, weil das wahre Modell in der Regel nicht bekannt ist und es schwer ist, die richtige Balance zwischen Einfachheit und Komplexität zu finden. Aufgrund des Zufalls kann es immer passieren, dass das Modell sich zu sehr an die zufälligen Daten anpasst (Stichwort: Overfitting). Es gibt unzählige Modellwahlmethoden, und leider garantiert keine, dass immer das beste Modell gefunden wird. Eine Möglichkeit ist die sogenannte Schrittweise-Rückwärtsselektion auf Basis des Akaike-Informationskriteriums (AIC)32. Diese ist nicht nur recht weit verbreitet - und liefert unter bestimmten Annahmen das “richtige” Modell - sondern in R durch den Befehl step() einfach umsetzbar: step(LinMod.4) #&gt; Start: AIC=20.5 #&gt; tip ~ total_bill + size + sex + smoker + day + time #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - day 3 0.6 247 15.1 #&gt; - time 1 0.0 247 18.5 #&gt; - sex 1 0.1 247 18.6 #&gt; - smoker 1 0.4 247 18.9 #&gt; &lt;none&gt; 247 20.5 #&gt; - size 1 4.1 251 22.5 #&gt; - total_bill 1 101.6 348 102.7 #&gt; #&gt; Step: AIC=15.1 #&gt; tip ~ total_bill + size + sex + smoker + time #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - time 1 0.0 247 13.1 #&gt; - sex 1 0.0 247 13.2 #&gt; - smoker 1 0.4 248 13.5 #&gt; &lt;none&gt; 247 15.1 #&gt; - size 1 4.3 251 17.4 #&gt; - total_bill 1 101.7 349 97.2 #&gt; #&gt; Step: AIC=13.1 #&gt; tip ~ total_bill + size + sex + smoker #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - sex 1 0.0 247 11.2 #&gt; - smoker 1 0.4 248 11.5 #&gt; &lt;none&gt; 247 13.1 #&gt; - size 1 4.3 251 15.4 #&gt; - total_bill 1 103.3 350 96.3 #&gt; #&gt; Step: AIC=11.2 #&gt; tip ~ total_bill + size + smoker #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - smoker 1 0.4 248 9.5 #&gt; &lt;none&gt; 247 11.2 #&gt; - size 1 4.3 252 13.4 #&gt; - total_bill 1 104.3 351 95.0 #&gt; #&gt; Step: AIC=9.53 #&gt; tip ~ total_bill + size #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 248 9.5 #&gt; - size 1 5.2 253 12.6 #&gt; - total_bill 1 106.3 354 94.7 #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill + size, data = tips) #&gt; #&gt; Coefficients: #&gt; (Intercept) total_bill size #&gt; 0.6689 0.0927 0.1926 In den letzten Zeilen der Ausgabe steht das beste Modell, das diese Methode (schrittweise, rückwärts) mit diesem Kriterium (AIC) bei diesen Daten findet (Punktprognose, d. h. ohne Residuum): tip = 0.66894 + 0.09271 * total_bill + 0.19260 * size Der Ausgabe können Sie auch entnehmen, welche Variablen in welcher Reihenfolge entfernt wurden: Zunächst day, dann time, danach sex und schließlich smoker. Hier sind also dieselben Variablen noch im Modell, die auch in LinMod.4 signifikant zum Niveau 10% waren, eine Auswahl der dort signifikanten Variablen hätte also dasselbe Modell ergeben. Das ist häufig so, aber nicht immer! 6.1.5.2 Interaktionen Wir haben gesehen, dass es einen Zusammenhang zwischen der Trinkgeldhöhe und der Rechnungshöhe gibt. Vielleicht unterscheidet sich der Zusammenhang je nachdem, ob geraucht wurde, d. h., vielleicht gibt es eine Interaktion (Wechselwirkung). Die kann in lm einfach durch ein * zwischen den unabhängigen Variablen modelliert werden: LinMod.6 &lt;- lm(tip ~ smoker*total_bill, data = tips) summary(LinMod.6) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ smoker * total_bill, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.679 -0.524 -0.120 0.475 4.900 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.36007 0.20206 1.78 0.07601 . #&gt; smokerYes 1.20420 0.31226 3.86 0.00015 *** #&gt; total_bill 0.13716 0.00968 14.17 &lt; 2e-16 *** #&gt; smokerYes:total_bill -0.06757 0.01419 -4.76 3.3e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.979 on 240 degrees of freedom #&gt; Multiple R-squared: 0.506, Adjusted R-squared: 0.5 #&gt; F-statistic: 81.9 on 3 and 240 DF, p-value: &lt;2e-16 Der Schätzwert für die Interaktion steht bei :. Hier also: Wenn geraucht wurde, ist die Steigung im Durchschnitt um 6,8 Cent geringer. Aber wenn geraucht wurde, ist die Rechnung im Achsenabschnitt erstmal um 1,20$ höher (Effekt, ceteris paribus). Wer will, kann ausrechnen, ab welcher Rechnungshöhe Rauchertische im Mittelwert lukrativer sind… Das gleiche Bild (höhere Achsenabschnitt, geringere Steigung) ergibt sich übrigens bei getrennten Regressionen: lm(tip~total_bill, data=tips, subset = smoker==&quot;Yes&quot;) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill, data = tips, subset = smoker == #&gt; &quot;Yes&quot;) #&gt; #&gt; Coefficients: #&gt; (Intercept) total_bill #&gt; 1.5643 0.0696 lm(tip~total_bill, data=tips, subset = smoker==&quot;No&quot;) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ total_bill, data = tips, subset = smoker == #&gt; &quot;No&quot;) #&gt; #&gt; Coefficients: #&gt; (Intercept) total_bill #&gt; 0.360 0.137 6.1.5.3 Weitere Modellierungsmöglichkeiten Über das Formelinterface y~x können auch direkt z. B. Polynome modelliert werden. Hier eine quadratische Funktion: summary(lm(tip~I(total_bill^2)+total_bill, data=tips)) #&gt; #&gt; Call: #&gt; lm(formula = tip ~ I(total_bill^2) + total_bill, data = tips) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.200 -0.559 -0.098 0.484 3.776 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.91e-01 3.47e-01 2.57 0.01078 * #&gt; I(total_bill^2) -5.71e-05 6.02e-04 -0.09 0.92457 #&gt; total_bill 1.08e-01 3.08e-02 3.51 0.00054 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.02 on 241 degrees of freedom #&gt; Multiple R-squared: 0.457, Adjusted R-squared: 0.452 #&gt; F-statistic: 101 on 2 and 241 DF, p-value: &lt;2e-16 D. h., die geschätzte Funktion ist eine “umgedrehte Parabel” (negatives Vorzeichen bei I(total_bill^2)), bzw. die Funktion ist konkav, die Steigung nimmt ab. Allerdings ist der Effekt nicht signifikant. Hinweis: Um zu “rechnen” und nicht beispielsweise Interaktion zu modellieren, geben Sie die Variablen in der Formel in der Funktion I() (As Is) ein. 6.1.5.4 Prognoseintervalle Insgesamt haben wir viel “Unsicherheit” u. a. aufgrund von Variabilität in den Beobachtungen und in den Schätzungen. Wie wirken sich diese auf die Prognose aus? Dazu können wir über die Funktion predict.lm Prognoseintervalle berechnen – hier für das einfache Modell LinMod.1: newdat &lt;- data.frame(total_bill = seq(0, 75)) preddat &lt;- predict(LinMod.1, newdata = newdat, interval = &quot;prediction&quot;) head(preddat) #&gt; fit lwr upr #&gt; 1 0.92 -1.117 2.96 #&gt; 2 1.03 -1.010 3.06 #&gt; 3 1.13 -0.903 3.16 #&gt; 4 1.24 -0.797 3.27 #&gt; 5 1.34 -0.690 3.37 #&gt; 6 1.45 -0.583 3.47 tail(preddat) #&gt; fit lwr upr #&gt; 71 8.27 6.13 10.4 #&gt; 72 8.38 6.23 10.5 #&gt; 73 8.48 6.33 10.6 #&gt; 74 8.59 6.43 10.7 #&gt; 75 8.69 6.53 10.9 #&gt; 76 8.80 6.63 11.0 matplot(newdat$total_bill, preddat, lty = c(1,2,2), type=&quot;l&quot; ) points(x=tips$total_bill, y=tips$tip) Sie sehen, dass 95% Prognoseintervall ist recht breit: über den gewählten Rechnungsbereich von \\(0-75\\)$ im Mittelwert bei 4.11$. favstats((preddat[,3]-preddat[,2])) #&gt; min Q1 median Q3 max mean sd n missing #&gt; 4.03 4.04 4.07 4.17 4.34 4.12 0.0904 76 0 Zu den Rändern hin wird es breiter. Am schmalsten ist es übrigens beim Mittelwert der unabhängigen Beobachtungen, hier also bei 19.79$. 6.1.6 Übung: Teaching Rating Dieser Datensatz analysiert u. a. den Zusammenhang zwischen Schönheit und Evaluierungsergebnis von Dozenten (Hamermesh and Parker 2005). Sie können ihn, sofern noch nicht geschehen, von https://goo.gl/6Y3KoK als csv herunterladen. Versuchen Sie, das Evaluierungsergebnis als abhängige Variable anhand geeigneter Variablen des Datensatzes zu erklären. Wie groß ist der Einfluss der Schönheit? Sind die Modellannahmen erfüllt und wie beurteilen Sie die Modellgüte? 6.1.7 Literatur David M. Diez, Christopher D. Barr, Mine Çetinkaya-Rundel (2014): Introductory Statistics with Randomization and Simulation, https://www.openintro.org/stat/textbook.php?stat_book=isrs, Kapitel 5, 6.1-6.3 Nicholas J. Horton, Randall Pruim, Daniel T. Kaplan (2015): Project MOSAIC Little Books A Student’s Guide to R, https://github.com/ProjectMOSAIC/LittleBooks/raw/master/StudentGuide/MOSAIC-StudentGuide.pdf, Kapitel 5.4, 10.2 Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013): An Introduction to Statistical Learning – with Applications in R, http://www-bcf.usc.edu/~gareth/ISL/, Kapitel 3 Maike Luhmann (2015): R für Einsteiger, Kapitel 16, 17.1-17.3 Andreas Quatember (2010): Statistik ohne Angst vor Formeln, Kapitel 3.11 Daniel Wollschläger (2014): Grundlagen der Datenanalyse mit R, Kapitel 6 Diese Übung basiert teilweise auf Übungen zum Buch OpenIntro von Andrew Bray und Mine Çetinkaya-Rundel unter der Lizenz Creative Commons Attribution-ShareAlike 3.0 Unported. 6.2 Klassifizierende Regression 6.2.1 Vorbereitung Hier werden wir den Datensatz Aktienkauf der Universität Zürich (Universität Zürich, Methodenberatung) analysieren. Es handelt es sich hierbei um eine Befragung einer Bank im Zusammenhang mit den Fakten, die mit der Wahrscheinlichkeit, dass jemand Aktien erwirbt, zusammenhängen. Es wurden 700 Personen befragt. Folgende Daten wurden erhoben: Aktienkauf (0 = nein, 1 = ja), Jahreseinkommen (in Tausend CHF), Risikobereitschaft (Skala von 0 bis 25) und Interesse an der aktuellen Marktlage (Skala von 0 bis 45). Den Datensatz können Sie in so als csv-Datei herunterladen: Aktien &lt;- read.csv2(&quot;https://raw.githubusercontent.com/luebby/Datenanalyse-mit-R/master/Daten/Aktienkauf.csv&quot;) Zur Unterstützung der Analyse wird (wieder) mosaic und ggplot2 verwendet. library(mosaic) library(ggplot2) 6.2.2 Problemstellung Können wir anhand der Risikobereitschaft abschätzen, ob die Wahrscheinlichkeit für einen Aktienkauf steigt? Schauen wir uns zunächst ein Streudiagramm an: xyplot(Aktienkauf ~ Risikobereitschaft, data = Aktien) Der Zusammenhang scheint nicht sehr ausgeprägt zu sein. Lassen Sie uns dennoch ein lineare Regression durchführen und das Ergebnis auswerten und graphisch darstellen. lm1 &lt;- lm(Aktienkauf ~ Risikobereitschaft, data = Aktien) summary(lm1) #&gt; #&gt; Call: #&gt; lm(formula = Aktienkauf ~ Risikobereitschaft, data = Aktien) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.684 -0.243 -0.204 0.348 0.814 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.18246 0.02001 9.12 &lt; 2e-16 *** #&gt; Risikobereitschaft 0.05083 0.00762 6.67 5.2e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.427 on 698 degrees of freedom #&gt; Multiple R-squared: 0.0599, Adjusted R-squared: 0.0586 #&gt; F-statistic: 44.5 on 1 and 698 DF, p-value: 5.25e-11 plotModel(lm1) Der Schätzer für die Steigung für Risikobereitschaft ist signifikant. Das Bestimmtheitsmaß R ist allerdings sehr niedrig, aber wir haben bisher ja auch nur eine unabhängige Variable für die Erklärung der abhängigen Variable herangezogen. Doch was bedeutet es, dass die Wahrscheinlichkeit ab einer Risikobereitsschaft von ca. 16 über 1 liegt? Wahrscheinlichkeiten müssen zwischen 0 und 1 liegen. Daher brauchen wir eine Funktion, die das Ergebnis einer linearen Regression in einen Bereich von 0 bis 1 bringt, die sogenannte Linkfunktion. Eine häufig dafür verwendete Funktion ist die logistische Funktion: \\[p(y=1)=\\frac{e^\\eta}{1+e^\\eta}=\\frac{1}{1+e^{-\\eta}}\\] \\(\\eta\\), das sogenannte Logit, ist darin die Linearkombination der Einflussgrößen: \\[\\eta=\\beta_0+\\beta_1\\cdot x_1+\\dots\\] Exemplarisch können wir die logistische Funktion für einen Bereich von \\(\\eta=-10\\) bis \\(+10\\) darstellen: 6.2.3 Die Idee der logistischen Regression Die logistische Regression ist eine Anwendung des allgemeinen linearen Modells (general linear model, GLM). Die Modellgleichung lautet: \\[p(y_i=1)=L\\bigl(\\beta_0+\\beta_1\\cdot x_{i1}+\\dots+\\beta_K\\cdot x_{ik}\\bigr)+\\epsilon_i\\] \\(L\\) ist die Linkfunktion, in unserer Anwendung die logistische Funktion. \\(x_{ik}\\) sind die beobachten Werte der unabhängigen Variablen \\(X_k\\). \\(k\\) sind die unabhängigen Variablen \\(1\\) bis \\(K\\). Die Funktion glm führt die logistische Regression durch. Wir schauen uns im Anschluss zunächst den Plot an. glm1 &lt;- glm(Aktienkauf ~ Risikobereitschaft, family = binomial(&quot;logit&quot;), data = Aktien) plotModel(glm1) Es werden ein Streudiagramm der beobachten Werte sowie die Regressionslinie ausgegeben. Wir können so z. B. ablesen, dass ab einer Risikobereitschaft von etwa 7 die Wahrscheinlichkeit für einen Aktienkauf nach unserem Modell bei mehr als 50 % liegt. Die Zusammenfassung des Modells zeigt folgendes: summary(glm1) #&gt; #&gt; Call: #&gt; glm(formula = Aktienkauf ~ Risikobereitschaft, family = binomial(&quot;logit&quot;), #&gt; data = Aktien) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.653 -0.738 -0.677 0.825 1.823 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.4689 0.1184 -12.4 &lt; 2e-16 *** #&gt; Risikobereitschaft 0.2573 0.0468 5.5 3.8e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 804.36 on 699 degrees of freedom #&gt; Residual deviance: 765.86 on 698 degrees of freedom #&gt; AIC: 769.9 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Der Achsenabschnitt (intercept) des logits \\(\\eta\\) wird mit -1.47 geschätzt, die Steigung in Richtung Risikobereitschaft mit 0.26. Die (Punkt-)Prognose für die Wahrscheinlickeit eines Aktienkaufs \\(p(y=1)\\) benötigt anders als in der linearen Regression noch die Linkfunktion und ergibt sich somit zu: \\[p(\\texttt{Aktienkauf}=1)=\\frac{1}{1+e^{-(-1.47 + 0.26 \\cdot \\texttt{Risikobereitschaft})}}\\] Die p-Werte der Koeffizienten können in der Spalte Pr(&gt;|z|) abgelesen werden. Hier wird ein Wald-Test durchgeführt, nicht wie bei der linearen Regression ein t-Test, ebenfalls mit der \\(H_0:\\beta_i=0\\). Die Teststastistik (z value) wird wie in der linearen Regression durch Divisions des Schätzers (Estimate) durch den Standardfehler (Std. Error) ermittelt. Im Wald-Test ist die Teststatistik allerdings \\(\\chi^2\\)-verteilt mit einem Freiheitsgrad. 6.2.4 Welche Unterschiede zur linearen Regression gibt es in der Ausgabe? Es gibt kein R im Sinne einer erklärten Streuung der \\(y\\)-Werte, da die beobachteten \\(y\\)-Werte nur \\(0\\) oder \\(1\\) annehmen können. Das Gütemaß bei der logistischen Regression ist das Akaike Information Criterion (AIC). Hier gilt allerdings: je kleiner, desto besser. (Anmerkung: es kann ein Pseudo-R berechnet werden – kommt später.) Es gibt keine F-Statistik (oder ANOVA) mit der Frage, ob das Modell als Ganzes signifikant ist. (Anmerkung: es kann aber ein vergleichbarer Test durchgeführt werden – kommt später.) 6.2.5 Interpretation der Koeffizienten 6.2.5.1 y-Achsenabschnitt (Intercept) \\(\\beta_0\\) Für \\(\\beta_0&gt;0\\) gilt, dass selbst wenn alle anderen unabhängigen Variablen \\(0\\) sind, es eine Wahrscheinlichkeit von mehr als 50% gibt, dass das modellierte Ereignis eintritt. Für \\(\\beta_0&lt;0\\) gilt entsprechend das Umgekehrte. 6.2.5.2 Steigung \\(\\beta_i\\) mit \\(i=1,2,...,K\\) Für \\(\\beta_i&gt;0\\) gilt, dass mit zunehmenden \\(x_i\\) die Wahrscheinlichkeit für das modellierte Ereignis steigt. Bei \\(\\beta_i&lt;0\\) nimmt die Wahrscheinlichkeit entsprechend ab. Eine Abschätzung der Änderung der Wahrscheinlichkeit (relatives Risiko, relative risk \\(RR\\)) kann über das Chancenverhältnis (Odds Ratio \\(OR\\)) gemacht werden.33 Es ergibt sich vereinfacht \\(e^{\\beta_i}\\). Die Wahrscheinlichkeit ändert sich näherungsweise um diesen Faktor, wenn sich \\(x_i\\) um eine Einheit erhöht. Hinweis: \\(RR\\approx OR\\) gilt nur, wenn der Anteil des modellierten Ereignisses in den beobachteten Daten sehr klein (\\(&lt;5\\%\\)) oder sehr groß ist (\\(&gt;95\\%\\)). Übung: Berechnen Sie das relative Risiko für unser Beispielmodell, wenn sich die Risikobereitschaft um 1 erhöht (Funktion exp()). Vergleichen Sie das Ergebnis mit der Punktprognose für Risikobereitschaft\\(=7\\) im Vergleich zu Risikobereitschaft\\(=8\\). Zur Erinnerung: Sie können makeFun(model) verwenden. # aus Koeffizient abgeschätzt exp(coef(glm1)[2]) #&gt; Risikobereitschaft #&gt; 1.29 # mit dem vollständigen Modell berechnet fun1 &lt;- makeFun(glm1) fun1(Risikobereitschaft = 7) #&gt; 1 #&gt; 0.582 fun1(Risikobereitschaft = 8) #&gt; 1 #&gt; 0.643 # als Faktor ausgeben fun1(Risikobereitschaft = 8)/fun1(Risikobereitschaft = 7) #&gt; 1 #&gt; 1.1 Sie sehen also, die ungefähr abgeschätzte Änderung der Wahrscheinlichkeit weicht hier doch deutlich von der genau berechneten Änderung ab. Der Anteil der Datensätze mit Risikobereitschaft\\(=1\\) liegt allerdings auch bei 0.26. 6.2.6 Kategoriale Variablen Wie schon in der linearen Regression können auch in der logistschen Regression kategoriale Variablen als unabhängige Variablen genutzt werden. Als Beispiel nehmen wir den Datensatz tips und versuchen abzuschätzen, ob sich die Wahrscheinlichkeit dafür, dass ein Raucher bezahlt hat (smoker == yes), in Abhängigkeit vom Wochentag ändert. Sofern noch nicht geschehen, können Sie so als csv-Datei herunterladen: tips &lt;- read.csv(&quot;https://sebastiansauer.github.io/data/tips.csv&quot;) Zunächst ein Plot: xyplot(jitter(as.numeric(smoker)) ~ day, data = tips) Hinweis: Um zu sehen, ob es an manchen Tagen mehr Raucher gibt, sollten Sie zumindest eine Variable “verrauschen” (“jittern”). Da die Variable smoker eine nominale Variable ist und die Funktion jitter() nur mit numerischen Variablen arbeitet, muss sie mit as.numeric() in eine numerische Variable umgewandelt werden. Die relativen Häufigkeiten zeigt folgende Tabelle: (tab_smoke &lt;- tally(smoker ~ day, data = tips, format = &quot;proportion&quot;)) #&gt; day #&gt; smoker Fri Sat Sun Thur #&gt; No 0.211 0.517 0.750 0.726 #&gt; Yes 0.789 0.483 0.250 0.274 Hinweis: Durch die Klammerung wird das Objekt tab_smoke direkt ausgegeben. Probieren wir die logistische Regression aus: glmtips &lt;- glm(smoker ~ day, family = binomial(&quot;logit&quot;),data = tips) summary(glmtips) #&gt; #&gt; Call: #&gt; glm(formula = smoker ~ day, family = binomial(&quot;logit&quot;), data = tips) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.765 -0.801 -0.758 1.207 1.665 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.322 0.563 2.35 0.01883 * #&gt; daySat -1.391 0.602 -2.31 0.02093 * #&gt; daySun -2.420 0.622 -3.89 1e-04 *** #&gt; dayThur -2.295 0.631 -3.64 0.00027 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 324.34 on 243 degrees of freedom #&gt; Residual deviance: 298.37 on 240 degrees of freedom #&gt; AIC: 306.4 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Auch hier können wir die Koeffizienten in Relation zur Referenzkategorie (hier: Freitag) interpretieren. Die Wahrscheinlichkeit ist an einem Samstag niedriger, der Wert für daySat ist negativ. Eine Abschätzung erhalten wir wieder mit \\(e^{\\beta_i}\\): exp(coef(glmtips)[2]) #&gt; daySat #&gt; 0.249 Daher ist das Chancenverhältnis (Odds Ratio), dass am Samstag ein Raucher am Tisch sitzt, näherungsweise um den Faktor 0.25 niedriger als am Freitag: \\[ { OR=\\frac{\\frac{P(Raucher|Samstag)}{1-P(Raucher|Samstag)}} {\\frac{P(Raucher|Freitag)}{1-P(Raucher|Freitag)}} =\\frac{\\frac{0.483}{0.517}} {\\frac{0.79}{0.21}} \\approx 0.249}\\] 6.2.7 Multiple logistische Regression Wir kehren wieder zurück zu dem Datensatz Aktienkauf. Können wir unser Model glm1 mit nur einer erklärenden Variable verbessern, indem weitere unabhängige Variablen hinzugefügt werden? glm2 &lt;- glm(Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse, family = binomial(&quot;logit&quot;),data = Aktien) plotModel(glm2) summary(glm2) #&gt; #&gt; Call: #&gt; glm(formula = Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse, #&gt; family = binomial(&quot;logit&quot;), data = Aktien) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.130 -0.715 -0.539 0.518 3.214 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.66791 0.27903 -5.98 2.3e-09 *** #&gt; Risikobereitschaft 0.34781 0.08822 3.94 8.1e-05 *** #&gt; Einkommen -0.02157 0.00564 -3.83 0.00013 *** #&gt; Interesse 0.08520 0.01775 4.80 1.6e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 804.36 on 699 degrees of freedom #&gt; Residual deviance: 679.01 on 696 degrees of freedom #&gt; AIC: 687 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Alle Schätzer sind signifkant zum 0.1 %-Niveau (*** in der Ausgabe). Zunehmende Risikobereitschaft (der Einfluss ist im Vergleich zum einfachen Modell stärker geworden) und zunehmendes Interesse erhöhen die Wahrscheinlichkeit für einen Aktienkauf. Steigendes Einkommen hingegen senkt die Wahrscheinlichkeit. Ist das Modell besser als das einfache? Ja, da der AIC-Wert von 769.86 auf 687.01 gesunken ist. Die Graphik zeigt die Verläufe in Abhängigkeit von den verschiedenen Variablen und den Kombinationen der Variablen. 6.2.8 Erweiterungen 6.2.8.1 Klassifikationsgüte Logistische Regressionsmodelle werden häufig zur Klassifikation verwendet, z. B. ob der Kredit für einen Neukunden ein “guter” Kredit ist oder nicht. Daher sind die Klassifikationseigenschaften bei logistischen Modellen wichtige Kriterien. Hierzu werden die aus dem Modell ermittelten Wahrscheinlichkeiten ab einem Schwellenwert (cutpoint), häufig \\(0.5\\), einer geschätzten \\(1\\) zugeordnet, unterhalb des Schwellenwertes einer \\(0\\). Diese aus dem Modell ermittelten Häufigkeiten werden dann in einer sogenannten Konfusionsmatrix (confusion matrix) mit den beobachteten Häufigkeiten verglichen. Daher sind wichtige Kriterien eines Modells, wie gut diese Zuordnung erfolgt. Dazu werden die Sensitivität (True Positive Rate, TPR), also der Anteil der mit \\(1\\) geschätzten an allen mit \\(1\\) beobachten Werten, und die Spezifität (True Negative Rate) berechnet. Ziel ist es, dass beide Werte möglichst hoch sind. Sie können die Konfusionsmatrix “zu Fuß” berechnen, in dem Sie eine neue Variable einfügen, die ab dem cutpoint \\(1\\) und sonst \\(0\\) ist und mit dem Befehl tally() ausgeben. Alternativ können Sie das Paket SDMTools verwenden mit der Funktion confusion.matrix(). Ein Parameter ist cutpoint, der standardmäßig auf \\(0.5\\) steht. # Konfusionsmatrix &quot;zu Fuß&quot; berechnen # cutpoint = 0.5 setzen # neue Variable predicted anlegen mit 1, wenn modellierte Wahrscheinlichkeit &gt; 1 ist cutpoint = 0.5 Aktien$predicted &lt;- ((glm1$fitted.values) &gt; cutpoint)*1 # Kreuztabelle berechnen (cm &lt;- tally(~predicted+Aktienkauf, data = Aktien)) #&gt; Aktienkauf #&gt; predicted 0 1 #&gt; 0 509 163 #&gt; 1 8 20 # Sensitivität (TPR) cm[2,2]/sum(cm[,2]) #&gt; [1] 0.109 # Spezifität (TNR) cm[1,1]/sum(cm[,1]) #&gt; [1] 0.985 # mit Hilfe des Pakets SDMTools # ggf. install.packages(&quot;SDMTools&quot;) library(SDMTools) # optional noch Parameter cutpoint = 0.5 angeben (cm &lt;- confusion.matrix(Aktien$Aktienkauf, glm1$fitted.values)) #&gt; obs #&gt; pred 0 1 #&gt; 0 509 163 #&gt; 1 8 20 #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;confusion.matrix&quot; sensitivity(cm) #&gt; [1] 0.109 specificity(cm) #&gt; [1] 0.985 Wenn die Anteile der \\(1\\) in den beobachteten Daten sehr gering sind (z. B. bei einem medizinischem Test auf eine seltene Krankheit, Klicks auf einen Werbebanner oder Kreditausfall), kommt eine Schwäche der logistischen Regression zum Tragen: Das Modell wird so optimiert, dass die Wahrscheinlichkeiten \\(p(y=1)\\) alle unter \\(0.5\\) liegen. Das würde zu einer Sensitität von \\(0\\) und einer Spezifiät von \\(1\\) führen. Daher kann es sinnvoll sein, den Cutpoint zu varieren. Daraus ergibt sich ein verallgemeinertes Gütemaß, die ROC-Kurve (Return Operating Characteristic) und den daraus abgeleiteten AUC-Wert (Area Under Curve). Hierzu wird der Cutpoint zwischen 0 und 1 variiert und die Sensitivität gegen \\(1-\\)Spezifität (welche Werte sind als \\(1\\) modelliert worden unter den beobachten \\(0\\), False Positive Rate, FPR). Um diese Werte auszugeben, benötigen Sie das Paket ROCR und die Funktion performance(). # ggf. install.packages(&quot;ROCR&quot;) library(ROCR) # Ein für die Auswertung notwendiges prediction Objekt anlegen pred &lt;- prediction(glm1$fitted.values, Aktien$Aktienkauf) # ROC Kurve perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf) abline(0,1, col = &quot;grey&quot;) # Area under curve (ROC-Wert) performance(pred,&quot;auc&quot;)@y.values #&gt; [[1]] #&gt; [1] 0.636 AUC liegt zwischen \\(0.5\\), wenn das Modell gar nichts erklärt (im Plot die graue Linie) und \\(1\\). Hier ist der Wert also recht gering. Akzeptable Werte liegen bei \\(0.7\\) und größer, gute Werte sind es ab \\(0.8\\).34 6.2.8.2 Modellschätzung Das Modell wird nicht wie bei der lineare Regression über die Methode der kleinsten Quadrate (OLS) geschätzt, sondern über die Maximum Likelihood Methode. Die Koeffizienten werden so gewählt, dass die beobachteten Daten am wahrscheinlichsten (Maximum Likelihood) werden. Das ist ein iteratives Verfahren (OLS erfolgt rein analytisch), daher wird in der letzten Zeile der Ausgabe auch die Anzahl der Iterationen (Fisher Scoring Iterations) ausgegeben. Die Devianz des Modells (Residual deviance) ist \\(-2\\) mal die logarithmierte Likelihood. Die Nulldevianz (Null deviance) ist die Devianz eines Nullmodells, d. h., alle \\(\\beta\\) außer der Konstanten sind 0. 6.2.8.3 Likelihood Quotienten Test Der Likelihood Quotienten Test (Likelihood Ratio Test, LR-Test) vergleicht die Likelihood \\(L_0\\) des Nullmodels mit der Likelihood \\(L_{\\beta}\\) des geschätzten Modells. Die Prüfgröße des LR-Tests ergibt sich aus: \\[{T=-2\\cdot ln\\left( \\frac{L_0}{L_{\\beta}}\\right)}\\] \\(T\\) ist näherungsweise \\(\\chi ^2\\)-verteilt mit \\(k\\) Freiheitsgraden. In R können Sie den Test mit lrtest() aufrufen. Sie benötigen dazu das Paket lmtest. library(lmtest) lrtest(glm2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse #&gt; Model 2: Aktienkauf ~ 1 #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 4 -340 #&gt; 2 1 -402 -3 125 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Das Modell glm2 ist als Ganzes signifikant, der p-Wert ist sehr klein. Den Likelihood Quotienten Test können Sie auch verwenden, um zwei Modelle miteinander zu vergleichen, z. B., wenn Sie eine weitere Variable hinzugenommen haben und wissen wollen, ob die Verbesserung auch signifikant war. lrtest(glm1, glm2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Aktienkauf ~ Risikobereitschaft #&gt; Model 2: Aktienkauf ~ Risikobereitschaft + Einkommen + Interesse #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 2 -383 #&gt; 2 4 -340 2 86.9 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ja, die Modelle glm1 (mit einer erklärenden Variable) und glm2 unterscheiden sich signifikant voneinander. 6.2.8.4 Pseudo-R Verschiedene Statistiker haben versucht, aus der Likelihood eine Größe abzuleiten, die dem R der linearen Regression entspricht. Exemplarisch sei hier McFaddens R gezeigt: \\[{R^2=1-\\frac{ln(L_{\\beta})}{ln(L_0)}}\\] Wie bei bei dem R der linearen Regression liegt der Wertebereich zwischen 0 und 1. Ab einem Wert von 0,4 kann die Modellanpassung als gut eingestuft werden. Wo liegen R der beiden Modelle glm1 und glm2? Sie können es direkt berechnen oder das Paket BaylorEdPsych verwenden. # direkte Berechnung 1 - glm1$deviance/glm1$null.deviance #&gt; [1] 0.0479 1 - glm2$deviance/glm2$null.deviance #&gt; [1] 0.156 # ggf. install.packages(&quot;BaylorEdPsych&quot;) library(BaylorEdPsych) PseudoR2(glm1) #&gt; McFadden Adj.McFadden Cox.Snell Nagelkerke #&gt; 0.0479 0.0404 0.0535 0.0783 #&gt; McKelvey.Zavoina Effron Count Adj.Count #&gt; 0.0826 0.0584 0.7557 0.0656 #&gt; AIC Corrected.AIC #&gt; 769.8624 769.8796 PseudoR2(glm2) #&gt; McFadden Adj.McFadden Cox.Snell Nagelkerke #&gt; 0.1558 0.1434 0.1640 0.2400 #&gt; McKelvey.Zavoina Effron Count Adj.Count #&gt; 0.2828 0.1845 0.7614 0.0874 #&gt; AIC Corrected.AIC #&gt; 687.0068 687.0644 Insgesamt ist die Modellanpassung, auch mit allen Variablen, als schlecht zu bezeichnen. Hinweis: Die Funktion PseudoR2(model) zeigt verschiedene Pseudo-R Statistiken, die jeweils unter bestimmten Bedingungen vorteilhaft einzusetzen sind. Für weitere Erläuterungen sei auf die Literatur verwiesen. 6.2.9 Übung: Rot- oder Weißwein? Der Datensatz untersucht den Zusammenhang zwischen der Qualität und physiochemischen Eigenschaften von portugisieschen Rot- und Weißweinen. P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009. Sie können in hier). Die Originaldaten finden Sie im UCI Machine Learning Repository. Versuchen Sie anhand geeigneter Variablen, Rot- und Weißweine zu klassifizieren.35 Zusatzaufgabe: Die Originaldaten bestehen aus einem Datensatz für Weißweine und einem für Rotweine. Laden Sie diese, beachten Sie die Fehlermeldung und beheben die damit verbundenen Fehler und fassen beide Datensätze zu einem gemeinsamen Datensatz zusammen, in dem eine zusätzliche Variable color aufgenommen wird (Rot = 0, Weiß = 1). 6.2.10 Literatur David M. Diez, Christopher D. Barr, Mine Çetinkaya-Rundel (2014): Introductory Statistics with Randomization and Simulation, https://www.openintro.org/stat/textbook.php?stat_book=isrs, Kapitel 6.4 Nicholas J. Horton, Randall Pruim, Daniel T. Kaplan (2015): Project MOSAIC Little Books A Student’s Guide to R, https://github.com/ProjectMOSAIC/LittleBooks/raw/master/StudentGuide/MOSAIC-StudentGuide.pdf, Kapitel 8 Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013): An Introduction to Statistical Learning – with Applications in R, http://www-bcf.usc.edu/~gareth/ISL/, Kapitel 4.1-4.3 Maike Luhmann (2015): R für Einsteiger, Kapitel 17.5 Daniel Wollschläger (2014): Grundlagen der Datenanalyse mit R, Kapitel 8.1 6.3 Baumbasierte Verfahren 6.3.1 Konjunturanalyse Der B3 Datensatz Heilemann, U. and Münch, H.J. (1996): West German Business Cycles 1963-1994: A Multivariate Discriminant Analysis. CIRET–Conference in Singapore, CIRET–Studien 50. enthält Quartalsweise Konjunkturdaten aus (West-)Deutschland. B3 &lt;- read.csv2(&quot;https://goo.gl/0YCEHf&quot;) str(B3) # Datenstruktur #&gt; &#39;data.frame&#39;: 157 obs. of 14 variables: #&gt; $ PHASEN : int 2 2 3 3 3 3 3 3 3 3 ... #&gt; $ BSP91JW : num 10.53 10.6 9.21 5.17 4.93 ... #&gt; $ CP91JW : num 9.31 12.66 6.55 7.87 8.6 ... #&gt; $ DEFRATE : num 0.05 0.06 0.05 0.05 0.04 0.04 0.04 0.03 0.03 0 ... #&gt; $ EWAJW : num 5.7 5.2 4.8 3.3 2.1 3.2 2.5 2.7 3 0.3 ... #&gt; $ EXIMRATE: num 3.08 1.96 2.82 3.74 4.16 2.9 3.65 4.57 4.37 2.89 ... #&gt; $ GM1JW : num 11.15 11.03 10.04 8.33 7.69 ... #&gt; $ IAU91JW : num 23.56 12.72 11.52 0.85 -2.08 ... #&gt; $ IB91JW : num 14.69 24.95 14.9 7.55 3.23 ... #&gt; $ LSTKJW : num 3 2.36 3.39 5.3 6.91 1.03 3.73 6.2 4.12 7.94 ... #&gt; $ PBSPJW : num 2.89 2.59 3.01 3.03 3.46 1.95 3.18 3.98 3.29 5.63 ... #&gt; $ PCPJW : num 1.91 2.2 3.09 2.08 1.48 1.65 1.47 3.29 3.59 4.19 ... #&gt; $ ZINSK : num 6.27 4.6 6.19 6.71 7.1 4.96 5.21 4.83 4.5 3.83 ... #&gt; $ ZINSLR : num 3.21 3.54 3.22 3.37 3.14 4.95 3.82 3.09 3.91 1.47 ... head(B3); tail(B3) #&gt; PHASEN BSP91JW CP91JW DEFRATE EWAJW EXIMRATE GM1JW IAU91JW IB91JW LSTKJW #&gt; 1 2 10.53 9.31 0.05 5.7 3.08 11.15 23.56 14.69 3.00 #&gt; 2 2 10.60 12.66 0.06 5.2 1.96 11.03 12.72 24.95 2.36 #&gt; 3 3 9.21 6.55 0.05 4.8 2.82 10.04 11.52 14.90 3.39 #&gt; 4 3 5.17 7.87 0.05 3.3 3.74 8.33 0.85 7.55 5.30 #&gt; 5 3 4.93 8.60 0.04 2.1 4.16 7.69 -2.08 3.23 6.91 #&gt; 6 3 8.39 5.62 0.04 3.2 2.90 6.62 -3.76 14.58 1.03 #&gt; PBSPJW PCPJW ZINSK ZINSLR #&gt; 1 2.89 1.91 6.27 3.21 #&gt; 2 2.59 2.20 4.60 3.54 #&gt; 3 3.01 3.09 6.19 3.22 #&gt; 4 3.03 2.08 6.71 3.37 #&gt; 5 3.46 1.48 7.10 3.14 #&gt; 6 1.95 1.65 4.96 4.95 #&gt; PHASEN BSP91JW CP91JW DEFRATE EWAJW EXIMRATE GM1JW IAU91JW IB91JW #&gt; 152 3 -1.27 1.29 -4.87 -1.97 6.03 9.79 -18.29 1.73 #&gt; 153 3 -2.13 -0.57 -2.98 -2.05 7.59 0.72 -15.82 -3.23 #&gt; 154 3 1.39 2.33 -2.86 -1.84 7.49 11.33 -10.59 4.62 #&gt; 155 4 1.63 0.64 1.20 -1.58 7.75 11.38 -4.90 3.62 #&gt; 156 1 1.40 0.57 -3.56 -1.34 5.58 9.53 -0.76 2.19 #&gt; 157 1 1.83 -0.08 -2.22 -0.93 7.50 15.20 2.75 6.12 #&gt; LSTKJW PBSPJW PCPJW ZINSK ZINSLR #&gt; 152 1.08 2.73 2.98 6.83 3.55 #&gt; 153 1.67 2.67 3.31 6.35 3.05 #&gt; 154 -0.12 2.66 2.94 5.88 3.17 #&gt; 155 -1.81 1.77 2.58 5.29 4.82 #&gt; 156 -1.54 1.85 2.60 5.01 5.27 #&gt; 157 -0.92 1.79 2.49 5.28 5.62 Dabei sind folgende Variablen enthalten: Bruttosozialprodukt (real): BSP91JW Privater Verbrauch (real): CP91JW Anteil Staatsdefizit am Bruttosozialprodukt (%): DEFRATE Abhängig Erwerbstätige: EWAJW Anteil Außenbeitrag am Bruttosozialprodukt (%): EXIMRATE Geldmenge M1: GM1JW Investitionen in Ausrüstungsgüter (real): IAU91JW Investitionen in Bauten (real): IB91JW Lohnstückkosten: LSTKJW Preisindex des Bruttosozialprodukts: PBSPJW Preisindex des privaten Verbrauchs: PCPJW Kurzfristiger Zinssatz (nominal): ZINSK Langfristiger Zinssatz (real): ZINSLR Konjunkturphase: 1. Aufschwung, 2. Oberer Wendepunkt, 3. Abschwung,4. Unterer Wendepunkt: PHASEN Variablen mit der Endung JW beziehen sich auf die jährliche Veränderung. 6.3.2 Regressionsbäume Um einen Regressionsbaum zu erzeugen, muss zunächst das Zusatzpaket rpart geladen werden: library(rpart) Um z. B. die Veränderung des Bruttosozialprodukt als Funktion von Privater Verbrauch, Investitionen in Ausrüstungsgüter, Investitionen in Bauten und Geldmenge M1 als Regressionsbaum zu modellieren reicht der Befehl regbaum &lt;- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3) Um das Ergebnis auszugeben genügt: regbaum #&gt; n= 157 #&gt; #&gt; node), split, n, deviance, yval #&gt; * denotes terminal node #&gt; #&gt; 1) root 157 1380.0 3.5000 #&gt; 2) CP91JW&lt; 3.71 79 380.0 1.5200 #&gt; 4) IAU91JW&lt; -0.365 38 120.0 -0.0379 #&gt; 8) IB91JW&lt; -2.22 20 45.1 -0.9020 * #&gt; 9) IB91JW&gt;=-2.22 18 43.9 0.9220 * #&gt; 5) IAU91JW&gt;=-0.365 41 81.6 2.9600 #&gt; 10) IB91JW&lt; 2.42 22 29.5 2.2100 * #&gt; 11) IB91JW&gt;=2.42 19 25.0 3.8400 * #&gt; 3) CP91JW&gt;=3.71 78 378.0 5.5000 #&gt; 6) IAU91JW&lt; 11.3 50 142.0 4.5700 #&gt; 12) IB91JW&lt; 3.2 22 40.0 3.5500 * #&gt; 13) IB91JW&gt;=3.2 28 61.3 5.3700 * #&gt; 7) IAU91JW&gt;=11.3 28 114.0 7.1700 #&gt; 14) IB91JW&lt; 7.55 17 51.2 6.2200 * #&gt; 15) IB91JW&gt;=7.55 11 23.7 8.6400 * Lesebeispiel: Wenn CP91JW&gt;=3.705 und IAU91JW&gt;=11.335 und IB91JW&gt;=7.55 liegt, dann liegt die durchschnittliche Veränderung des BSP91JW bei 8.639. 11 Beobachtungen erfüllen die Kriterien der unabhängigen Variablen Bzw. um den Baum zu zeichnen par(xpd = TRUE) # Grafikparameter der sicherstellt, dass alles ins Bild passt plot(regbaum, compress = TRUE) # Baum zeichnen text(regbaum) # Baum beschriften Eine deutlich schönere Ausgabe erhält man z. B. mit dem Zusatzpaket rpart.plot, welches man einmalig über install.packages(&quot;rpart.plot&quot;) installieren muss und dann benutzen kann. Zunächst laden library(rpart.plot) und dann zeichnen: rpart.plot(regbaum) 6.3.2.1 Kreuzvalidierung 6.3.2.1.1 Anpassungsgüte Wie gut ist das Modell? Über predict können die Punktprognosen berechnet werden: head(predict(regbaum)) #&gt; 1 2 3 4 5 6 #&gt; 8.64 8.64 8.64 5.37 5.37 5.37 Diese werden mit den beobachteten Werten verglichen: head(B3$BSP91JW) #&gt; [1] 10.53 10.60 9.21 5.17 4.93 8.39 Der Mean Squared Error ist dann baummse &lt;- mean( (predict(regbaum) - B3$BSP91JW)^2 ) baummse #&gt; [1] 2.04 Vergleichen wir das Ergebnis mit dem einer linearen Regression reglm &lt;- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3) summary(reglm) #&gt; #&gt; Call: #&gt; lm(formula = BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data = B3) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.048 -0.880 -0.057 0.801 3.674 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.22442 0.26280 4.66 6.9e-06 *** #&gt; CP91JW 0.38729 0.05846 6.63 5.7e-10 *** #&gt; IAU91JW 0.12752 0.01634 7.80 9.0e-13 *** #&gt; IB91JW 0.13880 0.01719 8.08 1.9e-13 *** #&gt; GM1JW -0.00996 0.02676 -0.37 0.71 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.37 on 152 degrees of freedom #&gt; Multiple R-squared: 0.792, Adjusted R-squared: 0.786 #&gt; F-statistic: 145 on 4 and 152 DF, p-value: &lt;2e-16 Der MSE der Linearen Regression liegt bei lmmse &lt;- mean( (predict(reglm) - B3$BSP91JW)^2 ) lmmse #&gt; [1] 1.83 Der Baum ist einfacher und weniger flexibel, aber auch schlechter im Bezug auf die Anpassungsgüte. 6.3.2.1.2 Prognosegüte Für eine k=3 fache Kreuzvalidierung müssen 3 Testdatensätze erzeugt werden. Zunächst wird dafür ein Aufteilungsvektor gebildet: aufteilung &lt;- rep(1:3, length.out=nrow(B3)) und dann wird aufgeteilt: test1 &lt;- B3[aufteilung==1,] train1 &lt;- B3[aufteilung!=1,] test2 &lt;- B3[aufteilung==2,] train2 &lt;- B3[aufteilung!=2,] test3 &lt;- B3[aufteilung==3,] train3 &lt;- B3[aufteilung!=3,] Anschließend werden die Modelle auf den Trainingsdaten geschätzt, und auf den Testdaten überprüft: # Runde 1 b1 &lt;- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1) l1 &lt;- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1) mseb1 &lt;- mean( (predict(b1, newdata = test1) - test1$BSP91JW)^2 ) msel1 &lt;- mean( (predict(l1, newdata = test1) - test1$BSP91JW)^2 ) # Runde 2 b2 &lt;- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2) l2 &lt;- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2) mseb2 &lt;- mean( (predict(b2, newdata = test2) - test2$BSP91JW)^2 ) msel2 &lt;- mean( (predict(l2, newdata = test2) - test2$BSP91JW)^2 ) # Runde 3 b3 &lt;- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3) l3 &lt;- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3) mseb3 &lt;- mean( (predict(b3, newdata = test3) - test3$BSP91JW)^2 ) msel3 &lt;- mean( (predict(l3, newdata = test3) - test3$BSP91JW)^2 ) # Ergebnisse zusammenfassen msecvb &lt;- c(mseb1, mseb2, mseb3) msecvl &lt;- c(msel1, msel2, msel3) # Mittelwert des Prognose MSE mean(msecvb) #&gt; [1] 3.62 mean(msecvl) #&gt; [1] 1.99 Bei den vorliegenden Daten ist also ein lineares Modell dem Baummodell im Bezug auf den MSE überlegen. Hinweis: In der Praxis führt man die Aufteilung nicht manuell sondern innerhalb von Schleifen durch. 6.3.3 Klassifikationbäume Untersuchen wir, ob makroökonomische Kennzahlen geeignet sind, die Konjunkturphasen zu unterscheiden. Zunächst stellen wir fest, dass die eigentlich kategorielle Variable PHASEN hier numerisch kodiert wurde, was aber schnell verwirren würde. typeof(B3$PHASEN) #&gt; [1] &quot;integer&quot; Typänderung zu factor geht einfach: B3$PHASEN &lt;- as.factor(B3$PHASEN) Wenn wir die einzelnen levels des Faktors als numerische Werte verwenden wollen würde man den Befehl as.numeric() verwenden. Aber sicherheitshalber vorher über levels() gucken, ob die Reihenfolge auch stimmt. Um die Interpretation zu erleichtern können wir hier einfach die Faktorstufe umbenennen. levels(B3$PHASEN) &lt;- c(&quot;Aufschwung&quot;, &quot;Oberer Wendepunkt&quot;, &quot;Abschwung&quot;, &quot;Unterer Wendepunkt&quot;) Um z. B. die Konjunkturphase als Funktion von Privater Verbrauch, Investitionen in Ausrüstungsgüter, Investitionen in Bauten und Geldmenge M1 als Regressionsbaum zu modellieren reicht jetzt der Befehl klassbaum &lt;- rpart(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3) Um das Ergebnis auszugeben genügt: klassbaum #&gt; n= 157 #&gt; #&gt; node), split, n, loss, yval, (yprob) #&gt; * denotes terminal node #&gt; #&gt; 1) root 157 98 Aufschwung (0.3758 0.1529 0.2994 0.1720) #&gt; 2) IAU91JW&gt;=-0.09 109 55 Aufschwung (0.4954 0.2110 0.2110 0.0826) #&gt; 4) CP91JW&lt; 5.19 68 20 Aufschwung (0.7059 0.1029 0.1324 0.0588) #&gt; 8) IB91JW&gt;=3.32 29 7 Aufschwung (0.7586 0.2069 0.0345 0.0000) * #&gt; 9) IB91JW&lt; 3.32 39 13 Aufschwung (0.6667 0.0256 0.2051 0.1026) #&gt; 18) GM1JW&lt; 11 32 7 Aufschwung (0.7813 0.0312 0.1250 0.0625) #&gt; 36) IAU91JW&gt;=3.25 25 3 Aufschwung (0.8800 0.0400 0.0000 0.0800) * #&gt; 37) IAU91JW&lt; 3.25 7 3 Abschwung (0.4286 0.0000 0.5714 0.0000) * #&gt; 19) GM1JW&gt;=11 7 3 Abschwung (0.1429 0.0000 0.5714 0.2857) * #&gt; 5) CP91JW&gt;=5.19 41 25 Oberer Wendepunkt (0.1463 0.3902 0.3415 0.1220) #&gt; 10) IAU91JW&gt;=7.49 31 15 Oberer Wendepunkt (0.1613 0.5161 0.2581 0.0645) #&gt; 20) CP91JW&gt;=7.69 10 2 Oberer Wendepunkt (0.1000 0.8000 0.1000 0.0000) * #&gt; 21) CP91JW&lt; 7.69 21 13 Oberer Wendepunkt (0.1905 0.3810 0.3333 0.0952) #&gt; 42) CP91JW&lt; 6.2 8 3 Oberer Wendepunkt (0.2500 0.6250 0.1250 0.0000) * #&gt; 43) CP91JW&gt;=6.2 13 7 Abschwung (0.1538 0.2308 0.4615 0.1538) * #&gt; 11) IAU91JW&lt; 7.49 10 4 Abschwung (0.1000 0.0000 0.6000 0.3000) * #&gt; 3) IAU91JW&lt; -0.09 48 24 Abschwung (0.1042 0.0208 0.5000 0.3750) #&gt; 6) GM1JW&lt; 11.4 38 14 Abschwung (0.0789 0.0000 0.6316 0.2895) #&gt; 12) IB91JW&gt;=-4.08 23 5 Abschwung (0.1304 0.0000 0.7826 0.0870) * #&gt; 13) IB91JW&lt; -4.08 15 6 Unterer Wendepunkt (0.0000 0.0000 0.4000 0.6000) * #&gt; 7) GM1JW&gt;=11.4 10 3 Unterer Wendepunkt (0.2000 0.1000 0.0000 0.7000) * Lesebeispiel: Wenn IAU91JW&lt; -0.09 und GM1JW&gt;=11.355 liegt, dann ist der Untere Wendepunkt die häufigste Merkmalsausprägung von PHASEN (relative Häufigkeit von PHASEN=4 hier: 0.7) 10 Beobachtungen erfüllen die Kriterien der unabhängigen Variablen. par(xpd = TRUE) # Grafikparameter der sicherstellt, dass alles ins Bild passt plot(klassbaum, compress = TRUE) # Baum zeichnen text(klassbaum) # Baum beschriften Bzw. “schöner”: rpart.plot(klassbaum) 6.3.3.1 Kreuzvalidierung Wie gut ist das Modell? Auch hier können über predict die Punktprognosen bestimmt werden: head(predict(klassbaum, type=&quot;class&quot;)) #&gt; 1 2 3 4 #&gt; Oberer Wendepunkt Oberer Wendepunkt Abschwung Abschwung #&gt; 5 6 #&gt; Abschwung Abschwung #&gt; Levels: Aufschwung Oberer Wendepunkt Abschwung Unterer Wendepunkt Diese werden mit den beobachteten Werten verglichen: head(B3$PHASEN) #&gt; [1] Oberer Wendepunkt Oberer Wendepunkt Abschwung Abschwung #&gt; [5] Abschwung Abschwung #&gt; Levels: Aufschwung Oberer Wendepunkt Abschwung Unterer Wendepunkt Die Fehlklassifikationsrate ist dann baumer &lt;- mean( (predict(klassbaum, type=&quot;class&quot;) != B3$PHASEN) ) baumer #&gt; [1] 0.293 also knapp 30%. Vergleichen kann man den Klassifikationsbaum z. B. mit der Linearen Diskriminanzanalyse. Diese ist im Paket MASS implementiert. library(MASS) klasslda &lt;- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3) klasslda #&gt; Call: #&gt; lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data = B3) #&gt; #&gt; Prior probabilities of groups: #&gt; Aufschwung Oberer Wendepunkt Abschwung #&gt; 0.376 0.153 0.299 #&gt; Unterer Wendepunkt #&gt; 0.172 #&gt; #&gt; Group means: #&gt; CP91JW IAU91JW IB91JW GM1JW #&gt; Aufschwung 3.55 6.8936 3.40 8.61 #&gt; Oberer Wendepunkt 6.43 11.3604 6.83 11.02 #&gt; Abschwung 3.66 -0.0034 1.67 6.84 #&gt; Unterer Wendepunkt 2.62 -1.9393 -1.49 9.61 #&gt; #&gt; Coefficients of linear discriminants: #&gt; LD1 LD2 LD3 #&gt; CP91JW 0.1583 -0.52094 0.0758 #&gt; IAU91JW -0.1409 0.06865 -0.0275 #&gt; IB91JW -0.0478 0.00242 -0.0457 #&gt; GM1JW -0.0203 0.08559 0.2245 #&gt; #&gt; Proportion of trace: #&gt; LD1 LD2 LD3 #&gt; 0.645 0.230 0.125 ldaer &lt;- mean( (predict(klasslda)$class != B3$PHASEN) ) ldaer #&gt; [1] 0.414 Im Bezug auf die Klassifikation scheint der Baum in der Anpassungsgüte besser als die Lineare Diskriminanzanalyse zu sein. Aber wie sieht es kreuzvalidiert, d. h. in der Prognose aus? Zunächst wird wieder dafür ein Aufteilungsvektor gebildet: aufteilung &lt;- rep(1:3, length.out=nrow(B3)) und dann wird aufgeteilt: test1 &lt;- B3[aufteilung==1,] train1 &lt;- B3[aufteilung!=1,] test2 &lt;- B3[aufteilung==2,] train2 &lt;- B3[aufteilung!=2,] test3 &lt;- B3[aufteilung==3,] train3 &lt;- B3[aufteilung!=3,] # Runde 1 b1 &lt;- rpart(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1) l1 &lt;- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1) erb1 &lt;- mean( (predict(b1, newdata = test1, type = &quot;class&quot;) != test1$PHASEN) ) erl1 &lt;- mean( (predict(l1, newdata = test1)$class != test1$PHASEN) ) # Runde 2 b2 &lt;- rpart(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2) l2 &lt;- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2) erb2 &lt;- mean( (predict(b2, newdata = test2, type = &quot;class&quot;) != test2$PHASEN) ) erl2 &lt;- mean( (predict(l2, newdata = test2)$class != test2$PHASEN) ) # Runde 3 b3 &lt;- rpart(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3) l3 &lt;- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3) erb3 &lt;- mean( (predict(b3, newdata = test3, type = &quot;class&quot;) != test3$PHASEN) ) erl3 &lt;- mean( (predict(l3, newdata = test3)$class != test3$PHASEN) ) # Ergebnisse zusammenfassen ercvb &lt;- c(erb1, erb2, erb3) ercvl &lt;- c(erl1, erl2, erl3) # Mittelwert des Prognose MSE mean(ercvb) #&gt; [1] 0.49 mean(ercvl) #&gt; [1] 0.44 In der Prognosegüte ist hier – anders als in der Anpassungsgüte – die Lineare Diskriminanzanalyse besser. 6.3.4 Parameter rpart Neben dem Splitkrierium können verschiedene Parameter des Algorithmus eingestellt werden (siehe ?rpart.control), u. a.: minsplit: Minimale Anzahl Beobachtungen im Knoten damit Aufteilung versucht wird minbucket: Minimale Anzahl Beobachtungen im Blatt cp: Komplexitätsparameter (pruning) xval: Anzahl Kreuzvaliderungen (pruning) maxdepth: Maximale Tiefe eines Blattes Diese können mit der Funktion train aus dem Paket caret automatisch optimiert werden. Alternativen/ Ergänzungen zu rpart: tree partykit Erweiterung: Viele Bäume: randomForest 6.4 Ausblick 6.5 Fallstudie: Überleben auf der Titanic In dieser YACSDA (Yet-another-case-study-on-data-analysis) geht es um die beispielhafte Analyse nominaler Daten anhand des “klassischen” Falls zum Untergang der Titanic. Eine Frage, die sich hier aufdrängt, lautet: Kann (konnte) man sich vom Tod freikaufen, etwas polemisch formuliert. Oder neutraler: Hängt die Überlebensquote von der Klasse, in der derPassagiers reist, ab? 6.5.1 Daten und Pakete laden # install.packages(&quot;titanic&quot;) library(&quot;titanic&quot;) data(titanic_train) Man beachte, dass ein Paket nur einmalig zu installieren ist (wie jede Software). Dann aber muss das Paket bei jedem Starten von R wieder von neuem gestartet werden. Außerdem ist es wichtig zu wissen, dass das Laden eines Pakets nicht automatisch die Datensätze aus dem Paket lädt. Man muss das oder die gewünschten Pakete selber (mit data(...)) laden. Und: Der Name eines Pakets (z.B. titanic) muss nicht identisch sein mit dem oder den Datensätzen des Pakets (z.B. titanic_train). library(tidyverse) 6.5.2 Erster Blick Werfen wir einen ersten Blick in die Daten: # install.packages(&quot;dplyr&quot;, dependencies = TRUE) # ggf. vorher installieren glimpse(titanic_train) #&gt; Observations: 891 #&gt; Variables: 12 #&gt; $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,... #&gt; $ Survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,... #&gt; $ Pclass &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3,... #&gt; $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bra... #&gt; $ Sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;mal... #&gt; $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, ... #&gt; $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4,... #&gt; $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1,... #&gt; $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;1138... #&gt; $ Fare &lt;dbl&gt; 7.25, 71.28, 7.92, 53.10, 8.05, 8.46, 51.86, 21.07... #&gt; $ Cabin &lt;chr&gt; &quot;&quot;, &quot;C85&quot;, &quot;&quot;, &quot;C123&quot;, &quot;&quot;, &quot;&quot;, &quot;E46&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, ... #&gt; $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, ... 6.5.3 Welche Variablen sind interessant? Von 12 Variablen des Datensatzes interessieren uns offenbar Pclass und Survived; Hilfe zum Datensatz kann man übrigens mit help(titanic_train) bekommen. Diese beiden Variablen sind kategorial (nicht-metrisch), wobei sie in der Tabelle mit Zahlen kodiert sind. Natürlich ändert die Art der Codierung (hier als Zahl) nichts am eigentlichen Skalenniveau. Genauso könnte man “Mann” mit 1 und “Frau” mit 2 kodieren; ein Mittelwert bliebe genauso (wenig) aussagekräftig. Zu beachten ist hier nur, dass sich manche R-Befehle verunsichern lassen, wenn nominale Variablen mit Zahlen kodiert sind. Daher ist es oft besser, nominale Variablen mit Text-Werten zu benennen (wie “survived” vs. “drowned” etc.). Wir kommen später auf diesen Punkt zurück. 6.5.4 Univariate Häufigkeiten Bevor wir uns in kompliziertere Fragestellungen stürzen, halten wir fest: Wir untersuchen zwei nominale Variablen. Sprich: wir werden Häufigkeiten auszählen. Häufigkeiten (und relative Häufigkeiten, also Anteile oder Quoten) sind das, was uns hier beschäftigt. Zählen wir zuerst die univariaten Häufigkeiten aus: Wie viele Passagiere gab es pro Klasse? Wie viele Passagiere gab es pro Wert von Survived (also die überlebten bzw. nicht überlebten)? c1 &lt;- dplyr::count(titanic_train, Pclass) c1 #&gt; # A tibble: 3 × 2 #&gt; Pclass n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 216 #&gt; 2 2 184 #&gt; 3 3 491 Achtung - Namenskollision! Sowohl im Paket mosaic als auch im Paket dplyr gibt es einen Befehl count. Für select gilt das gleiche. Das arme R weiß nicht, welchen von beiden wir meinen und entscheidet sich im Zweifel für den falschen. Da hilft, zu sagen, aus welchem Paket wir den Befehl beziehen wollen. Das macht der Operator ::. Aha. Zur besseren Anschaulichkeit können wir das auch plotten (ein Diagramm dazu malen). # install.packages(&quot;ggplot2&quot;, dependencies = TRUE) library(ggplot2) qplot(x = Pclass, y = n, data = c1) Der Befehl qplot zeichnet automatisch Punkte, wenn auf beiden Achsen “Zahlen-Variablen” stehen (also Variablen, die keinen “Text”, sondern nur Zahlen beinhalten. In R sind das Variablen vom Typ int (integer), also Ganze Zahlen oder vom Typ num (numeric), also reelle Zahlen). c2 &lt;- dplyr::count(titanic_train, Survived) c2 #&gt; # A tibble: 2 × 2 #&gt; Survived n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 549 #&gt; 2 1 342 Man beachte, dass der Befehl count stehts eine Tabelle (data.frame bzw. tibble) verlangt und zurückliefert. 6.5.5 Bivariate Häufigkeiten OK, gut. Jetzt wissen wir die Häufigkeiten pro Wert von Survived (dasselbe gilt für Pclass). Eigentlich interessiert uns aber die Frage, ob sich die relativen Häufigkeiten der Stufen von Pclass innerhalb der Stufen von Survived unterscheiden. Einfacher gesagt: Ist der Anteil der Überlebenden in der 1. Klasse größer als in der 3. Klasse? Zählen wir zuerst die Häufigkeiten für alle Kombinationen von Survived und Pclass: c3 &lt;- dplyr::count(titanic_train, Survived, Pclass) c3 #&gt; Source: local data frame [6 x 3] #&gt; Groups: Survived [?] #&gt; #&gt; Survived Pclass n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 1 80 #&gt; 2 0 2 97 #&gt; 3 0 3 372 #&gt; 4 1 1 136 #&gt; 5 1 2 87 #&gt; 6 1 3 119 Da Pclass 3 Stufen hat (1., 2. und 3. Klasse) und innerhalb jeder dieser 3 Klassen es die Gruppe der Überlebenden und der Nicht-Überlebenden gibt, haben wir insgesamt 3*2=6 Gruppen. Es ist hilfreich, sich diese Häufigkeiten wiederum zu plotten; wir nehmen den gleichen Befehl wie oben. qplot(x = Pclass, y = n, data = c3) Hm, nicht so hilfreich. Schöner wäre, wenn wir (farblich) erkennen könnten, welcher Punkt für “Überlebt” und welcher Punkt für “Nicht-Überlebt” steht. Mit qplot geht das recht einfach: Wir sagen der Funktion qplot, dass die Farbe (color) der Punkte den Stufen von Survived zugeordnet werden sollen: qplot(x = Pclass, y = n, color = Survived, data = c3) Viel besser. Was noch stört, ist, dass Survived als metrische Variable verstanden wird. Das Farbschema lässt Nuancen, feine Farbschattierungen, zu. Für nominale Variablen macht das keinen Sinn; es gibt da keine Zwischentöne. Tot ist tot, lebendig ist lebendig. Wir sollten daher der Funktion sagen, dass es sich um nominale Variablen handelt: qplot(x = factor(Pclass), y = n, color = factor(Survived), data = c3) Viel besser. Jetzt noch ein bisschen Schnickschnack: qplot(x = factor(Pclass), y = n, color = factor(Survived), data = c3) + labs(x = &quot;Klasse&quot;, title = &quot;Überleben auf der Titanic&quot;, colour = &quot;Überlebt?&quot;) 6.5.6 Signifikanztest Manche Leute mögen Signifikanztests. Ich persönlich stehe ihnen kritisch gegenüber, da ein p-Wert eine Funktion der Stichprobengröße ist und außerdem zumeist missverstanden wird (er gibt nicht die Wahrscheinlichkeit der getesteten Hypothese an, was die Frage aufwirft, warum er mich dann interessieren sollte). Aber seisdrum, berechnen wir mal einen p-Wert. Es gibt mehrere statistische Tests, die sich hier potenziell anböten (was die Frage nach der Objektivität von statistischen Tests in ein ungünstiges Licht rückt). Nehmen wir den \\(\\chi^2\\)-Test. chisq.test(titanic_train$Survived, titanic_train$Pclass) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: titanic_train$Survived and titanic_train$Pclass #&gt; X-squared = 100, df = 2, p-value &lt;2e-16 Der p-Wert ist kleiner als 5%, daher entscheiden wir uns, entsprechend der üblichen Gepflogenheit, gegen die H0 und für die H1: “Es gibt einen Zusammenhang von Überlebensrate und Passagierklasse”. 6.5.7 Effektstärke Abgesehen von der Signifikanz, und interessanter, ist die Frage, wie sehr die Variablen zusammenhängen. Für Häufigkeitsanalysen mit 2*2-Feldern bietet sich das “Odds Ratio” (OR), das Chancenverhältnis an. Das Chancen-Verhältnis beantwortet die Frage: “Um welchen Faktor ist die Überlebenschance in der einen Klasse größer als in der anderen Klasse?”. Eine interessante Frage, als schauen wir es uns an. Das OR ist nur definiert für 2*2-Häufigkeitstabellen, daher müssen wir die Anzahl der Passagierklassen von 3 auf 2 verringern. Nehmen wir nur 1. und 3. Klasse, um den vermuteten Effekt deutlich herauszuschälen: t2 &lt;- filter(titanic_train, Pclass != 2) # &quot;!=&quot; heißt &quot;nicht&quot; Alternativ (synonym) könnten wir auch schreiben: t2 &lt;- filter(titanic_train, Pclass == 1 | Pclass == 3) # &quot;|&quot; heißt &quot;oder&quot; Und dann zählen wir wieder die Häufigkeiten aus pro Gruppe: c4 &lt;- dplyr::count(t2, Pclass) c4 #&gt; # A tibble: 2 × 2 #&gt; Pclass n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 216 #&gt; 2 3 491 Schauen wir nochmal den p-Wert an, da wir jetzt ja mit einer veränderten Datentabelle operieren: chisq.test(t2$Survived, t2$Pclass) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: t2$Survived and t2$Pclass #&gt; X-squared = 100, df = 1, p-value &lt;2e-16 Ein \\(\\chi^2\\)-Wert von ~96 bei einem n von 707. Dann berechnen wir die Effektstärke (OR) mit dem Paket compute.es (muss ebenfalls installiert sein). library(compute.es) chies(chi.sq = 96, n = 707) #&gt; Mean Differences ES: #&gt; #&gt; d [ 95 %CI] = 0.79 [ 0.63 , 0.95 ] #&gt; var(d) = 0.01 #&gt; p-value(d) = 0 #&gt; U3(d) = 78.6 % #&gt; CLES(d) = 71.2 % #&gt; Cliff&#39;s Delta = 0.42 #&gt; #&gt; g [ 95 %CI] = 0.79 [ 0.63 , 0.95 ] #&gt; var(g) = 0.01 #&gt; p-value(g) = 0 #&gt; U3(g) = 78.6 % #&gt; CLES(g) = 71.2 % #&gt; #&gt; Correlation ES: #&gt; #&gt; r [ 95 %CI] = 0.37 [ 0.3 , 0.43 ] #&gt; var(r) = 0 #&gt; p-value(r) = 0 #&gt; #&gt; z [ 95 %CI] = 0.39 [ 0.31 , 0.46 ] #&gt; var(z) = 0 #&gt; p-value(z) = 0 #&gt; #&gt; Odds Ratio ES: #&gt; #&gt; OR [ 95 %CI] = 4.21 [ 3.15 , 5.61 ] #&gt; p-value(OR) = 0 #&gt; #&gt; Log OR [ 95 %CI] = 1.44 [ 1.15 , 1.73 ] #&gt; var(lOR) = 0.02 #&gt; p-value(Log OR) = 0 #&gt; #&gt; Other: #&gt; #&gt; NNT = 3.57 #&gt; Total N = 707 Die Chance zu überleben ist also in der 1. Klasse mehr als 4 mal so hoch wie in der 3. Klasse. Es scheint: Money buys you live… 6.5.8 Logististische Regression Berechnen wir noch das Odds Ratio mit Hilfe der logistischen Regression. Zum Einstieg: Ignorieren Sie die folgende Syntax und schauen Sie sich das Diagramm an. Hier sehen wir die (geschätzten) Überlebens-Wahrscheinlichkeiten für Passagiere der 1. Klasse vs. Passagiere der 3. Klasse. titanic2 &lt;- titanic_train %&gt;% filter(Pclass %in% c(1,3)) %&gt;% mutate(Pclass = factor(Pclass)) glm1 &lt;- glm(data = titanic2, formula = Survived ~ Pclass, family = &quot;binomial&quot;) exp(coef(glm1)) #&gt; (Intercept) Pclass3 #&gt; 1.700 0.188 titanic2$pred_prob &lt;- predict(glm1, type = &quot;response&quot;) Wir sehen, dass die Überlebens-Wahrscheinlichkeit in der 1. Klasse höher ist als in der 3. Klasse. Optisch grob geschätzt, ~60% in der 1. Klasse und ~25% in der 3. Klasse. Schauen wir uns die logistische Regression an: Zuerst haben wir den Datensatz auf die Zeilen beschränkt, in denen Personen aus der 1. und 3. Klasse vermerkt sind (zwecks Vergleichbarkeit zu oben). Dann haben wir mit glm und family = &quot;binomial&quot; eine logistische Regression angefordert. Man beachte, dass der Befehl sehr ähnlich zur normalen Regression (lm(...)) ist. Da die Koeffizienten in der Logit-Form zurückgegeben werden, haben wir sie mit der Exponential-Funktion in die “normale” Odds-Form gebracht (delogarithmiert, boa). Wir sehen, dass die Überlebens-Chance (Odds) 1.7 zu 1 betrug - bei der ersten Stufe von Pclass (1)36; von 27 Menschen überlebten in dieser Gruppe also 17 (17/27 = .63 Überlebens-Wahrscheinlichkeit); s. Intercept; der Achsenabschnitt gibt den Odds an, wenn die Prädiktor-Variable(n) den Wert “Null” hat/ haben, bzw. die erste Ausprägung, hier 1. Im Vergleich dazu wird die Überlebens-Chance deutlich schlechter, wenn man die nächste Gruppe von Pclass (3) betrachtet. Die Odds verändern sich um den Faktor ~0.2. Da der Faktor kleiner als 1 ist, ist das kein gutes Zeichen. Die Überlebens-Chance sinkt; etwas genauer auf: 1.7 * 0.2 ≈ 0.34. Das heißt, die Überlebens-Chance ist in der 3. Klasse nur noch ca. 1 zu 3 (Überlebens-Wahrscheinlichkeit: ~25%). Komfortabler können wir uns die Überlebens-Wahrscheinlichkeiten mit der Funktion predict ausgeben lassen. predict(glm1, newdata = data.frame(Pclass = factor(&quot;1&quot;)), type = &quot;response&quot;) #&gt; 1 #&gt; 0.63 predict(glm1, newdata = data.frame(Pclass = factor(&quot;3&quot;)), type = &quot;response&quot;) #&gt; 1 #&gt; 0.242 Alternativ kann man die Häufigkeiten auch noch “per Hand” bestimmen: titanic_train %&gt;% filter(Pclass %in% c(1,3)) %&gt;% dplyr::select(Survived, Pclass) %&gt;% group_by(Pclass, Survived) %&gt;% summarise(n = n() ) %&gt;% mutate(Anteil = n / sum(n)) #&gt; Source: local data frame [4 x 4] #&gt; Groups: Pclass [2] #&gt; #&gt; Pclass Survived n Anteil #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 0 80 0.370 #&gt; 2 1 1 136 0.630 #&gt; 3 3 0 372 0.758 #&gt; 4 3 1 119 0.242 6.5.9 Effektstärken visualieren Zum Abschluss schauen wir uns die Stärke des Zusammenhangs noch einmal graphisch an. Wir berechnen dafür die relativen Häufigkeiten pro Gruppe (im Datensatz ohne 2. Klasse, der Einfachheit halber). c5 &lt;- dplyr::count(t2, Pclass, Survived) c5$prop &lt;- c5$n / 707 c5 #&gt; Source: local data frame [4 x 4] #&gt; Groups: Pclass [?] #&gt; #&gt; Pclass Survived n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 0 80 0.113 #&gt; 2 1 1 136 0.192 #&gt; 3 3 0 372 0.526 #&gt; 4 3 1 119 0.168 Genauer gesagt haben die Häufigkeiten pro Gruppe in Bezug auf die Gesamtzahl aller Passagiere berechnet; die vier Anteile addieren sich also zu 1 auf. Das visualisieren wir wieder qplot(x = factor(Pclass), y = prop, fill = factor(Survived), data = c5, geom = &quot;col&quot;) Das geom = &quot;col&quot; heißt, dass als “geometrisches Objekt” dieses Mal keine Punkte, sondern Säulen (columns) verwendet werden sollen. qplot(x = factor(Pclass), y = prop, fill = factor(Survived), data = c5, geom = &quot;col&quot;) Ganz nett, aber die Häufigkeitsunterscheide von Survived zwischen den beiden Werten von Pclass stechen noch nicht so ins Auge. Wir sollten es anders darstellen. Hier kommt der Punkt, wo wir von qplot auf seinen großen Bruder, ggplot wechseln sollten. qplot ist in Wirklichkeit nur eine vereinfachte Form von ggplot; die Einfachheit wird mit geringeren Möglichkeiten bezahlt. Satteln wir zum Schluss dieser Fallstudie also um: ggplot(data = c5) + aes(x = factor(Pclass), y = n, fill = factor(Survived)) + geom_col(position = &quot;fill&quot;) + labs(x = &quot;Passagierklasse&quot;, fill = &quot;Überlebt?&quot;, caption = &quot;Nur Passagiere, keine Besatzung&quot;) Jeden sehen wir die Häufigkeiten des Überlebens bedingt auf die Passagierklasse besser. Wir sehen auf den ersten Blick, dass sich die Überlebensraten deutlich unterscheiden: Im linken Balken überleben die meisten; im rechten Balken ertrinken die meisten. Diese letzte Analyse zeigt deutlich die Kraft von (Daten-)Visualisierungen auf. Der zu untersuchende Effekt tritt hier am stärken zu Tage; außerdem ist die Analyse relativ einfach. Eine alternative Darstellung ist diese: c5 %&gt;% ggplot + aes(x = factor(Pclass), y = factor(Survived), fill = n) + geom_tile() Hier werden die vier “Fliesen” gleich groß dargestellt; die Fallzahl wird durch die Füllfarbe besorgt. 6.5.10 Fazit In der Datenanalyse (mit R) kommt man mit wenigen Befehlen schon sehr weit; dplyr und ggplot2 zählen (zu Recht) zu den am häufigsten verwendeten Paketen. Beide sind flexibel, konsistent und spielen gerne miteinander. Die besten Einblicke haben wir aus deskriptiver bzw. explorativer Analyse (Diagramme) gewonnen. Signifikanztests oder komplizierte Modelle waren nicht zentral. In vielen Studien/Projekten der Datenanalyse gilt ähnliches: Daten umformen und verstehen bzw. “veranschaulichen” sind zentrale Punkte, die häufig viel Zeit und Wissen fordern. Bei der Analyse von nominalskalierten sind Häufigkeitsauswertungen ideal. References "],
["ungeleitetes-modellieren.html", "Kapitel 7 Ungeleitetes Modellieren 7.1 Clusteranalyse 7.2 Dimensionsreduktion 7.3 Hauptkomponentenanalyse (PCA) 7.4 Exploratorische Faktorenanalyse (EFA) 7.5 Assoziationsanalyse", " Kapitel 7 Ungeleitetes Modellieren 7.1 Clusteranalyse Wir werden einen simulierten Datensatz aus Chapman &amp; Feit (2015): R for Marketing Research and Analytics. Springer analysieren (http://r-marketing.r-forge.r-project.org). Näheres dazu siehe Kapitel 5 dort. Sie können ihn von hier als csv-Datei herunterladen: #download.file(&quot;https://goo.gl/eUm8PI&quot;, destfile = &quot;segment.csv&quot;) Das Einlesen erfolgt, sofern die Daten im Arbeitsverzeichnis liegen, wieder über: segment &lt;- read.csv2(&quot;https://goo.gl/eUm8PI&quot;) Ein Überblick über die Daten: str(segment) #&gt; &#39;data.frame&#39;: 300 obs. of 7 variables: #&gt; $ Alter : num 50.2 40.7 43 40.3 41.1 ... #&gt; $ Geschlecht : Factor w/ 2 levels &quot;Frau&quot;,&quot;Mann&quot;: 2 2 1 2 1 2 1 2 1 1 ... #&gt; $ Einkommen : num 51356 64411 71615 42728 71641 ... #&gt; $ Kinder : int 0 3 2 1 4 2 5 1 1 0 ... #&gt; $ Eigenheim : Factor w/ 2 levels &quot;Ja&quot;,&quot;Nein&quot;: 2 2 1 2 2 1 2 2 2 2 ... #&gt; $ Mitgliedschaft: Factor w/ 2 levels &quot;Ja&quot;,&quot;Nein&quot;: 2 2 2 2 2 2 1 1 2 2 ... #&gt; $ Segment : Factor w/ 4 levels &quot;Aufsteiger&quot;,&quot;Gemischte Vorstadt&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... head(segment) #&gt; Alter Geschlecht Einkommen Kinder Eigenheim Mitgliedschaft #&gt; 1 50.2 Mann 51356 0 Nein Nein #&gt; 2 40.7 Mann 64411 3 Nein Nein #&gt; 3 43.0 Frau 71615 2 Ja Nein #&gt; 4 40.3 Mann 42728 1 Nein Nein #&gt; 5 41.1 Frau 71641 4 Nein Nein #&gt; 6 40.2 Mann 60325 2 Ja Nein #&gt; Segment #&gt; 1 Gemischte Vorstadt #&gt; 2 Gemischte Vorstadt #&gt; 3 Gemischte Vorstadt #&gt; 4 Gemischte Vorstadt #&gt; 5 Gemischte Vorstadt #&gt; 6 Gemischte Vorstadt Zur Unterstützung der Analyse wird (wieder) mosaic und tidyverse verwendet: library(tidyverse) library(mosaic) Das Ziel einer Clusteranalyse ist es, Gruppen von Beobachtungen (d. h. Cluster) zu finden, die innerhalb der Cluster möglichst homogen, zwischen den Clustern möglichst heterogen sind. Um die Ähnlichkeit von Beobachtungen zu bestimmen, können verschiedene Distanzmaße herangezogen werden. Für metrische Merkmale wird z. B. häufig die euklidische Metrik verwendet, d. h., Ähnlichkeit und Distanz werden auf Basis des euklidischen Abstands bestimmt. Aber auch andere Abstände wie Manhatten oder Gower sind möglich. Letztere haben den Vorteil, dass sie nicht nur für metrische Daten sondern auch für gemischte Variablentypen verwendet werden können. Auf Basis der drei metrischen Merkmale (d. h. Alter, Einkommen und Kinder) ergeben sich für die ersten sechs Beobachtungen folgende Abstände: dist(head(segment)) #&gt; 1 2 3 4 5 #&gt; 2 19941.8 #&gt; 3 30946.1 11004.3 #&gt; 4 13179.5 33121.3 44125.6 #&gt; 5 30985.9 11044.0 39.9 44165.3 #&gt; 6 13700.4 6241.5 17245.8 26879.9 17285.5 Sie können erkennen, dass die Beobachtungen 5 und 3 den kleinsten Abstand haben, während 5 und 4 den größten haben. Allerdings zeigen die Rohdaten auch, dass die euklidischen Abstände von der Skalierung der Variablen abhängen (Einkommen streut stärker als Kinder). Daher kann es evt. sinnvoll sein, die Variablen vor der Analyse zu standardisieren (z. B. über scale()). Die Funktion daisy() aus dem Paket cluster bietet hier nützliche Möglichkeiten. library(cluster) daisy(head(segment)) #&gt; Dissimilarities : #&gt; 1 2 3 4 5 #&gt; 2 0.307 #&gt; 3 0.560 0.390 #&gt; 4 0.219 0.184 0.502 #&gt; 5 0.516 0.220 0.242 0.404 #&gt; 6 0.401 0.206 0.239 0.268 0.426 #&gt; #&gt; Metric : mixed ; Types = I, N, I, I, N, N, N #&gt; Number of objects : 6 7.1.1 Hierarchische Clusteranalyse Bei hierarchischen Clusterverfahren werden Beobachtungen sukzessiv zusammengefasst (agglomerativ). Zunächst ist jede Beobachtung ein eigener Cluster, die dann je nach Ähnlichkeitsmaß zusammengefasst werden. Fassen wir die Beobachtungen ohne die Segmentvariable Segment, Variable 7, zusammen: seg.dist &lt;- daisy(segment[,-7]) # Abstände seg.hc &lt;- hclust(seg.dist) # Hierarchische Clusterung Das Ergebnis lässt sich schön im Dendrogramm darstellen: plot(seg.hc) Je höher (Height) die Stelle ist, an der zwei Beobachtungen oder Cluster zusammengefasst werden, desto größer ist die Distanz. D. h., Beobachtungen bzw. Cluster, die unten zusammengefasst werden, sind sich ähnlich, die, die oben zusammengefasst werden unähnlich. Hier wurde übrigens die Standardeinstellung für die Berechnung des Abstands von Clustern verwendet: Complete Linkage bedeutet, dass die Distanz zwischen zwei Clustern auf Basis des maximalen Abstands der Beobachtungen innerhalb des Clusters gebildet wird. Es ist nicht immer einfach zu entscheiden, wie viele Cluster es gibt. In der Praxis und Literatur finden sich häufig Zahlen zwischen 3 und 10. Evt. gibt es im Dendrogramm eine Stelle, an der der Baum gut geteilt werden kann. In unserem Fall vielleicht bei einer Höhe von \\(0.6\\), da sich dann 3 Cluster ergeben: plot(seg.hc) rect.hclust(seg.hc, h=0.6, border=&quot;red&quot;) Das Ergebnis, d. h. die Clusterzuordnung, kann durch den Befehl cutree() den Beobachtungen zugeordnet werden. segment$hc.clust &lt;- cutree(seg.hc, k=3) Z. B. haben wir folgende Anzahlen für Beobachtungen je Cluster: mosaic::tally(~hc.clust, data=segment) #&gt; hc.clust #&gt; 1 2 3 #&gt; 140 122 38 Cluster 3 ist also mit Abstand der kleinste Cluster (mit 38 Beobachtungen). Für den Mittelwert des Alters je Cluster gilt: segment %&gt;% group_by(hc.clust) %&gt;% summarise(Alter_nach_Cluster = mean(Alter)) #&gt; # A tibble: 3 × 2 #&gt; hc.clust Alter_nach_Cluster #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 38.5 #&gt; 2 2 46.4 #&gt; 3 3 34.5 D. h. die Durchschnittsalter ist in Cluster der Cluster unterscheiden sich. Das spiegelt sich auch im Einkommen wieder: segment %&gt;% group_by(hc.clust) %&gt;% summarise(Einkommen_nach_Cluster = mean(Einkommen)) #&gt; # A tibble: 3 × 2 #&gt; hc.clust Einkommen_nach_Cluster #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 49452 #&gt; 2 2 54355 #&gt; 3 3 44113 Allerdings sind die Unterschiede in der Geschlechtsverteilung eher gering: mosaic::tally(Geschlecht~hc.clust, data=segment, format=&quot;proportion&quot;) #&gt; hc.clust #&gt; Geschlecht 1 2 3 #&gt; Frau 0.543 0.549 0.526 #&gt; Mann 0.457 0.451 0.474 7.1.2 k-Means Clusteranalyse Beim k-Means Clusterverfahren handelt es sich im Gegensatz zur hierarchischen Clusteranalyse um ein partitionierendes Verfahren. Die Daten werde in k Cluster aufgeteilt – dabei muss die Anzahl der Cluster im vorhinein feststehen. Ziel ist es, dass die Quadratsumme der Abweichungen der Beobachtungen im Cluster zum Clusterzentrum minimiert wird. Der Ablauf des Verfahrens ist wie folgt: Zufällige Beobachtungen als Clusterzentrum Zuordnung der Beobachtungen zum nächsten Clusterzentrum (Ähnlichkeit, z. B. über die euklidische Distanz) Neuberechnung der Clusterzentren als Mittelwert der dem Cluster zugeordneten Beobachtungen Dabei werden die Schritte 2. und 3. solange wiederholt, bis sich keine Änderung der Zuordnung mehr ergibt – oder eine maximale Anzahl an Iterationen erreicht wurde. Hinweis: Die (robuste) Funktion pam() aus dem Paket cluster kann auch mit allgemeinen Distanzen umgehen. Außerdem für gemischte Variablentypen gut geeignet: Das Paket clustMixType. Zur Vorbereitung überführen wir die nominalen Merkmale in logische, d. h. binäre Merkmale, und löschen die Segmente sowie das Ergebnis der hierarchischen Clusteranalyse: segment.num &lt;- segment %&gt;% mutate(Frau = Geschlecht==&quot;Frau&quot;) %&gt;% mutate(Eigenheim = Eigenheim==&quot;Ja&quot;) %&gt;% mutate(Mitgliedschaft = Mitgliedschaft==&quot;Ja&quot;) %&gt;% dplyr::select(-Geschlecht, -Segment, -hc.clust) Über die Funktion mutate() werden Variablen im Datensatz erzeugt oder verändert. Über select() werden einzene Variablen ausgewählt. Die “Pfeife” %&gt;% übergeben das Ergebnis der vorherigen Funktion an die folgende. Aufgrund von (1.) hängt das Ergebnis einer k-Means Clusteranalyse vom Zufall ab. Aus Gründen der Reproduzierbarkeit sollte daher der Zufallszahlengenerator gesetzt werden. Außerdem bietet es sich an verschiedene Startkonfigurationen zu versuchen. in der Funktion kmeans() erfolgt dies durch die Option nstart=. Hier mit k=4 Clustern: set.seed(1896) seg.k &lt;- kmeans(segment.num, centers = 4, nstart = 10) seg.k #&gt; K-means clustering with 4 clusters of sizes 111, 26, 58, 105 #&gt; #&gt; Cluster means: #&gt; Alter Einkommen Kinder Eigenheim Mitgliedschaft Frau #&gt; 1 42.9 46049 1.649 0.505 0.1081 0.568 #&gt; 2 56.4 85973 0.385 0.538 0.0385 0.538 #&gt; 3 27.0 22608 1.224 0.276 0.2069 0.414 #&gt; 4 43.6 62600 1.505 0.457 0.1238 0.590 #&gt; #&gt; Clustering vector: #&gt; [1] 1 4 4 1 4 4 4 1 2 4 1 1 4 4 1 1 1 1 1 4 4 4 1 4 1 1 1 1 4 1 4 4 1 1 2 #&gt; [36] 1 4 1 1 4 4 4 1 4 4 4 4 1 1 1 1 1 2 1 1 4 4 4 4 1 4 1 4 1 1 1 1 4 4 4 #&gt; [71] 4 1 1 4 1 1 4 4 4 4 1 4 1 3 1 4 1 1 1 1 4 4 4 1 1 4 1 4 4 4 3 3 3 3 3 #&gt; [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 #&gt; [141] 3 3 3 3 3 3 3 3 3 3 1 2 4 2 2 4 1 1 2 2 4 4 1 1 4 2 4 4 1 2 2 3 4 1 2 #&gt; [176] 2 4 2 3 4 4 4 1 1 1 1 1 1 4 3 1 4 4 4 4 1 1 1 2 4 4 1 2 4 4 1 4 2 1 2 #&gt; [211] 4 3 4 2 2 4 2 1 4 3 1 2 2 4 2 4 4 1 4 4 1 1 1 1 1 3 1 1 4 1 4 3 1 4 1 #&gt; [246] 4 1 4 1 4 4 4 4 1 1 1 4 4 1 1 1 1 1 1 4 1 1 1 1 1 2 4 4 1 4 1 1 1 1 2 #&gt; [281] 4 4 4 4 1 4 1 4 4 4 1 4 1 4 1 4 1 1 4 1 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 3.18e+09 2.22e+09 1.69e+09 2.81e+09 #&gt; (between_SS / total_SS = 90.6 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; #&gt; [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; #&gt; [9] &quot;ifault&quot; Neben der Anzahl Beobachtungen im Cluster (z. B. 26 in Cluster 2) werden auch die Clusterzentren ausgegeben. Diese können dann direkt verglichen werden. Sie sehen z. B., dass das Durchschnittsalter in Cluster 3 mit 27 am geringsten ist. Der Anteil der Eigenheimbesitzer ist mit 54 % in Cluster 2 am höchsten. Einen Plot der Scores auf den beiden ersten Hauptkomponenten können Sie über die Funktion clusplot() aus dem Paket cluster erhalten. clusplot(segment.num, seg.k$cluster, color = TRUE, shade = TRUE, labels = 4) Wie schon im deskriptiven Ergebnis: Die Cluster 1 und 4 unterscheiden sich (in den ersten beiden Hauptkomponenten) nicht wirklich. Vielleicht sollten dies noch zusammengefasst werden, d. h., mit centers=3 die Analyse wiederholt werden?37 7.1.3 Übung: B3 Datensatz Der B3 Datensatz Heilemann, U. and Münch, H.J. (1996): West German Business Cycles 1963-1994: A Multivariate Discriminant Analysis. CIRET–Conference in Singapore, CIRET–Studien 50. enthält Quartalsweise Konjunkturdaten aus (West-)Deutschland. Er kann von https://goo.gl/0YCEHf heruntergeladen werden. Wenn die Konjunkturphase PHASEN nicht berücksichtigt wird, wie viele Cluster könnte es geben? Ändert sich das Ergebnis, wenn die Variablen standardisiert werden? Führen Sie eine k-Means Clusteranalyse mit 4 Clustern durch. Worin unterscheiden sich die gefundenen Segmente? 7.1.4 Literatur Chris Chapman, Elea McDonnell Feit (2015): R for Marketing Research and Analytics, Kapitel 11.3 Reinhold Hatzinger, Kurt Hornik, Herbert Nagel (2011): R – Einführung durch angewandte Statistik. Kapitel 12 Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013): An Introduction to Statistical Learning – with Applications in R, http://www-bcf.usc.edu/~gareth/ISL/, Kapitel 10.3, 10.5 Diese Übung orientiert sich am Beispiel aus Kapitel 11.3 aus Chapman und Feit (2015) und steht unter der Lizenz Creative Commons Attribution-ShareAlike 3.0 Unported. Der Code steht unter der Apache Lizenz 2.0 7.2 Dimensionsreduktion Datensätze in den Sozialwissenschaften haben oft viele Variablen - oder auch Dimensionen Es ist vorteilhaft, diese auf eine kleinere Anzahl von Variablen (oder Dimensionen) zu reduzieren: Zusammenhänge zwischen Konstrukten können so klarer identifiziert werden. In diese Übung betrachten wir zwei gängige Methoden, um die Komplexität von multivarianten, metrischen Daten zu reduzieren, indem wir die Anzahl der Dimensionen in den Daten reduzieren. Die Hauptkomponentenanalyse (PCA) versucht, unkorrelierte Linearkombinationen zu finden, die die maximale Varianz in den Daten erfassen. Die Blickrichtung ist von den Daten zu den Komponenten. Die Exploratorische Faktorenanalyse (EFA) versucht die Varianz auf Basis einer kleinen Anzahl von Dimensionen zu modellieren, während sie gleichzeitig versucht, die Dimensionen in Bezug auf die ursprünglichen Variablen interpretierbar zu machen. Es wird davon ausgegangen, dass die Daten einem Faktoren Modell entsprechen. Die Blickrichtung ist von den Faktoren zu den Daten. 7.2.1 Gründe für die Notwendigkeit der Datenreduktion Im technischen Sinne der Dimensionsreduktion können wir statt Variablen-Sets die Faktorwerte verwenden. Wir können Unsicherheit verringern. Wenn wir glauben, dass ein Konstrukt nicht eindeutig messbar ist, dann kann mit einem Variablen-Set die unsicherheit reduziert werden. Wir könnten den Aufwand bei der Datenerfassung vereinfachen, indem wir uns auf Variablen konzentrieren, von denen bekannt ist, dass sie einen hohen Beitrag zum interressierenden Faktor leisten. Wenn wir feststellen, dass einige Variablen für einen Faktor nicht wichtig sind, können wir sie aus dem Datensatz eliminieren. 7.2.2 Benötigte Pakete Pakete, die für die Datenanalyse benötigt werden, müssen vorher einmalig in R installiert werden. # install.packages(&quot;corrplot&quot;) # install.packages(&quot;gplots&quot;) # install.packages(&quot;scatterplot3d&quot;) library(&quot;corrplot&quot;) library(&quot;gplots&quot;) library(&quot;scatterplot3d&quot;) library(&quot;tidyverse&quot;) 7.2.3 Daten einlesen Wir untersuchen die Dimensionalität mittels eines simulierten Datensatzes der typisch für die Wahrnehmung von Umfragen ist. Die Daten spiegeln Verbraucherbewertungen von Marken in Bezug auf Adjektive wieder, die in Umfragen in folgender Form abgefragt werden: Auf einer Skala von 1 bis 10 (wobei 1 am wenigsten und 10 am meisten zutrifft) wie…[ADJECTIV]… ist …[Marke A]…? Wir verwenden einen simulierten Datensatz aus Chapman &amp; Feit (2015): R for Marketing Research and Analytics. Springer (http://r-marketing.r-forge.r-project.org). Die Daten umfassen simulierte Bewertungen von 10 Marken (“a” bis “j”) mit 9 Adjektiven (“performance”, “leader”, “latest”, “fun” usw.) für n = 100 simulierte Befragte. Das Einlesen der Daten erfolgt direkt über das Internet. brand.ratings &lt;- read.csv(&quot;http://goo.gl/IQl8nc&quot;) Wir überprüfen zuerst die Struktur des Datensatzes, die ersten 6 Zeilen und die Zusammenfassung str(brand.ratings) #&gt; &#39;data.frame&#39;: 1000 obs. of 10 variables: #&gt; $ perform: int 2 1 2 1 1 2 1 2 2 3 ... #&gt; $ leader : int 4 1 3 6 1 8 1 1 1 1 ... #&gt; $ latest : int 8 4 5 10 5 9 5 7 8 9 ... #&gt; $ fun : int 8 7 9 8 8 5 7 5 10 8 ... #&gt; $ serious: int 2 1 2 3 1 3 1 2 1 1 ... #&gt; $ bargain: int 9 1 9 4 9 8 5 8 7 3 ... #&gt; $ value : int 7 1 5 5 9 7 1 7 7 3 ... #&gt; $ trendy : int 4 2 1 2 1 1 1 7 5 4 ... #&gt; $ rebuy : int 6 2 6 1 1 2 1 1 1 1 ... #&gt; $ brand : Factor w/ 10 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... head(brand.ratings) #&gt; perform leader latest fun serious bargain value trendy rebuy brand #&gt; 1 2 4 8 8 2 9 7 4 6 a #&gt; 2 1 1 4 7 1 1 1 2 2 a #&gt; 3 2 3 5 9 2 9 5 1 6 a #&gt; 4 1 6 10 8 3 4 5 2 1 a #&gt; 5 1 1 5 8 1 9 9 1 1 a #&gt; 6 2 8 9 5 3 8 7 1 2 a summary(brand.ratings) #&gt; perform leader latest fun #&gt; Min. : 1.00 Min. : 1.00 Min. : 1.0 Min. : 1.00 #&gt; 1st Qu.: 1.00 1st Qu.: 2.00 1st Qu.: 4.0 1st Qu.: 4.00 #&gt; Median : 4.00 Median : 4.00 Median : 7.0 Median : 6.00 #&gt; Mean : 4.49 Mean : 4.42 Mean : 6.2 Mean : 6.07 #&gt; 3rd Qu.: 7.00 3rd Qu.: 6.00 3rd Qu.: 9.0 3rd Qu.: 8.00 #&gt; Max. :10.00 Max. :10.00 Max. :10.0 Max. :10.00 #&gt; #&gt; serious bargain value trendy #&gt; Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 #&gt; 1st Qu.: 2.00 1st Qu.: 2.00 1st Qu.: 2.00 1st Qu.: 3.00 #&gt; Median : 4.00 Median : 4.00 Median : 4.00 Median : 5.00 #&gt; Mean : 4.32 Mean : 4.26 Mean : 4.34 Mean : 5.22 #&gt; 3rd Qu.: 6.00 3rd Qu.: 6.00 3rd Qu.: 6.00 3rd Qu.: 7.00 #&gt; Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 #&gt; #&gt; rebuy brand #&gt; Min. : 1.00 a :100 #&gt; 1st Qu.: 1.00 b :100 #&gt; Median : 3.00 c :100 #&gt; Mean : 3.73 d :100 #&gt; 3rd Qu.: 5.00 e :100 #&gt; Max. :10.00 f :100 #&gt; (Other):400 Jeder der 100 simulierten Befragten beurteilt 10 Marken, das ergibt insgesamt 1000 Beobachtungen (Zeilen) im Datensatz. Wir sehen in der summary (), dass die Bereiche der Bewertungen für jedes Adjektiv 1-10 sind. In str() sehen wir, dass die Bewertungen als numerisch einghelesen wurden, während die Markennamen als Faktoren eingelesen wurden. Die Daten sind somit richtig formatiert. 7.2.4 Neuskalierung der Daten In vielen Fällen ist es sinnvoll, Rohdaten neu zu skalieren. Dies wird üblicherweise als Standardisierung, Normierung, oder Z Scoring/ Transformation bezeichnet. Als Ergebnis ist der Mittelwert aller Variablen über alle Beobachtungen dann 0. Da wir hier gleiche Skalenstufen haben ist ein Skalieren nicht unbedingt notwendig, wir führen es aber trotzdem durch. Ein einfacher Weg, alle Variablen im Datensatz auf einmal zu skalieren ist der Befehl scale(). Da wir die Rohdaten nie ändern wollen, weisen wir die Rohwerte zuerst einem neuen Dataframe brand.sc zu und scalieren anschließend die Daten. Wir skalieren in unserem Datensatz nur die ersten 9 Variablen, weil die 10. Variable der Faktor für die Markenamen ist. brand.sc &lt;- brand.ratings brand.ratings %&gt;% mutate_each(funs(scale), -brand) -&gt; brand.sc summary(brand.sc) #&gt; perform.V1 leader.V1 latest.V1 fun.V1 #&gt; Min. :-1.089 Min. :-1.310 Min. :-1.688 Min. :-1.847 #&gt; 1st Qu.:-1.089 1st Qu.:-0.927 1st Qu.:-0.713 1st Qu.:-0.754 #&gt; Median :-0.152 Median :-0.160 Median : 0.262 Median :-0.025 #&gt; Mean : 0.000 Mean : 0.000 Mean : 0.000 Mean : 0.000 #&gt; 3rd Qu.: 0.784 3rd Qu.: 0.607 3rd Qu.: 0.911 3rd Qu.: 0.704 #&gt; Max. : 1.721 Max. : 2.140 Max. : 1.236 Max. : 1.433 #&gt; #&gt; serious.V1 bargain.V1 value.V1 trendy.V1 #&gt; Min. :-1.196 Min. :-1.222 Min. :-1.391 Min. :-1.539 #&gt; 1st Qu.:-0.836 1st Qu.:-0.847 1st Qu.:-0.974 1st Qu.:-0.810 #&gt; Median :-0.116 Median :-0.097 Median :-0.140 Median :-0.080 #&gt; Mean : 0.000 Mean : 0.000 Mean : 0.000 Mean : 0.000 #&gt; 3rd Qu.: 0.604 3rd Qu.: 0.653 3rd Qu.: 0.693 3rd Qu.: 0.649 #&gt; Max. : 2.043 Max. : 2.153 Max. : 2.361 Max. : 1.743 #&gt; #&gt; rebuy.V1 brand #&gt; Min. :-1.072 a :100 #&gt; 1st Qu.:-1.072 b :100 #&gt; Median :-0.286 c :100 #&gt; Mean : 0.000 d :100 #&gt; 3rd Qu.: 0.500 e :100 #&gt; Max. : 2.465 f :100 #&gt; (Other):400 Die Daten wurden offenbar richtig skaliert, da Mittelwert aller Variablen über alle Beobachtungen 0 ist. 7.2.5 Zusammenhänge in den Daten Wir verwenden den Befehl corrplot() für die Erstinspektion von bivariaten Beziehungen zwischen den Variablen. Das Argument order = &quot;hclust&quot; ordnet die Zeilen und Spalten entsprechend der Ähnlichkeit der Variablen in einer hierarchischen Cluster-Lösung der Variablen (mehr dazu im Teil Clusteranalyse) neu an. brand.sc %&gt;% dplyr::select(-brand) %&gt;% cor() %&gt;% corrplot() Die Visualisierung der Korelation der Adjektive scheint drei allgemeine Cluster zu zeigen: fun/latest/trendy rebuy/bargain/value perform/leader/serious 7.2.6 Aggregation der durchschnittlichen Bewertungen nach Marke Um die Frage “Was ist die durchschnittliche (mittlere) Bewertung der Marke auf jedem Adjektiv?” zu benatworten, können wir den Befel aggregate() verwenden. Dieser berechnet den Mittelwert jeder Variable nach Marke. brand.mean &lt;- aggregate(.~ brand, data=brand.sc, mean) # brand.mean &lt;- # brand.sc %&gt;% # group_by(brand) %&gt;% # summarise_all(funs(mean)) brand.mean #&gt; brand V1 V2 V3 V4 V5 V6 V7 V8 #&gt; 1 a -0.8859 -0.528 0.411 0.657 -0.9189 0.2141 0.1847 -0.5251 #&gt; 2 b 0.9309 1.071 0.726 -0.972 1.1831 0.0416 0.1513 0.7403 #&gt; 3 c 0.6499 1.163 -0.102 -0.845 1.2227 -0.6070 -0.4407 0.0255 #&gt; 4 d -0.6799 -0.593 0.352 0.187 -0.6922 -0.8808 -0.9326 0.7367 #&gt; 5 e -0.5644 0.193 0.456 0.296 0.0421 0.5516 0.4182 0.1386 #&gt; 6 f -0.0587 0.270 -1.262 -0.218 0.5892 0.8740 1.0227 -0.8132 #&gt; 7 g 0.9184 -0.168 -1.285 -0.517 -0.5338 0.8965 1.2562 -1.2764 #&gt; 8 h -0.0150 -0.298 0.502 0.715 -0.1415 -0.7383 -0.7825 0.8643 #&gt; 9 i 0.3346 -0.321 0.356 0.412 -0.1487 -0.2546 -0.8034 0.5908 #&gt; 10 j -0.6299 -0.789 -0.154 0.285 -0.6022 -0.0971 -0.0738 -0.4814 #&gt; V9 #&gt; 1 -0.5962 #&gt; 2 0.2370 #&gt; 3 -0.1324 #&gt; 4 -0.4940 #&gt; 5 0.0365 #&gt; 6 1.3570 #&gt; 7 1.3609 #&gt; 8 -0.6040 #&gt; 9 -0.2032 #&gt; 10 -0.9616 rownames(brand.mean) &lt;- brand.mean[, 1] # Markenname als Fallbezeichnung setzen brand.mean &lt;- brand.mean[, -1] # Variablenname brand entfernen brand.mean #&gt; V1 V2 V3 V4 V5 V6 V7 V8 V9 #&gt; a -0.8859 -0.528 0.411 0.657 -0.9189 0.2141 0.1847 -0.5251 -0.5962 #&gt; b 0.9309 1.071 0.726 -0.972 1.1831 0.0416 0.1513 0.7403 0.2370 #&gt; c 0.6499 1.163 -0.102 -0.845 1.2227 -0.6070 -0.4407 0.0255 -0.1324 #&gt; d -0.6799 -0.593 0.352 0.187 -0.6922 -0.8808 -0.9326 0.7367 -0.4940 #&gt; e -0.5644 0.193 0.456 0.296 0.0421 0.5516 0.4182 0.1386 0.0365 #&gt; f -0.0587 0.270 -1.262 -0.218 0.5892 0.8740 1.0227 -0.8132 1.3570 #&gt; g 0.9184 -0.168 -1.285 -0.517 -0.5338 0.8965 1.2562 -1.2764 1.3609 #&gt; h -0.0150 -0.298 0.502 0.715 -0.1415 -0.7383 -0.7825 0.8643 -0.6040 #&gt; i 0.3346 -0.321 0.356 0.412 -0.1487 -0.2546 -0.8034 0.5908 -0.2032 #&gt; j -0.6299 -0.789 -0.154 0.285 -0.6022 -0.0971 -0.0738 -0.4814 -0.9616 7.2.7 Visualisierung der aggregierten Markenbewertungen Eine Heatmap ist eine nützliche Darstellungsmöglichkeit, um solche Ergebnisse zu visualisierung und zu analysieren, da sie Datenpunkte durch die Intensitäten ihrer Werte färbt. Hierzu laden wir das Paket gplot. library(gplots) heatmap.2(as.matrix(brand.mean)) heatmap.2() sortiert die Spalten und Zeilen, um Ähnlichkeiten und Muster in den Daten hervorzuheben. Eine zusätzliche Analysehilfe ist das Spalten- und Zeilendendrogramm. Hier werden Beobachtungen die nahe beineinanderliegen in einem Baum abgebildet. (Näheres hierzu bei der Clusteranalyse.) Auch hier sehen wir wieder die gleiche Zuordnung der Adjektive nach fun/latest/trendy rebuy/bargain/value perform/leader/serious Zusätzlich können die Marken nach Ähnlichkeit bezüglich bestimmer Adjektive zugeordnet werden: f und g b und c i, h und d a und j 7.3 Hauptkomponentenanalyse (PCA) Die PCA berechnet ein Variablenset (Komponenten) in Form von linearen Gleichungen, die die die lineare Beziehungen in den Daten erfassen. Die erste Komponente erfasst so viel Streuung (Varianz) wie möglich von allen Variablen als eine einzige lineare Funktion. Die zweite Komponente erfasst unkorreliert zur ersten Komponente so viel Streuung wie möglich, die nach der ersten Komponente verbleibt. Das geht so lange weiter, bis es so viele Komponenten gibt wie Variablen. 7.3.1 Bestimmung der Anzahl der Hauptkomponenten Betrachten wir in einem ersten Schritt die wichtigsten Komponenten für die Brand-Rating-Daten. Wir finden die Komponenten mit prcomp(), wobei wir wieder nur die Bewertungsspalten 1-9 auswählen: brand.pc &lt;- prcomp(brand.sc[, 1:9]) summary(brand.pc) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 PC6 PC7 #&gt; Standard deviation 1.726 1.448 1.039 0.8528 0.7985 0.7313 0.6246 #&gt; Proportion of Variance 0.331 0.233 0.120 0.0808 0.0708 0.0594 0.0433 #&gt; Cumulative Proportion 0.331 0.564 0.684 0.7647 0.8355 0.8950 0.9383 #&gt; PC8 PC9 #&gt; Standard deviation 0.5586 0.493 #&gt; Proportion of Variance 0.0347 0.027 #&gt; Cumulative Proportion 0.9730 1.000 # Berchnung der Gesamtvarianz Gesamtvarianz&lt;-1.726^2+1.4479^2+ 1.0389^2+ 0.8528^2+ 0.79846^2+ 0.73133^2+ 0.62458^2 +0.55861^2 +0.49310^2 # Varianzanteil der ersten Hauptkomponente 1.726^2/Gesamtvarianz #&gt; [1] 0.331 7.3.2 Scree-Plot Der Standard-Plot plot() für die PCA ist ein Scree-Plot, Dieser zeigt uns die Varianzen der Hauptkomponenten und die aufeinanderfolgende Varianzen, die von jeder Komponente berücksichtigt wird. Wir plotten ein Liniendiagramm mit dem Argument typ = &quot;l&quot; (l für Linie): plot(brand.pc, type=&quot;l&quot;) Wir sehen anahnd des Scree-Plots, dass bei den Brand-Rating-Daten der Anteil der Streuung nach der dritten Komponente nicht mehr wesentlich abnimmt. 7.3.3 Elbow-Kriterium Nach diesem Kriterium werden alle Hauptkomponenten berücksichtigt, die links von der Knickstelle im Scree-Plot liegen. Gibt es mehrere Knicks, dann wählt man jene Hauptkomponenten, die links vom rechtesten Knick liegen. Gibt es keinen Knick, dann hilft der Scree-Plot nicht weiter. Bei den Brand-Rating-Daten tritt der Ellbogen, je nach Interpretation, entweder bei drei oder vier Komponenten auf. Dies deutet darauf hin, dass die ersten zwei oder drei Komponenten die meiste Streuung in den Markendaten erklären. 7.3.4 Biplot Eine gute Möglichkeit die Ergebnisse der PCA zu analysieren besteht darin, die ersten Komponenten zuzuordnen, die es uns ermöglichen, die Daten in einem niedrigdimensionalen Raum zu visualisieren. Eine gemeinsame Visualisierung ist ein Biplot. Dies ist ein zweidimensionales Diagramm von Datenpunkten in Bezug auf die ersten beiden PCA-Komponenten, die mit einer Projektion der Variablen auf die Komponenten überlagert sind. Dazu verwenden wir biplot(): biplot(brand.pc) Die Adjektiv-Gruppierungen auf den Variablen sind als rote Ladungspfeile sichbar. ZUsätzlich erhalten wir einen Einblick in die Bewertungscluster (als dichte Bereiche von Beobachtungspunkten). Der Biplot ist durch die große Anzahl an Beobachtung recht unübersichtlich. Deshalb führen wir die PCA mit den aggregierten Daten durch: brand.mean #&gt; V1 V2 V3 V4 V5 V6 V7 V8 V9 #&gt; a -0.8859 -0.528 0.411 0.657 -0.9189 0.2141 0.1847 -0.5251 -0.5962 #&gt; b 0.9309 1.071 0.726 -0.972 1.1831 0.0416 0.1513 0.7403 0.2370 #&gt; c 0.6499 1.163 -0.102 -0.845 1.2227 -0.6070 -0.4407 0.0255 -0.1324 #&gt; d -0.6799 -0.593 0.352 0.187 -0.6922 -0.8808 -0.9326 0.7367 -0.4940 #&gt; e -0.5644 0.193 0.456 0.296 0.0421 0.5516 0.4182 0.1386 0.0365 #&gt; f -0.0587 0.270 -1.262 -0.218 0.5892 0.8740 1.0227 -0.8132 1.3570 #&gt; g 0.9184 -0.168 -1.285 -0.517 -0.5338 0.8965 1.2562 -1.2764 1.3609 #&gt; h -0.0150 -0.298 0.502 0.715 -0.1415 -0.7383 -0.7825 0.8643 -0.6040 #&gt; i 0.3346 -0.321 0.356 0.412 -0.1487 -0.2546 -0.8034 0.5908 -0.2032 #&gt; j -0.6299 -0.789 -0.154 0.285 -0.6022 -0.0971 -0.0738 -0.4814 -0.9616 brand.mu.pc&lt;- prcomp(brand.mean, scale=TRUE) summary(brand.mu.pc) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 PC6 PC7 #&gt; Standard deviation 2.135 1.735 0.7690 0.615 0.5098 0.3666 0.21506 #&gt; Proportion of Variance 0.506 0.334 0.0657 0.042 0.0289 0.0149 0.00514 #&gt; Cumulative Proportion 0.506 0.841 0.9064 0.948 0.9773 0.9922 0.99737 #&gt; PC8 PC9 #&gt; Standard deviation 0.14588 0.04867 #&gt; Proportion of Variance 0.00236 0.00026 #&gt; Cumulative Proportion 0.99974 1.00000 Dem Befehl prcomp() wurde Skalierung = TRUE hinzugefügt, um die Daten neu zu skalieren. Obwohl die Rohdaten bereits skaliert waren, haben die aggregierten Daten eine etwas andere Skala als die standardisierten Rohdaten. Die Ergebnisse zeigen, dass die ersten beiden Komponenten für 84% der erklärbaren Streuung bei den aggregierten Daten verantwortlich sind. 7.3.5 Wahrnehmungsraum Wenn ein Biplot Marken in einem Zweidimensionalen Raum abbildet, dann nennt man diesen Raum zweidimensionaler Wahrnehmungsraum. biplot(brand.mu.pc) Der Biplot der PCA-Lösung für die Mittelwerte gibt einen interpretierbaren Wahrnehmungsraum, der zeigt, wo die Marken in Bezug auf die ersten beiden Hauptkomponenten liegen. Die variablen auf den beiden Komponenten sind mit der PCA auf den gesamten Datensatz konsistent. Wir sehen zunächst vier Bereiche (Positionen) mit gut differenzierten Adjektiven und Marken. 7.4 Exploratorische Faktorenanalyse (EFA) EFA ist eine Methode, um die Beziehung von Konstrukten (Konzepten), d. h. Faktoren, zu Variablen zu beurteilen. Dabei werden die Faktoren als latente Variablen betrachtet, die nicht direkt beobachtet werden können. Stattdessen werden sie empirisch durch mehrere Variablen beobachtet, von denen jede ein Indikator der zugrundeliegenden Faktoren ist. Diese beobachteten Werte werden als manifeste Variablen bezeichnet und umfassen Indikatoren. Die EFA versucht den Grad zu bestimmen, in dem Faktoren die beobachtete Streuung der manifesten Variablen berücksichtigen. Das Ergebnis der EFA ist ähnlich zur PCA: eine Matrix von Faktoren (ähnlich zu den PCA-Komponenten) und ihre Beziehung zu den ursprünglichen Variablen (Ladung der Faktoren auf die Variablen). Im Gegensatz zur PCA versucht die EFA, Lösungen zu finden, die in den manifesten Variablen maximal interpretierbar sind. Im allgemeinen versucht sie, Lösungen zu finden, bei denen eine kleine Anzahl von Ladungen für jeden Faktor sehr hoch ist, während andere Ladungen für diesen Faktor gering sind. Wenn dies möglich ist, kann dieser Faktor mit diesem Variablen-Set interpretiert werden. Innerhalb einer PCA kann die Interpretierbarkeit über eine Rotation (z. B. varimax()) erhöht werden. 7.4.1 Finden einer EFA Lösung Als erstes muss die Anzahl der zu schätzenden Faktoren bestimmt werden. Hierzu verwenden wir zwei gebräuchliche Methoden: 1. Das Elbow-Kriterium Den Skreeplot haben wir bereits bei der PCA durchgeführt. Ein Knick konnten wir bei der dritte oder vierten Hauptkomponente feststellen. Somit zeigt der Skreeplot eine 2 oder 3 Faktorenlösung an. Durch das Paket nFactors bekommen wir eine formalisierte Berechnung der Scree-Plot Lösung mit dem Befehl nScree() library(nFactors) nScree(brand.sc[, 1:9]) #&gt; noc naf nparallel nkaiser #&gt; 1 3 2 3 3 nScree gibt vier methodische Schätzungen für die Anzahl an Faktoren durch den Scree-Plot aus. Wir sehen, dass drei von vier Methoden drei Faktoren vorschlagen. 2. Das Eigenwert-Kriterium Der Eigenwert ist eine Metrik für den Anteil der erklärten Varianz. Die Anzahl Eigenwerte können wir über den Befehl eigen() ausgeben. eigen(cor(brand.sc[, 1:9])) #&gt; $values #&gt; [1] 2.979 2.097 1.079 0.727 0.638 0.535 0.390 0.312 0.243 #&gt; #&gt; $vectors #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] -0.237 -0.4199 0.0385 0.5263 0.4679 0.337 0.36418 -0.1444 #&gt; [2,] -0.206 -0.5238 -0.0951 0.0892 -0.2945 0.297 -0.61367 0.2877 #&gt; [3,] 0.370 -0.2015 -0.5327 -0.2141 0.1059 0.174 -0.18548 -0.6429 #&gt; [4,] 0.251 0.2504 -0.4178 0.7506 -0.3315 -0.141 -0.00711 0.0746 #&gt; [5,] -0.160 -0.5105 -0.0407 -0.0989 -0.5552 -0.392 0.44530 -0.1835 #&gt; [6,] -0.399 0.2185 -0.4899 -0.1673 -0.0126 0.139 0.28826 0.0579 #&gt; [7,] -0.447 0.1898 -0.3692 -0.1512 -0.0633 0.220 0.01716 0.1483 #&gt; [8,] 0.351 -0.3185 -0.3709 -0.1676 0.3665 -0.266 0.15357 0.6145 #&gt; [9,] -0.439 -0.0151 -0.1246 0.1303 0.3557 -0.675 -0.38866 -0.2021 #&gt; [,9] #&gt; [1,] -0.0522 #&gt; [2,] 0.1789 #&gt; [3,] -0.0575 #&gt; [4,] -0.0315 #&gt; [5,] -0.0907 #&gt; [6,] 0.6472 #&gt; [7,] -0.7281 #&gt; [8,] -0.0591 #&gt; [9,] 0.0172 Der Eigenwert eines Faktors sagt aus, wie viel Varianz dieser Faktor an der Gesamtvarianz aufklärt. Lauf dem Eigenwert-Kriterium sollen nur Faktoren mit einem Eigenwert größer 1 extrahiert werden. Dies sind bei den Brand-Rating Daten drei Faktoren, da drei Eigenwerte größer 1 sind. Dies kann auch grafisch mit dem VSS.Scree geplotet werden. VSS.scree(brand.sc[, 1:9]) Schätzung der EFA Eine EFA wir geschätzt mit dem Befehl factanal(x,factors=k), wobei k die Anzahl Faktoren angibt. brand.fa&lt;-factanal(brand.sc[, 1:9], factors=3) brand.fa #&gt; #&gt; Call: #&gt; factanal(x = brand.sc[, 1:9], factors = 3) #&gt; #&gt; Uniquenesses: #&gt; perform leader latest fun serious bargain value trendy rebuy #&gt; 0.624 0.327 0.005 0.794 0.530 0.302 0.202 0.524 0.575 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 Factor3 #&gt; perform 0.607 #&gt; leader 0.810 0.106 #&gt; latest -0.163 0.981 #&gt; fun -0.398 0.205 #&gt; serious 0.682 #&gt; bargain 0.826 -0.122 #&gt; value 0.867 -0.198 #&gt; trendy -0.356 0.586 #&gt; rebuy 0.499 0.296 -0.298 #&gt; #&gt; Factor1 Factor2 Factor3 #&gt; SS loadings 1.853 1.752 1.510 #&gt; Proportion Var 0.206 0.195 0.168 #&gt; Cumulative Var 0.206 0.401 0.568 #&gt; #&gt; Test of the hypothesis that 3 factors are sufficient. #&gt; The chi square statistic is 64.6 on 12 degrees of freedom. #&gt; The p-value is 3.28e-09 Eine Übersichtlichere Ausgabe bekommen wir mit dem printBefehl, in dem wir zusätzlich noch die Dezimalstellen kürzen mit digits=2, alle Ladungen kleiner als 0,5 ausblenden mit cutoff=.5 und die Ladungen so sortieren, dass die Ladungen die auf einen Faktor laden untereinander stehen mit sort=TRUE. print(brand.fa, digits=2, cutoff=.5, sort=TRUE) #&gt; #&gt; Call: #&gt; factanal(x = brand.sc[, 1:9], factors = 3) #&gt; #&gt; Uniquenesses: #&gt; perform leader latest fun serious bargain value trendy rebuy #&gt; 0.62 0.33 0.00 0.79 0.53 0.30 0.20 0.52 0.58 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 Factor3 #&gt; bargain 0.83 #&gt; value 0.87 #&gt; perform 0.61 #&gt; leader 0.81 #&gt; serious 0.68 #&gt; latest 0.98 #&gt; trendy 0.59 #&gt; fun #&gt; rebuy #&gt; #&gt; Factor1 Factor2 Factor3 #&gt; SS loadings 1.85 1.75 1.51 #&gt; Proportion Var 0.21 0.19 0.17 #&gt; Cumulative Var 0.21 0.40 0.57 #&gt; #&gt; Test of the hypothesis that 3 factors are sufficient. #&gt; The chi square statistic is 64.6 on 12 degrees of freedom. #&gt; The p-value is 3.28e-09 Standardmäßig wird bei der factanal eine Varimax-Rotation durchgeführt. Diese verwendet, dass es keine Korrelationen zwischen den Faktoren gibt. Sollen Korrelationen zwischen den Faktoren zugelassen werden, empfielt sich die Oblimin-Rotation mit dem Argument rotation=&quot;oblimin&quot; aus dem Paket GPArotation. 7.4.2 Heatmap mit Ladungen In der obigen Ausgabe werden die Item-to-Faktor-Ladungen angezeigt. Im zurückgegebenen Objekt brand.fa sind diese als $loadings vorhanden. Wir können die Item-Faktor-Beziehungen mit einer Heatmap von $loadings visualisieren: heatmap.2(brand.fa$loadings) Das Ergebnis aus der Heatmap zeigt eine deutliche Trennung der Items in 3 Faktoren, die grob interpretierbar sind als value, leader und latest. 7.4.3 Berechnung der Faktor-Scores Zusätzlich zur Schätzung der Faktorstruktur kann die EFA auch die latenten Faktorwerte für jede Beobachtung schätzen. Die gängige Extraktionsmethodi ist Bartlett-Methode. brand.fa.ob &lt;- factanal(brand.sc[, 1:9], factors=3, scores=&quot;Bartlett&quot;) brand.scores &lt;- data.frame(brand.fa.ob$scores) head(brand.scores) #&gt; Factor1 Factor2 Factor3 #&gt; 1 1.910 -0.867 0.845 #&gt; 2 -1.579 -1.507 -1.119 #&gt; 3 1.172 -1.130 -0.296 #&gt; 4 0.496 -0.430 1.302 #&gt; 5 2.114 -2.047 -0.210 #&gt; 6 1.691 0.155 1.219 Wir können dann die Faktor-Scores verwenden, um die Positionen der Marken auf den Faktoren zu bestimmen. brand.scores$brand &lt;- brand.sc$brand # Zuweisung der Markennamen zur Scores-Matrix brand.fa.mean &lt;- aggregate(. ~ brand, data=brand.scores, mean) # Aggregation Marken rownames(brand.fa.mean) &lt;- brand.fa.mean[, 1] # Fallbezeichnung mit Markennamen setzen brand.fa.mean &lt;- brand.fa.mean[, -1] # Erste Spalte löschen names(brand.fa.mean) &lt;- c(&quot;Leader&quot;, &quot;Value&quot;, &quot;Latest&quot;) # Spaltennamen neu zuweisen brand.fa.mean #&gt; Leader Value Latest #&gt; a 0.378 -1.1206 0.3888 #&gt; b 0.202 1.4631 0.8938 #&gt; c -0.706 1.5096 -0.0995 #&gt; d -1.000 -0.7239 0.1346 #&gt; e 0.656 -0.1291 0.5657 #&gt; f 0.929 0.4516 -1.0946 #&gt; g 1.169 0.0245 -1.1166 #&gt; h -0.849 -0.2763 0.3500 #&gt; i -0.624 -0.1788 0.2450 #&gt; j -0.155 -1.0202 -0.2671 Mittels Heatmap kann dann sehr schnell analysiert werden, welche Marke auf welcher Dimension gute oder schlechte Ausprägungen hat. heatmap.2(as.matrix(brand.fa.mean)) Drei Dimensionen lassen sich in einem dreidimensionalen Raum darstellen: library(scatterplot3d) attach(brand.fa.mean) # Datensatz zum Suchpfad hinzufügen scatterplot3d(Leader~Value+Latest, pch=row.names(brand.fa.mean)) detach(brand.fa.mean) # Datensatz vom Suchpfad entfernen 7.4.4 Interne Konsistenz der Skalen Das einfachste Maß für die interne Konsistenz ist die Split-Half-Reliabilität. Die Items werden in zwei Hälften unterteilt, und die resultierenden Scores sollten in ihren Kenngrößen ähnlich sein. Hohe Korrelationen zwischen den Hälften deuten auf eine hohe interne Konsistenz hin. Das Problem ist, dass die Ergebnisse davon abhängen, wie die Items aufgeteilt werden. Ein üblicher Ansatz zur Lösung dieses Problems besteht darin, den Koeffizienten Alpha (Cronbachs Alpha) zu verwenden. Der Koeffizient Alpha ist der Mittelwert aller möglichen Split-Half-Koeffizienten, die sich aus verschiedenen Arten der Aufteilung der Items ergeben. Dieser Koeffizient variiert von 0 bis 1. Formal ist es ein korrigierter durschnittlicher Korrelationskoeffizient. Faustreglen für die Bewertung von Cronbachs Alpha: Alpha Bedeutung größer 0,9 excellent größer 0,8 gut größer 0,7 akzeptabel größer 0,6 fragwürdig größer 0,5 schlecht Wir Bewertungen nun die interne Konsistent der Itmes für die Konstrukte Leader, Value und Latest. alpha(brand.sc[, c(&quot;leader&quot;,&quot;serious&quot;,&quot;perform&quot;)], check.keys=TRUE) #&gt; #&gt; Reliability analysis #&gt; Call: alpha(x = brand.sc[, c(&quot;leader&quot;, &quot;serious&quot;, &quot;perform&quot;)], check.keys = TRUE) #&gt; #&gt; raw_alpha std.alpha G6(smc) average_r S/N ase mean sd #&gt; 0.73 0.73 0.66 0.48 2.7 0.015 -7.5e-17 0.81 #&gt; #&gt; lower alpha upper 95% confidence boundaries #&gt; 0.7 0.73 0.76 #&gt; #&gt; Reliability if an item is dropped: #&gt; raw_alpha std.alpha G6(smc) average_r S/N alpha se #&gt; leader 0.53 0.53 0.36 0.36 1.1 0.030 #&gt; serious 0.67 0.67 0.50 0.50 2.0 0.021 #&gt; perform 0.73 0.73 0.57 0.57 2.7 0.017 #&gt; #&gt; Item statistics #&gt; n raw.r std.r r.cor r.drop mean sd #&gt; leader 1000 0.86 0.86 0.76 0.65 7.0e-17 1 #&gt; serious 1000 0.80 0.80 0.64 0.54 -1.5e-16 1 #&gt; perform 1000 0.77 0.77 0.57 0.48 -1.6e-16 1 alpha(brand.sc[, c(&quot;value&quot;, &quot;bargain&quot;, &quot;rebuy&quot;)], check.keys=TRUE) #&gt; #&gt; Reliability analysis #&gt; Call: alpha(x = brand.sc[, c(&quot;value&quot;, &quot;bargain&quot;, &quot;rebuy&quot;)], check.keys = TRUE) #&gt; #&gt; raw_alpha std.alpha G6(smc) average_r S/N ase mean sd #&gt; 0.8 0.8 0.75 0.57 4 0.011 9.7e-18 0.84 #&gt; #&gt; lower alpha upper 95% confidence boundaries #&gt; 0.78 0.8 0.82 #&gt; #&gt; Reliability if an item is dropped: #&gt; raw_alpha std.alpha G6(smc) average_r S/N alpha se #&gt; value 0.64 0.64 0.47 0.47 1.8 0.0230 #&gt; bargain 0.67 0.67 0.51 0.51 2.0 0.0207 #&gt; rebuy 0.85 0.85 0.74 0.74 5.7 0.0095 #&gt; #&gt; Item statistics #&gt; n raw.r std.r r.cor r.drop mean sd #&gt; value 1000 0.89 0.89 0.83 0.73 1.2e-16 1 #&gt; bargain 1000 0.87 0.87 0.80 0.70 -1.2e-16 1 #&gt; rebuy 1000 0.78 0.78 0.57 0.52 5.2e-17 1 alpha(brand.sc[, c(&quot;latest&quot;,&quot;trendy&quot;,&quot;fun&quot;)], check.keys=TRUE) #&gt; #&gt; Reliability analysis #&gt; Call: alpha(x = brand.sc[, c(&quot;latest&quot;, &quot;trendy&quot;, &quot;fun&quot;)], check.keys = TRUE) #&gt; #&gt; raw_alpha std.alpha G6(smc) average_r S/N ase mean sd #&gt; 0.6 0.6 0.58 0.33 1.5 0.022 4.4e-17 0.75 #&gt; #&gt; lower alpha upper 95% confidence boundaries #&gt; 0.56 0.6 0.64 #&gt; #&gt; Reliability if an item is dropped: #&gt; raw_alpha std.alpha G6(smc) average_r S/N alpha se #&gt; latest 0.23 0.23 0.13 0.13 0.29 0.049 #&gt; trendy 0.39 0.39 0.25 0.25 0.65 0.038 #&gt; fun 0.77 0.77 0.63 0.63 3.37 0.014 #&gt; #&gt; Item statistics #&gt; n raw.r std.r r.cor r.drop mean sd #&gt; latest 1000 0.84 0.84 0.76 0.58 -9.7e-17 1 #&gt; trendy 1000 0.79 0.79 0.68 0.48 8.8e-17 1 #&gt; fun 1000 0.61 0.61 0.26 0.21 1.5e-16 1 Bis auf Latest sind alle Konstrukte bezüglich ihrer internen Konsistenz akzeptabel. Bei dem Konstrukt Latest können wir durch Elimination von fun das Cronbachs Alpha von einem fragwürdigen Wert auf einen akteptablen Wert von 0,77 erhöhen. Das Argument check.keys=TRUE gibt uns eine Warung aus, sollte die Ladung eines oder mehrerer Items negativ sein. Dies ist hier nicht der Fall, somit müssen auch keine Items recodiert werden. 7.4.5 Übung Führen Sie eine Dimensionsreduktion mit den nichtskalierten original Daten durch. Berechenn Sie zur Interpretaion keine Faktor-Scores, sondern berechnen Sie stattdessen den Mittelwert der Variablen, die hoch (mindestens 0,5) auf einen Faktor laden. Für die Berechnung verwenden Sie Datensatz$Neue_Variable &lt;- apply(Datensatz[,c(“Variable1”,“Variable2”, “etc..”)],1,mean,na.rm=TRUE) 7.4.6 Literatur Chris Chapman, Elea McDonnell Feit (2015): R for Marketing Research and Analytics, Kapitel 8.1-8.3 Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013): An Introduction to Statistical Learning – with Applications in R, http://www-bcf.usc.edu/~gareth/ISL/, Kapitel 10.2, 10.4 Reinhold Hatzinger, Kurt Hornik, Herbert Nagel (2011): R – Einführung durch angewandte Statistik. Kapitel 11 Maike Luhmann (2015): R für Einsteiger, Kapitel 19 Diese Übung orientiert sich am Beispiel aus Kapitel 8 aus Chapman und Feit (2015) und steht unter der Lizenz Creative Commons Attribution-ShareAlike 3.0 Unported. Der Code steht unter der Apache Lizenz 2.0 7.5 Assoziationsanalyse 7.5.1 Einführung Im Rahmen einer Assoziationsanalyse werden Zusammenhänge (Assoziationen) zwischen Ereignissen innerhalb von Datensätzen untersucht, z. B. im Rahmen einer Warenkorbanalyse. Dabei wird eine Menge (Set) von Items (z. B. gekauften Produkten, \\(I=\\{i_1, i_2, \\ldots i_m \\}\\)) innerhalb der Transaktionen (\\(D=\\{t_1, t_2 ,\\ldots ,t_n\\}\\)) betrachtet. Eine Regel (\\(A \\rightarrow B\\)) ist dann ein Zusammenhang zwischen den Item Sets. \\(A\\) und \\(B\\) sind dabei disjunkte, ggfs. leere oder einelementige Teilmengen von Items (\\(A, B \\subseteq I; A\\cap B=\\emptyset\\)). Achtung: Eine Regel impliziert keinen Kausalzusammenhang! Die computationale Herausforderung einer Assoziationsanalyse besteht darin, bei vielen Items (und damit sehr, sehr vielen Kombinationsmöglichkeiten) innerhalb von vielen Transaktionen diejenigen Sets zu finden, die häufig vorkommen. Eine wichtige Anwendung im (e)CRM ist z. B. die Analyse des Cross Buying Verhaltens. Assoziationsregeln sind eine explorative Analysemethode, die keine (statistische) Inferenz oder Kausalität aussagt. D. h. evt. Schlüsse aus der Analysen sollten vorsichtig schrittweise getestet und verglichen werden. 7.5.2 Kennzahlen einer Assoziationsanalyse Support(\\(A\\))\\(=\\frac{|\\{t\\in D; A \\subseteq t\\}|}{|D|}\\): Relative Häufigkeit, d. h. der Anteil der Transaktionen, in denen eine Menge von Items (\\(A\\)) vorkommt, bezogen auf alle Transaktionen (vgl. Wahrscheinlichkeit, \\(P(A)\\)) Support(\\(A\\rightarrow B\\))=Support(\\(A\\cup B\\))\\(=\\frac{|\\{t\\in D; (A\\cup B) \\subseteq t\\}|}{|D|}\\): Relative Häufigkeit, d. h. der Anteil der Transaktionen, in denen die Vereinigung einer Menge von Items \\(A\\) und \\(B\\) vorkommt, bezogen auf alle Transaktionen (vgl. gemeinsame Wahrscheinlichkeit, \\(P(A \\cap B)\\)) Confidence(\\(A \\rightarrow B\\))\\(=\\frac{support(A \\cup B)}{support(A)}\\): Relative Häufigkeit von \\(B\\) (rechte Seite), bezogen auf alle Transaktionen mit \\(A\\) (linke Seite; vgl. bedingte Wahrscheinlichkeit, \\(P(B|A)\\)) Lift(\\(A \\rightarrow B\\))\\(=\\frac{support(A \\cup B)}{support(A) \\cdot support(B)}\\): Steigerung des Supports von \\(A\\) und \\(B\\) im Vergleich zur unter Unabhängigkeit von \\(A\\) und \\(B\\) erwarteten Häufigkeit (vgl. \\(\\frac{P(A\\cap B)}{P(A) \\cdot P(B)}=\\frac{P(B|A)}{P(B)}\\)) Hinweis: Support und Lift sind symmetrisch, Confidence nicht. Als Schätzwerte für Wahrscheinlichkeiten etc. können die genannten Kennzahlen ggfs. ungeeignet sein (insbesondere für seltene Items). 7.5.3 Beispiel Support(Chips)\\(=0.05\\), d. h. 5% der Transaktionen enthalten Chips Support(Bier)\\(=0.01\\), d. h. 1% der Transaktionen enthalten Bier Support(Bier und Chips)\\(=0.002\\), d. h. 0,2% der Transaktionen enthalten Bier und Chips, dann: Confidence(Bier\\(\\rightarrow\\)Chips)\\(=\\frac{0.002}{0.01}=0.2\\), d. h. 20% aller Transaktionen mit Bier enthalten auch Chips Lift(Bier\\(\\rightarrow\\)Chips)\\(=\\frac{0.002}{0.01\\cdot 0.05}=\\frac{0.2}{0.05}=4\\), d. h. die Chance, dass eine Transaktion Chips enthält ist 4x größer als zu erwarten wäre, wenn es keinen Zusammenhang zwischen Bier und Chips gäbe 7.5.4 Assoziationsanalyse mit R Für eine Assoziationsanalyse kann in R das Zusatzpaket arules https://cran.r-project.org/package=arules verwendet werden. Die (einmalige) Installation erfolgt über: r install.packages(c(&quot;arules&quot;, &quot;arulesViz&quot;)) Geladen wird das Paket dann über library(arules) ## Loading required package: Matrix ## ## Attaching package: &#39;arules&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## abbreviate, write Eine Einführung erhält man über die Paket Vignette vignette(&quot;arules&quot;) 7.5.4.1 Beispieldaten Im Paket arules sind Point-Of-Sale Daten eines Lebensmittelgeschäftes von einem Monat enthalten.38 Die Lebensmittel wurden zu 169 Kategorien zusammengefasst, und es gibt 9835 Transaktionen: r data(Groceries, package = &quot;arules&quot;) # Daten laden summary(Groceries) ## transactions as itemMatrix in sparse format with ## 9835 rows (elements/itemsets/transactions) and ## 169 columns (items) and a density of 0.02609146 ## ## most frequent items: ## whole milk other vegetables rolls/buns soda ## 2513 1903 1809 1715 ## yogurt (Other) ## 1372 34055 ## ## element (itemset/transaction) length distribution: ## sizes ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 ## 16 17 18 19 20 21 22 23 24 26 27 28 29 32 ## 46 29 14 14 9 11 4 6 1 1 1 1 3 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 3.000 4.409 6.000 32.000 ## ## includes extended item information - examples: ## labels level2 level1 ## 1 frankfurter sausage meat and sausage ## 2 sausage sausage meat and sausage ## 3 liver loaf sausage meat and sausage Hinweis: Um einen Datensatz als Transaktionsdatensatz zu definieren wird der Befehl daten.trans &lt;- as(daten, &quot;transactions&quot;) verwendet. Siehe auch Hilfeseite zu transactions-class. Über inspect() können Transaktionen und Regeln betrachtet werden: r inspect(head(Groceries)) ## items ## [1] {citrus fruit, ## semi-finished bread, ## margarine, ## ready soups} ## [2] {tropical fruit, ## yogurt, ## coffee} ## [3] {whole milk} ## [4] {pip fruit, ## yogurt, ## cream cheese , ## meat spreads} ## [5] {other vegetables, ## whole milk, ## condensed milk, ## long life bakery product} ## [6] {whole milk, ## butter, ## yogurt, ## rice, ## abrasive cleaner} 7.5.4.2 Regeln finden Es existieren verschiedene Algorithmen um Assoziationsregeln zu finden. Hier wird der Apriori Algorithmus verwendet, wobei verschiedene Parameter (wie z. B. minimalen Support und Confidence) eingestellt werden können: r lebensmittel.regeln &lt;- apriori(Groceries, parameter=list(supp=0.02, conf=0.1, target=&quot;rules&quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.1 0.1 1 none FALSE TRUE 5 0.02 1 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 196 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. ## sorting and recoding items ... [59 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 done [0.00s]. ## writing ... [128 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. inspect(subset(lebensmittel.regeln, lift&gt;2.5)) ## lhs rhs support ## [1] {pip fruit} =&gt; {tropical fruit} 0.02043721 ## [2] {tropical fruit} =&gt; {pip fruit} 0.02043721 ## [3] {other vegetables,whole milk} =&gt; {root vegetables} 0.02318251 ## confidence lift ## [1] 0.2701613 2.574648 ## [2] 0.1947674 2.574648 ## [3] 0.3097826 2.842082 Das Ergebnis zeigt, dass in ca. 2% der Transaktionen Kern- (“pip”, z. B. Äpfel, Birnen) und Südfrüchte (“tropical”, z. B. Zitronen) enthalten waren (support). 27% der Transaktionen mit Kernfrüchten (lhs) enthielten auch Südfrüchte (rhs, confidence). Wenn also eine Transaktion eine Kernfrucht enthält ist ist es 2,57x häufiger, dass die Transaktion auch Südfrüchte enthält unter unabhängigen Häufigkeiten (lift). Um die “Top” Regeln zu betrachten müssen die Regeln nach dem gewünschten Kriterium sortiert werden: r topregeln &lt;- head(sort(lebensmittel.regeln, by=&quot;confidence&quot;), 20) inspect(topregeln) ## lhs rhs support ## [1] {other vegetables,yogurt} =&gt; {whole milk} 0.02226741 ## [2] {butter} =&gt; {whole milk} 0.02755465 ## [3] {curd} =&gt; {whole milk} 0.02613116 ## [4] {root vegetables,other vegetables} =&gt; {whole milk} 0.02318251 ## [5] {root vegetables,whole milk} =&gt; {other vegetables} 0.02318251 ## [6] {domestic eggs} =&gt; {whole milk} 0.02999492 ## [7] {whipped/sour cream} =&gt; {whole milk} 0.03223183 ## [8] {root vegetables} =&gt; {whole milk} 0.04890696 ## [9] {root vegetables} =&gt; {other vegetables} 0.04738180 ## [10] {frozen vegetables} =&gt; {whole milk} 0.02043721 ## [11] {margarine} =&gt; {whole milk} 0.02419929 ## [12] {beef} =&gt; {whole milk} 0.02125064 ## [13] {tropical fruit} =&gt; {whole milk} 0.04229792 ## [14] {whipped/sour cream} =&gt; {other vegetables} 0.02887646 ## [15] {yogurt} =&gt; {whole milk} 0.05602440 ## [16] {pip fruit} =&gt; {whole milk} 0.03009659 ## [17] {whole milk,yogurt} =&gt; {other vegetables} 0.02226741 ## [18] {brown bread} =&gt; {whole milk} 0.02521607 ## [19] {other vegetables} =&gt; {whole milk} 0.07483477 ## [20] {pork} =&gt; {whole milk} 0.02216573 ## confidence lift ## [1] 0.5128806 2.007235 ## [2] 0.4972477 1.946053 ## [3] 0.4904580 1.919481 ## [4] 0.4892704 1.914833 ## [5] 0.4740125 2.449770 ## [6] 0.4727564 1.850203 ## [7] 0.4496454 1.759754 ## [8] 0.4486940 1.756031 ## [9] 0.4347015 2.246605 ## [10] 0.4249471 1.663094 ## [11] 0.4131944 1.617098 ## [12] 0.4050388 1.585180 ## [13] 0.4031008 1.577595 ## [14] 0.4028369 2.081924 ## [15] 0.4016035 1.571735 ## [16] 0.3978495 1.557043 ## [17] 0.3974592 2.054131 ## [18] 0.3887147 1.521293 ## [19] 0.3867578 1.513634 ## [20] 0.3844797 1.504719 7.5.5 Visualisierung Eine mögliche Visualisierung ist ein Streudiagramm von Support und Confidence library(arulesViz) plot(lebensmittel.regeln) Mit Hilfe der Option interactive=TRUE kann in Bereiche gezoomt werden – und Regeln ausgewählt: r plot(lebensmittel.regeln, interactive=TRUE) Aber auch z. B. ein Graph eines entsprechenden Netzwerks ist möglich: r plot(topregeln, method=&quot;graph&quot;) 7.5.5.1 Literatur Chris Chapman und Elea McDonnell Feit (2015), R for Marketing Research and Analytics, Springer Michael Hahsler (2015), A Probabilistic Comparison of Commonly Used Interest Measures for Association Rules, URL: http://michael.hahsler.net/research/association_rules/measures.html Michael Hahsler, Sudheer Chelluboina, Kurt Hornik, und Christian Buchta (2011), The arules R-package ecosystem: Analyzing interesting patterns from large transaction datasets. Journal of Machine Learning Research, 12:1977–1981 Michael Hahsler, Bettina Gruen und Kurt Hornik (2005), arules - A Computational Environment for Mining Association Rules and Frequent Item Sets. Journal of Statistical Software 14/15. Michael Hahsler, Kurt Hornik, und Thomas Reutterer (2006), Implications of probabilistic data modeling for mining association rules. In: M. Spiliopoulou, R. Kruse, C. Borgelt, A. Nuernberger, und W. Gaul, Editors, From Data and Information Analysis to Knowledge Engineering, Studies in Classification, Data Analysis, and Knowledge Organization, Seiten 598–605. Springer-Verlag. 7.5.5.2 Versionshinweise: Die Darstellung orientiert sich an den Folienunterlagen von Chapman &amp; Feit zum Buch R for Marketing Research and Analytics, Springer, 2015, siehe http://r-marketing.r-forge.r-project.org/Instructor/slides-index.html Das Paket NbClust, siehe Malika Charrad, Nadia Ghazzali, Veronique Boiteau, Azam Niknafs (2014) NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set, Journal of Statistical Software, 61(6), 1-36. http://dx.doi.org/10.18637/jss.v061.i06, bietet viele Möglichkeiten die Anzahl der Cluster optimal zu bestimmen.↩ Michael Hahsler, Kurt Hornik, und Thomas Reutterer (2006) Implications of probabilistic data modeling for mining association rules. In: M. Spiliopoulou, R. Kruse, C. Borgelt, A. Nuernberger, und W. Gaul, Editors, From Data and Information Analysis to Knowledge Engineering, Studies in Classification, Data Analysis, and Knowledge Organization, Seiten 598–605. Springer-Verlag.↩ "],
["textmining.html", "Kapitel 8 Textmining 8.1 Einführung 8.2 Grundlegende Analyse 8.3 Sentiment-Analyse 8.4 Verknüpfung mit anderen Variablen 8.5 Vertiefung 8.6 Verweise", " Kapitel 8 Textmining In diesem Kapitel benötigte R-Pakete: library(tidyverse) # Datenjudo library(okcupiddata) # Daten library(stringr) # Textverarbeitung library(tidytext) # Textmining library(pdftools) # PDF einlesen library(downloader) # Daten herunterladen library(knitr) # HTML-Tabellen library(lsa) # Stopwörter library(SnowballC) # Wörter trunkieren library(wordcloud) # Wordcloud anzeigen Ein großer Teil der zur Verfügung stehenden Daten liegt nicht als braves Zahlenmaterial vor, sondern in “unstrukturierter” Form, z.B. in Form von Texten. Im Gegensatz zur Analyse von numerischen Daten ist die Analyse von Texten39 weniger verbreitet bisher. In Anbetracht der Menge und der Informationsreichhaltigkeit von Text erscheint die Analyse von Text als vielversprechend. In gewisser Weise ist das Textmining ein alternative zu klassischen qualitativen Verfahren der Sozialforschung. Geht es in der qualitativen Sozialforschung primär um das Verstehen eines Textes, so kann man für das Textmining ähnliche Ziele formulieren. Allerdings: Das Textmining ist wesentlich schwächer und beschränkter in der Tiefe des Verstehens. Der Computer ist einfach noch wesentlich dümmer als ein Mensch, in dieser Hinsicht. Allerdings ist er auch wesentlich schneller als ein Mensch, was das Lesen betrifft. Daher bietet sich das Textmining für das Lesen großer Textmengen an, in denen eine geringe Informationsdichte vermutet wird. Sozusagen maschinelles Sieben im großen Stil. Da fällt viel durch die Maschen, aber es werden Tonnen von Sand bewegt. In der Regel wird das Textmining als gemischte Methode verwendet: sowohl qualitative als auch qualitative Aspekte spielen eine Rolle. Damit vermittelt das Textmining auf konstruktive Art und Weise zwischen den manchmal antagonierenden Schulen der qualitativ-idiographischen und der quantitativ-nomothetischen Sichtweise auf die Welt. Man könnte es auch als qualitative Forschung mit moderner Technik bezeichnen - mit den skizzierten Einschränkungen wohlgemerkt. 8.1 Einführung 8.1.1 Grundbegriffe Die computergestützte Analyse von Texten speiste (und speist) sich reichhaltig aus Quellen der Linguistik; entsprechende Fachtermini finden Verwendung: Ein Corpus bezeichnet die Menge der zu analyisierenden Dokumente; das könnten z.B. alle Reden der Bundeskanzlerin Angela Merkel sein oder alle Tweets von “@realDonaldTrump”. Ein Token (Term) ist ein elementarer Baustein eines Texts, die kleinste Analyseeinheit, häufig ein Wort. Unter tidy text versteht man einen Dataframe, in dem pro Zeile nur ein Term steht (Silge and Robinson 2016). 8.2 Grundlegende Analyse 8.2.1 Tidy Text Dataframes Basteln wir uns einen tidy text Dataframe. Wir gehen dabei von einem Vektor mit mehreren Text-Elementen aus, das ist ein realistischer Startpunkt. Unser Text-Vektor40 besteht aus 4 Elementen. text &lt;- c(&quot;Wir haben die Frauen zu Bett gebracht,&quot;, &quot;als die Männer in Frankreich standen.&quot;, &quot;Wir hatten uns das viel schöner gedacht.&quot;, &quot;Wir waren nur Konfirmanden.&quot;) Als nächstes machen wir daraus einen Dataframe. text_df &lt;- data_frame(Zeile = 1:4, text) Zeile text 1 Wir haben die Frauen zu Bett gebracht, 2 als die Männer in Frankreich standen. 3 Wir hatten uns das viel schöner gedacht. 4 Wir waren nur Konfirmanden. Und “dehnen” diesen Dataframe zu einem tidy text Dataframe. text_df %&gt;% unnest_tokens(wort, text) #&gt; # A tibble: 24 × 2 #&gt; Zeile wort #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 wir #&gt; 2 1 haben #&gt; 3 1 die #&gt; # ... with 21 more rows Das unnest_tokens kann übersetzt werden als “entschachtele” oder “dehne” die Tokens - so dass in jeder Zeile nur noch ein Wort (Token) steht. Die Syntax ist unnest_tokens(Ausgabespalte, Eingabespalte). Nebenbei werden übrigens alle Buchstaben auf Kleinschreibung getrimmt. Als nächstes filtern wir die Satzzeichen heraus, da die Wörter für die Analyse wichtiger (oder zumindest einfacher) sind. text_df %&gt;% unnest_tokens(wort, text) %&gt;% filter(str_detect(wort, &quot;[a-z]&quot;)) #&gt; # A tibble: 24 × 2 #&gt; Zeile wort #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 wir #&gt; 2 1 haben #&gt; 3 1 die #&gt; # ... with 21 more rows 8.2.2 Text-Daten einlesen Nun lesen wir Text-Daten ein; das können beliebige Daten sein. Eine gewisse Reichhaltigkeit ist von Vorteil. Nehmen wir das Parteiprogramm der Partei AfD41. afd_url &lt;- &quot;https://www.alternativefuer.de/wp-content/uploads/sites/7/2016/05/2016-06-27_afd-grundsatzprogramm_web-version.pdf&quot; afd_pfad &lt;- &quot;data/afd_programm.pdf&quot; download(afd_url, afd_pfad) afd_raw &lt;- pdf_text(afd_pfad) afd_raw[3] #&gt; [1] &quot;3\\t Programm für Deutschland | Inhalt\\n 7 | Kultur, Sprache und Identität\\t\\t\\t\\t 45 9 | Einwanderung, Integration und Asyl\\t\\t\\t 57\\n 7.1 \\t\\t Deutsche Kultur, Sprache und Identität erhalten\\t 47 9.1\\t Keine irreguläre Einwanderung über das Asylrecht\\t 59\\n 7.2 \\t\\t Deutsche Leitkultur statt Multikulturalismus\\t\\t 47 9.1.1\\t Asylzuwanderung - für einen Paradigmenwechsel\\t 59\\n 7.3 \\t\\t Die deutsche Sprache als Zentrum unserer Identität\\t 47 9.1.2\\t Rückführung - Schluss mit Fehlanreizen und \\t\\t\\t\\n 7.4 \\t \\t Kultur und Kunst von Einflussnahme der Parteien befreien\\t 48 \\t\\t falscher Nachsicht\\t\\t\\t\\t\\t 60\\n 7.5 \\t\\t Für eine zeitgemäße Medienpolitik: Rundfunkbeitrag abschaffen\\t 48 9.2\\t Einwanderung aus EU-Staaten\\t\\t\\t\\t 61\\n 7.6 \\t\\t Der Islam im Spannungsverhältnis zu unserer Werteordnung\\t 48 9.3\\t Gesteuerte Einwanderung aus Drittstaaten\\t\\t 62\\n 7.6.1\\t\\t Der Islam gehört nicht zu Deutschland\\t\\t\\t 49 9.4\\t Integration - Mehr als nur Deutsch lernen\\t\\t 63\\n 7.6.2\\t\\t Kritik am Islam muss erlaubt sein\\t\\t\\t 49 9.5\\t Kosten der Einwanderung - Transparenz herstellen\\t 63\\n 7.6.3\\t\\t Auslandsfinanzierung von Moscheen beenden\\t \\t 49 9.6\\t Einwandererkriminalität - nichts verschleiern,\\n 7.6.4\\t\\t Keine öffentlich-rechtliche Körperschaft für \\t\\t nichts verschweigen\\t\\t\\t\\t\\t 64\\n \\t\\t\\t islamische Organisationen\\t\\t\\t\\t 50 9.7\\t Einbürgerung - Abschluss gelungener Integration\\t 65\\n 7.6.5\\t\\t Keine Vollverschleierung im öffentlichen Raum\\t 50\\n 10 | Wirtschaft, digitale Welt und Verbraucherschutz\\t 66\\n 8 | Schule, Hochschule und Forschung\\t\\t\\t 51 10.1\\t\\t Freier Wettbewerb sichert unseren Wohlstand\\t\\t 67\\n 8.1 \\t\\t Forschung und Lehre: In Freiheit und als Einheit\\t 52 10.2 \\t\\t Soziale Marktwirtschaft statt Planwirtschaft\\t\\t 67\\n 8.1.1\\t \\t Autonomie durch Grundfinanzierung stärken\\t \\t 52 10.3 \\t\\t Internationale Wirtschaftspolitik neu ausrichten\\t 67\\n 8.1.2\\t\\t Förderung der “Gender-Forschung” beenden\\t\\t 52 10.4 \\t\\t Hohe Standards für Handelsabkommen\\t\\t 68\\n 8.1.3\\t\\t Diplom, Magister und Staatsexamen wieder einführen\\t 52 10.5 \\t\\t Bürokratie abbauen\\t\\t\\t\\t\\t 68\\n 8.1.4\\t\\t Studienanforderungen erhöhen\\t\\t\\t 53 10.6 \\t\\t Den Technologiestandort Deutschland voranbringen\\t 68\\n 8.2 \\t\\t Unser Schulsystem: Stark durch Differenzierung\\t 53 10.7 \\t\\t Staatliche Subventionen reduzieren und befristen\\t 69\\n 8.2.1\\t\\t Die Einheitsschule führt zu Qualitätsverlust\\t\\t 53 10.8 \\t\\t Keine Privatisierung gegen den Willen der Bürger\\t 69\\n 8.2.2\\t\\t Wissensvermittlung muss zentrales Anliegen bleiben\\t 53 10.9 \\t\\t Der Mittelstand als Herz unserer Wirtschaftskraft\\t 69\\n 8.2.3\\t\\t Leistungsbereitschaft und Disziplin stärken\\t\\t 54 10.10 \\tDigitalisierung als Chance und Herausforderung\\t 69\\n 8.2.4\\t\\t Politisch-ideologische Indoktrination darf es an 10.10.1 Quelloffene Software und sichere Hardware\\t\\t 69\\n \\t\\t\\t der Schule nicht geben\\t\\t\\t\\t\\t 54 10.10.2 Sichere Kommunikation als Standortvorteil\\n 8.2.5\\t\\t Duale berufliche Bildung stärken und erhalten\\t \\t 54 \\t\\t und Bürgerrecht\\t\\t\\t\\t\\t 70\\n 8.2.6\\t\\t Keine Inklusion “um jeden Preis”. Förder- und 10.10.3 Deutsche Literatur im Inland digitalisieren\\t\\t 70\\n \\t\\t\\t Sonderschulen erhalten\\t\\t\\t\\t 54 10.11\\t\\t Verbraucherschutz modernisieren und stärken\\t\\t 70\\n 8.2.7 \\t\\t Koranschulen schließen. Islamkunde in den 10.11.1 Lebensmittel besser kennzeichnen\\t\\t\\t 71\\n \\t\\t\\t Ethikunterricht integrieren\\t\\t\\t\\t 55 10.11.2 Langlebige Produkte statt geplante Obsoleszenz\\t 71\\n 8.2.8 \\t Keine Sonderrechte für muslimische Schüler\\t\\t 55 10.11.3 Textilien und Kinderspielzeug auf Schadstoffe prüfen\\t 71\\n 8.3 \\t\\t Nein zu “Gender-Mainstreaming” und 10.11.4 Wasseraufbereitung modernisieren und verbessern\\t 71\\n \\t\\t\\t Frühsexualisierung\\t\\t\\t\\t\\t 55\\n 8.3.1 \\t\\t Keine “geschlechterneutrale” Umgestaltung der\\n \\t\\t\\t deutschen Sprache\\t\\t\\t\\t\\t 55\\n 8.3.2 \\t Geschlechterquoten sind leistungsfeindlich\\n \\t\\t\\t und ungerecht\\t\\t\\t\\t\\t\\t 56\\n&quot; Mit download haben wir die Datei mit der Url afd_url heruntergeladen und als afd_pfad gespeichert. Für uns ist pdf_text sehr praktisch, da diese Funktion Text aus einer beliebige PDF-Datei in einen Text-Vektor einliest. Der Vektor afd_raw hat 96 Elemente (entsprechend der Seitenzahl des Dokzements); zählen wir die Gesamtzahl an Wörtern. Dazu wandeln wir den Vektor in einen tidy text Dataframe um. Auch die Stopwörter entfernen wir wieder wie gehabt. afd_df &lt;- data_frame(Zeile = 1:96, afd_raw) afd_df %&gt;% unnest_tokens(token, afd_raw) %&gt;% filter(str_detect(token, &quot;[a-z]&quot;)) -&gt; afd_df count(afd_df) #&gt; # A tibble: 1 × 1 #&gt; n #&gt; &lt;int&gt; #&gt; 1 26396 Eine substanzielle Menge von Text. Was wohl die häufigsten Wörter sind? 8.2.3 Worthäufigkeiten auszählen afd_df %&gt;% na.omit() %&gt;% # fehlende Werte löschen count(token, sort = TRUE) #&gt; # A tibble: 7,087 × 2 #&gt; token n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 die 1151 #&gt; 2 und 1147 #&gt; 3 der 870 #&gt; # ... with 7,084 more rows Die häufigsten Wörter sind inhaltsleere Partikel, Präpositionen, Artikel… Solche sogenannten “Stopwörter” sollten wir besser herausfischen, um zu den inhaltlich tragenden Wörtern zu kommen. Praktischerweise gibt es frei verfügbare Listen von Stopwörtern, z.B. im Paket lsa. data(stopwords_de) stopwords_de &lt;- data_frame(word = stopwords_de) stopwords_de &lt;- stopwords_de %&gt;% rename(token = word) afd_df %&gt;% anti_join(stopwords_de) -&gt; afd_df Unser Datensatz hat jetzt viel weniger Zeilen; wir haben also durch anti_join Zeilen gelöscht (herausgefiltert). Das ist die Funktion von anti_join: Die Zeilen, die in beiden Dataframes vorkommen, werden herausgefiltert. Es verbleiben also nicht “Nicht-Stopwörter” in unserem Dataframe. Damit wird es schon interessanter, welche Wörter häufig sind. afd_df %&gt;% count(token, sort = TRUE) -&gt; afd_count afd_count %&gt;% top_n(10) %&gt;% knitr::kable() token n deutschland 190 afd 171 programm 80 wollen 67 bürger 57 euro 55 dafür 53 eu 53 deutsche 47 deutschen 47 Ganz interessant; aber es gibt mehrere Varianten des Themas “deutsch”. Es ist wohl sinnvoller, diese auf den gemeinsamen Wortstamm zurückzuführen und diesen nur einmal zu zählen. Dieses Verfahren nennt man “stemming” oder trunkieren. afd_df %&gt;% mutate(token_stem = wordStem(.$token, language = &quot;german&quot;)) %&gt;% count(token_stem, sort = TRUE) -&gt; afd_count afd_count %&gt;% top_n(10) %&gt;% knitr::kable() token_stem n deutschland 219 afd 171 deutsch 119 polit 88 staat 85 programm 81 europa 80 woll 67 burg 66 soll 63 Das ist schon informativer. Dem Befehl wordStem füttert man einen Vektor an Wörtern ein und gibt die Sprache an (Default ist Englisch42). Das ist schon alles. 8.2.4 Visualisierung Zum Abschluss noch eine Visualisierung mit einer “Wordcloud” dazu. wordcloud(words = afd_count$token_stem, freq = afd_count$n, max.words = 100, scale = c(2,.5), colors=brewer.pal(6, &quot;Dark2&quot;)) Man kann die Anzahl der Wörter, Farben und einige weitere Formatierungen der Wortwolke beeinflussen43. Weniger verspielt ist eine schlichte visualisierte Häufigkeitsauszählung dieser Art, z.B. mit Balkendiagrammen (gedreht). afd_count %&gt;% top_n(30) %&gt;% ggplot() + aes(x = reorder(token_stem, n), y = n) + geom_col() + labs(title = &quot;mit Trunkierung&quot;) + coord_flip() -&gt; p1 afd_df %&gt;% count(token, sort = TRUE) %&gt;% top_n(30) %&gt;% ggplot() + aes(x = reorder(token, n), y = n) + geom_col() + labs(title = &quot;ohne Trunkierung&quot;) + coord_flip() -&gt; p2 library(gridExtra) grid.arrange(p1, p2, ncol = 2) Die beiden Diagramme vergleichen die trunkierten Wörter mit den nicht trunktierten Wörtern. Mit reorder ordnen wir die Spalte token nach der Spalte n. coord_flip dreht die Abbildung um 90°, d.h. die Achsen sind vertauscht. grid.arrange packt beide Plots in eine Abbildung, welche 2 Spalten (ncol) hat. 8.3 Sentiment-Analyse Eine weitere interessante Analyse ist, die “Stimmung” oder “Emotionen” (Sentiments) eines Textes auszulesen. Die Anführungszeichen deuten an, dass hier ein Maß an Verständnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so: Schau dir jeden Token aus dem Text an. Prüfe, ob sich das Wort im Lexikon der Sentiments wiederfindet. Wenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert. Wenn nein, dann gehe weiter zum nächsten Wort. Liefere zum Schluss die Summenwerte pro Sentiment zurück. Es gibt Sentiment-Lexika, die lediglich einen Punkt für “positive Konnotation” bzw. “negative Konnotation” geben; andere Lexiko weisen differenzierte Gefühlskonnotationen auf. Wir nutzen hier dieses Lexikon (Remus, Quasthoff, and Heyer 2010). Der Einfachheit halber gehen wir im Folgenden davon aus, dass das Lexikon schon aufbereitet vorliegt. Die Aufbereitung unten im Abschnitt zur Vertiefung nachgelesen werden. Unser Sentiment-Lexikon sieht so aus: library(knitr) kable(head(sentiment_df)) neg_pos Wort Wert Inflektionen neg Abbau -0.058 Abbaus,Abbaues,Abbauen,Abbaue neg Abbruch -0.005 Abbruches,Abbrüche,Abbruchs,Abbrüchen neg Abdankung -0.005 Abdankungen neg Abdämpfung -0.005 Abdämpfungen neg Abfall -0.005 Abfalles,Abfälle,Abfalls,Abfällen neg Abfuhr -0.337 Abfuhren 8.3.1 Ungewichtete Sentiment-Analyse Nun können wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei zählen wir die Treffer für positive bzw. negative Terme. Besser wäre noch: Wir könnten die Sentiment-Werte pro Treffer addieren (und nicht für jeden Term 1 addieren). Aber das heben wir uns für später auf. sentiment_neg &lt;- match(afd_df$token, filter(sentiment_df, neg_pos == &quot;neg&quot;)$Wort) neg_score &lt;- sum(!is.na(sentiment_neg)) sentiment_pos &lt;- match(afd_df$token, filter(sentiment_df, neg_pos == &quot;pos&quot;)$Wort) pos_score &lt;- sum(!is.na(sentiment_pos)) round(pos_score/neg_score, 1) #&gt; [1] 2.7 Hier schauen wir für jedes negative (positive) Token, ob es einen “Match” im Sentiment-Lexikon (sentiment_df$Wort) gibt; das geht mit match. match liefert NA zurück, wenn es keinen Match gibt (ansonsten die Nummer des Sentiment-Worts). Wir brauchen also nur die Anzahl der Nicht-Nas (!is.na) auszuzählen, um die Anzahl der Matches zu bekommen. Entgegen dem, was man vielleicht erwarten würde, ist der Text offenbar positiv geprägt. Der “Positiv-Wert” ist ca. 2.6 mal so groß wie der “Negativ-Wert”. Fragt sich, wie sich dieser Wert mit anderen vergleichbaren Texten (z.B. andere Parteien) misst. Hier sei noch einmal betont, dass die Sentiment-Analyse bestenfalls grobe Abschätzungen liefern kann und keinesfalls sich zu einem hermeneutischen Verständnis aufschwingt. Welche negativen Wörter und welche positiven Wörter wurden wohl verwendet? Schauen wir uns ein paar an. afd_df %&gt;% mutate(sentiment_neg = sentiment_neg, sentiment_pos = sentiment_pos) -&gt; afd_df afd_df %&gt;% filter(!is.na(sentiment_neg)) %&gt;% select(token) -&gt; negative_sentiments head(negative_sentiments$token,50) #&gt; [1] &quot;mindern&quot; &quot;verbieten&quot; &quot;unmöglich&quot; &quot;töten&quot; #&gt; [5] &quot;träge&quot; &quot;schädlich&quot; &quot;unangemessen&quot; &quot;unterlassen&quot; #&gt; [9] &quot;kalt&quot; &quot;schwächen&quot; &quot;ausfallen&quot; &quot;verringern&quot; #&gt; [13] &quot;verringern&quot; &quot;verringern&quot; &quot;verringern&quot; &quot;belasten&quot; #&gt; [17] &quot;belasten&quot; &quot;fremd&quot; &quot;schädigenden&quot; &quot;klein&quot; #&gt; [21] &quot;klein&quot; &quot;klein&quot; &quot;klein&quot; &quot;eingeschränkt&quot; #&gt; [25] &quot;eingeschränkt&quot; &quot;entziehen&quot; &quot;schwer&quot; &quot;schwer&quot; #&gt; [29] &quot;schwer&quot; &quot;schwer&quot; &quot;verharmlosen&quot; &quot;unerwünscht&quot; #&gt; [33] &quot;abgleiten&quot; &quot;wirkungslos&quot; &quot;schwach&quot; &quot;verschleppen&quot; #&gt; [37] &quot;vermindern&quot; &quot;vermindern&quot; &quot;ungleich&quot; &quot;widersprechen&quot; #&gt; [41] &quot;zerstört&quot; &quot;zerstört&quot; &quot;erschweren&quot; &quot;auffallen&quot; #&gt; [45] &quot;unvereinbar&quot; &quot;unvereinbar&quot; &quot;unvereinbar&quot; &quot;abhängig&quot; #&gt; [49] &quot;abhängig&quot; &quot;abhängig&quot; afd_df %&gt;% filter(!is.na(sentiment_pos)) %&gt;% select(token) -&gt; positive_sentiments head(positive_sentiments$token, 50) #&gt; [1] &quot;optimal&quot; &quot;aufstocken&quot; &quot;locker&quot; #&gt; [4] &quot;zulässig&quot; &quot;gleichwertig&quot; &quot;wiederbeleben&quot; #&gt; [7] &quot;beauftragen&quot; &quot;wertvoll&quot; &quot;nah&quot; #&gt; [10] &quot;nah&quot; &quot;nah&quot; &quot;überzeugt&quot; #&gt; [13] &quot;genehmigen&quot; &quot;genehmigen&quot; &quot;überleben&quot; #&gt; [16] &quot;überleben&quot; &quot;genau&quot; &quot;verständlich&quot; #&gt; [19] &quot;erlauben&quot; &quot;aufbereiten&quot; &quot;zugänglich&quot; #&gt; [22] &quot;messbar&quot; &quot;erzeugen&quot; &quot;erzeugen&quot; #&gt; [25] &quot;ausgleichen&quot; &quot;ausreichen&quot; &quot;mögen&quot; #&gt; [28] &quot;kostengünstig&quot; &quot;gestiegen&quot; &quot;gestiegen&quot; #&gt; [31] &quot;bedeuten&quot; &quot;massiv&quot; &quot;massiv&quot; #&gt; [34] &quot;massiv&quot; &quot;massiv&quot; &quot;einfach&quot; #&gt; [37] &quot;finanzieren&quot; &quot;vertraulich&quot; &quot;steigen&quot; #&gt; [40] &quot;erweitern&quot; &quot;verstehen&quot; &quot;schnell&quot; #&gt; [43] &quot;zugreifen&quot; &quot;tätig&quot; &quot;unternehmerisch&quot; #&gt; [46] &quot;entlasten&quot; &quot;entlasten&quot; &quot;entlasten&quot; #&gt; [49] &quot;entlasten&quot; &quot;helfen&quot; 8.3.2 Anzahl der unterschiedlichen negativen bzw. positiven Wörter Allerdings müssen wir unterscheiden zwischen der Anzahl der negativen bzw. positiven Wörtern und der Anzahl der unterschiedlichen Wörter. Zählen wir noch die Anzahl der unterschiedlichen Wörter im negativen und positiven Fall. afd_df %&gt;% filter(!is.na(sentiment_neg)) %&gt;% summarise(n_distinct_neg = n_distinct(token)) #&gt; # A tibble: 1 × 1 #&gt; n_distinct_neg #&gt; &lt;int&gt; #&gt; 1 96 afd_df %&gt;% filter(!is.na(sentiment_pos)) %&gt;% summarise(n_distinct_pos = n_distinct(token)) #&gt; # A tibble: 1 × 1 #&gt; n_distinct_pos #&gt; &lt;int&gt; #&gt; 1 187 Dieses Ergebnis passt zum vorherigen: Die Anzahl der positiven Wörter (187) ist ca. doppelt so groß wie die Anzahl der negativen Wörter (96). 8.3.3 Gewichtete Sentiment-Analyse Oben haben wir nur ausgezählt, ob ein Term der Sentiment-Liste im Corpus vorkam. Genauer ist es, diesen Term mit seinem Sentiment-Wert zu gewichten, also eine gewichtete Summe zu erstellen. sentiment_df %&gt;% rename(token = Wort) -&gt; sentiment_df afd_df %&gt;% left_join(sentiment_df, by = &quot;token&quot;) -&gt; afd_df afd_df %&gt;% filter(!is.na(Wert)) %&gt;% summarise(Sentimentwert = sum(Wert, na.rm = TRUE)) -&gt; afd_sentiment_summe afd_sentiment_summe$Sentimentwert #&gt; [1] -23.9 Zuerst bennenen wir Wort in token um, damit es beiden Dataframes (sentiment_df und afd_df) eine Spalte mit gleichen Namen gibt. Diese Spalte können wir dann zum “Verheiraten” (left_join) der beiden Spalten nutzen. Dann summieren wir den Sentiment-Wert jeder nicht-leeren Zeile auf. Siehe da: Nun ist der Duktus deutlich negativer als positiver. Offebar werden mehr positive Wörter als negative verwendet, aber die negativen sind viel intensiver. 8.3.4 Tokens mit den extremsten Sentimentwerten Schauen wir uns die intensivesten Wörter mal an. afd_df %&gt;% filter(neg_pos == &quot;pos&quot;) %&gt;% distinct(token, .keep_all = TRUE) %&gt;% arrange(-Wert) %&gt;% filter(row_number() &lt; 11) %&gt;% select(token, Wert) %&gt;% kable() token Wert besonders 0.539 genießen 0.498 wichtig 0.382 sicher 0.373 helfen 0.373 miteinander 0.370 groß 0.369 wertvoll 0.357 motiviert 0.354 gepflegt 0.350 afd_df %&gt;% filter(neg_pos == &quot;neg&quot;) %&gt;% distinct(token, .keep_all = TRUE) %&gt;% arrange(Wert) %&gt;% filter(row_number() &lt; 11) %&gt;% select(token, Wert) %&gt;% kable() token Wert schädlich -0.927 schwach -0.921 brechen -0.799 ungerecht -0.784 behindern -0.775 falsch -0.762 gemein -0.720 gefährlich -0.637 verbieten -0.629 vermeiden -0.526 Tatsächlich erscheinen die negativen Wörter “dampfender” und “fauchender” als die positiven. Die Syntax kann hier so übersetzt werden: Nehmen den Dataframe adf_df UND DANN filtere die Token mit negativen Sentiment UND DANN lösche doppelte Zeilen UND DANN sortiere (absteigend) UND DANN filtere nur die Top 10 UND DANN zeige nur die Saplten token und Wert UND DANN zeige eine schöne Tabelle. 8.3.5 Relativer Sentiments-Wert Nun könnte man noch den erzielten “Netto-Sentimentswert” des Corpus ins Verhältnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, wäre ein negativer Sentimentwer in einem beliebigen Corpus nicht überraschend. sentiment_df %&gt;% filter(!is.na(Wert)) %&gt;% ggplot() + aes(x = Wert) + geom_histogram() Es scheint einen (leichten) Überhang an negativen Wörtern zu geben. Schauen wir auf die genauen Zahlen. sentiment_df %&gt;% filter(!is.na(Wert)) %&gt;% count(neg_pos) #&gt; # A tibble: 2 × 2 #&gt; neg_pos n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 neg 1818 #&gt; 2 pos 1650 Tatsächlich ist die Zahl negativ konnotierter Terme etwas größer als die Zahl der positiv konnotierten. Jetzt gewichten wir die Zahl mit dem Sentimentswert der Terme, in dem wir die Sentimentswerte (die ein negatives bzw. ein positives Vorzeichen aufweisen) aufaddieren. sentiment_df %&gt;% filter(!is.na(Wert)) %&gt;% summarise(sentiment_summe = sum(Wert)) -&gt; sentiment_lexikon_sum sentiment_lexikon_sum$sentiment_summe #&gt; [1] -187 Im Vergleich zum Sentiment der Lexikons ist unser Corpus deutlich negativer. Um genau zu sein, um diesen Faktor: sentiment_lexikon_sum$sentiment_summe / afd_sentiment_summe$Sentimentwert #&gt; [1] 7.83 Der relative Sentimentswert (relativ zum Sentiment-Lexikon) beträgt also ~7.8. 8.4 Verknüpfung mit anderen Variablen Kann man die Textdaten mit anderen Daten verknüpfen, so wird die Analyse reichhaltiger. So könnte man überprüfen, ob sich zwischen Sentiment-Gehalt und Zeit oder Autor ein Muster findet/bestätigt. Uns liegen in diesem Beispiel keine andere Daten vor, so dass wir dieses Beispiel nicht weiter verfolgen. 8.5 Vertiefung 8.5.1 Erstellung des Sentiment-Lexikons Der Zweck dieses Abschnitts ist es, eine Sentiment-Lexikon in deutscher Sprache einzulesen. Dazu wird das Sentiment-Lexikon dieser Quelle verwendet (CC-BY-NC-SA 3.0). In diesem Paper finden sich Hintergründe. Von dort lassen sich die Daten herunter laden. Im folgenden gehe ich davon aus, dass die Daten herunter geladen sind und sich im Working Directory befinden. Wir benötigen diese Pakete (es ginge auch über base): library(stringr) library(readr) library(dplyr) Dann lesen wir die Daten ein, zuerst die Datei mit den negativen Konnotationen: neg_df &lt;- read_tsv(&quot;SentiWS_v1.8c_Negative.txt&quot;, col_names = FALSE) names(neg_df) &lt;- c(&quot;Wort_POS&quot;, &quot;Wert&quot;, &quot;Inflektionen&quot;) glimpse(neg_df) Dann parsen wir aus der ersten Spalte (Wort_POS) zum einen den entsprechenden Begriff (z.B. “Abbau”) und zum anderen die Wortarten-Tags (eine Erläuterung zu den Wortarten-Tags findet sich hier). neg_df %&gt;% mutate(Wort = str_sub(Wort_POS, 1, regexpr(&quot;\\\\|&quot;, .$Wort_POS)-1), POS = str_sub(Wort_POS, start = regexpr(&quot;\\\\|&quot;, .$Wort_POS)+1)) -&gt; neg_df str_sub parst zuerst das Wort. Dazu nehmen wir den Wort-Vektor Wort_POS, und für jedes Element wird der Text von Position 1 bis vor dem Zeichen | geparst; da der Querstrich ein Steuerzeichen in Regex muss er escaped werden. Für POS passiert das gleiche von Position |+1 bis zum Ende des Text-Elements. Das gleiche wiederholen wir für positiv konnotierte Wörter. pos_df &lt;- read_tsv(&quot;SentiWS_v1.8c_Positive.txt&quot;, col_names = FALSE) names(pos_df) &lt;- c(&quot;Wort_POS&quot;, &quot;Wert&quot;, &quot;Inflektionen&quot;) pos_df %&gt;% mutate(Wort = str_sub(Wort_POS, 1, regexpr(&quot;\\\\|&quot;, .$Wort_POS)-1), POS = str_sub(Wort_POS, start = regexpr(&quot;\\\\|&quot;, .$Wort_POS)+1)) -&gt; pos_df Schließlich schweißen wir beide Dataframes in einen: bind_rows(&quot;neg&quot; = neg_df, &quot;pos&quot; = pos_df, .id = &quot;neg_pos&quot;) -&gt; sentiment_df sentiment_df %&gt;% select(neg_pos, Wort, Wert, Inflektionen, -Wort_POS) -&gt; sentiment_df knitr::kable(head(sentiment_df)) neg_pos token Wert Inflektionen neg Abbau -0.058 Abbaus,Abbaues,Abbauen,Abbaue neg Abbruch -0.005 Abbruches,Abbrüche,Abbruchs,Abbrüchen neg Abdankung -0.005 Abdankungen neg Abdämpfung -0.005 Abdämpfungen neg Abfall -0.005 Abfalles,Abfälle,Abfalls,Abfällen neg Abfuhr -0.337 Abfuhren 8.6 Verweise Das Buch Tidy Text Minig (Julia and David 2017) ist eine hervorragende Quelle vertieftem Wissens zum Textmining mit R. References "],
["ergebnisse-kommunizieren.html", "Kapitel 9 Ergebnisse kommunizieren 9.1 Markdown und RMarkdown 9.2 Reproduzierbarkeit", " Kapitel 9 Ergebnisse kommunizieren Nicht Daten, sondern Einsichten. 9.1 Markdown und RMarkdown 9.2 Reproduzierbarkeit "],
["rahmen-teil-2.html", "Kapitel 10 Rahmen - Teil 2 10.1 Trends", " Kapitel 10 Rahmen - Teil 2 10.1 Trends Big Data Open Science Computerisierung Neue Methoden zur numerischen Vorhersage Textmining "]
]
